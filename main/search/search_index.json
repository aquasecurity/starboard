{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Starboard","text":"<p>There are lots of security tools in the cloud native world, created by Aqua and by others, for identifying and informing users about security issues in Kubernetes workloads and infrastructure components. However powerful and useful they might be, they tend to sit alongside Kubernetes, with each new product requiring users to learn a separate set of commands and installation steps in order to operate them and find critical security information.</p> <p>Starboard attempts to integrate heterogeneous security tools by incorporating their outputs into Kubernetes CRDs (Custom Resource Definitions) and from there, making security reports accessible through the Kubernetes API. This way users can find and view the risks that relate to different resources in what we call a Kubernetes-native way.</p> <p>Starboard provides:</p> <ul> <li>Automated vulnerability scanning for Kubernetes workloads.</li> <li>Automated configuration audits for Kubernetes resources with predefined rules or custom Open Policy Agent (OPA) policies.</li> <li>Automated infrastructures scanning and compliance checks with CIS Benchmarks published by the Center for Internet Security (CIS).</li> <li>Automated compliance report - NSA, CISA Kubernetes Hardening Kubernetes Guidance v1.0</li> <li>Penetration test results for a Kubernetes cluster.</li> <li>Custom Resource Definitions and a Go module to work with and integrate a range of security scanners.</li> <li>The Octant Plugin and the Lens Extension that make security reports available through familiar Kubernetes interfaces.</li> </ul> The high-level design diagram of Starboard toolkit. <p>Starboard can be used:</p> <ul> <li>As a Kubernetes operator to automatically update security reports in response to workload and other changes on a   Kubernetes cluster - for example, initiating a vulnerability scan when a new Pod is started or running CIS Benchmarks   when a new Node is added.</li> <li>As a command, so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD   pipeline.</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Learn how to install the Starboard command From the Binary Releases and   follow the Getting Started guide to generate your first vulnerability and configuration   audit reports.</li> <li>Install the Starboard Operator with kubectl and follow the   Getting Started guide to see how vulnerability and configuration audit reports are   generated automatically.</li> <li>Read more about the motivations for the project in the Starboard: The Kubernetes-Native Toolkit for Unifying Security   blog.</li> <li>See a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020.</li> <li>Join the community, and talk to us about any matter in GitHub Discussions or Slack.</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?","text":"<p>Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command:</p> <pre><code>kubectl get vulnerabilityreports \\\n  -l starboard.resource.kind=Deployment \\\n  -l starboard.resource.name=wordpress\n</code></pre> <p>Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the <code>wordpress</code> Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.</p>"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","title":"Why do you create an instance of the VulnerabilityReport for each container?","text":"<p>The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p>"},{"location":"faq/#is-starboard-cli-required-to-run-starboard-operator-or-vice-versa","title":"Is Starboard CLI required to run Starboard Operator or vice versa?","text":"<p>No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.</p>"},{"location":"further-reading/","title":"Further Reading","text":"<ul> <li>Discover Security Risks with Starboard Extension for Lens Kubernetes IDE | Aqua Blog | March 3, 2021</li> <li>Automating Configuration Auditing with Starboard Operator By Aqua | Aqua Blog | February 4, 2021</li> <li>Starboard: Putting all the Kubernetes Security Pieces into One Place | The New Stack | November 30, 2020</li> <li>Automating Kubernetes Security Reporting with Starboard Operator by Aqua | Aqua Blog | November 2, 2020</li> <li>Starboard: The Kubernetes-Native Toolkit for Unifying Security | Aqua Blog | June 1, 2020</li> </ul>"},{"location":"settings/","title":"Settings","text":"<p>The Starboard CLI and Starboard Operator read configuration settings from ConfigMaps, as well as Secrets that holds confidential settings (such as a GitHub token). Starboard plugins read configuration and secret data from ConfigMaps and Secrets named after the plugin. For example, Trivy configuration is stored in the ConfigMap and Secret named <code>starboard-trivy-config</code>.</p> <p>The <code>starboard install</code> command ensures the <code>starboard</code> ConfigMap and the <code>starboard</code> Secret in the <code>starboard</code> namespace with default settings. Similarly, the operator ensures the <code>starboard</code> ConfigMap and the <code>starboard</code> Secret in the <code>OPERATOR_NAMESPACE</code>.</p> <p>You can change the default settings with <code>kubectl patch</code> or <code>kubectl edit</code> commands. For example, by default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). However, you can display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>starboard-trivy-config</code> ConfigMap:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>To set the GitHub token used by Trivy add the <code>trivy.githubToken</code> value to the <code>starboard-trivy-config</code> Secret:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\nGITHUB_TOKEN=&lt;your token&gt;\n</code></pre> <pre><code>kubectl patch secret starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>The following table lists available settings with their default values. Check plugins' documentation to see configuration settings for common use cases. For example, switch Trivy from Standalone to ClientServer mode.</p> CONFIGMAP KEY DEFAULT DESCRIPTION <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>. <code>vulnerabilityReports.scanJobsInSameNamespace</code> <code>\"false\"</code> Whether to run vulnerability scan jobs in same namespace of workload. Set <code>\"true\"</code> to enable. <code>configAuditReports.scanner</code> <code>Polaris</code> The name of the plugin that generates config audit reports. Either <code>Polaris</code> or <code>Conftest</code>. <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code> <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.templateLabel</code> N/A One-line comma-separated representation of the template labels which the user wants the scanner pods to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the scanner pods with the labels <code>foo: bar</code> and <code>env: stage</code> <code>kube-bench.imageRef</code> <code>docker.io/aquasec/kube-bench:v0.6.9</code> kube-bench image reference <code>kube-hunter.imageRef</code> <code>docker.io/aquasec/kube-hunter:0.6.5</code> kube-hunter image reference <code>kube-hunter.quick</code> <code>\"false\"</code> Whether to use kube-hunter's \"quick\" scanning mode (subnet 24). Set to <code>\"true\"</code> to enable. <code>compliance.failEntriesLimit</code> <code>\"10\"</code> Limit the number of fail entries per control check in the cluster compliance detail report. <p>Tip</p> <p>You can find it handy to delete a configuration key, which was not created by default by the <code>starboard install</code> command. For example, the following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key: <pre><code>STARBOARD_NAMESPACE=&lt;your starboard namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard-trivy-config -n $STARBOARD_NAMESPACE \\\n  --type json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre></p>"},{"location":"cli/","title":"Overview","text":"<p>Starboard CLI is a single executable binary which can be used to find risks, such as vulnerabilities or insecure pod descriptors, in Kubernetes workloads. By default, the risk assessment reports are stored as instances of Custom Resource Definitions.</p> <p>Note</p> <p>Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes resources. Therefore, the Starboard Operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments and Nodes, and running appropriate scanners.</p> <p>To learn more about the available Starboard CLI commands, run <code>starboard help</code> or type a command followed by the <code>--help</code> flag:</p> <pre><code>starboard scan kubehunterreports --help\n</code></pre>"},{"location":"cli/#whats-next","title":"What's Next?","text":"<ul> <li>Install the command and follow the Getting Started guide.</li> </ul>"},{"location":"cli/getting-started/","title":"Getting Started","text":""},{"location":"cli/getting-started/#before-you-begin","title":"Before you Begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind, or you can use one of these Kubernetes playgrounds:</p> <ul> <li>Katacoda</li> <li>Play with Kubernetes</li> </ul> <p>You also need the <code>starboard</code> command to be installed, e.g. From the Binary Releases. By default, it will use the same configuration as kubectl to communicate with the cluster.</p>"},{"location":"cli/getting-started/#scanning-workloads","title":"Scanning Workloads","text":"<p>The easiest way to get started with Starboard is to use an imperative <code>starboard</code> command, which allows ad hoc scanning of Kubernetes workloads deployed in your cluster.</p> <p>To begin with, execute the following one-time setup command:</p> <pre><code>starboard install\n</code></pre> <p>The <code>install</code> subcommand creates the <code>starboard</code> namespace, in which Starboard executes Kubernetes jobs to perform scans. It also sends custom security resources definitions to the Kubernetes API and creates default configuration objects:</p> <pre><code>kubectl api-resources --api-group aquasecurity.github.io\n</code></pre> Result <pre><code>NAME                             SHORTNAMES                 APIVERSION                        NAMESPACED   KIND\nciskubebenchreports              kubebench                  aquasecurity.github.io/v1alpha1   false        CISKubeBenchReport\nclustercompliancedetailreports   compliancedetail           aquasecurity.github.io/v1alpha1   false        ClusterComplianceDetailReport\nclustercompliancereports         compliance                 aquasecurity.github.io/v1alpha1   false        ClusterComplianceReport\nclusterconfigauditreports        clusterconfigaudit         aquasecurity.github.io/v1alpha1   false        ClusterConfigAuditReport\nclustervulnerabilityreports      clustervuln,clustervulns   aquasecurity.github.io/v1alpha1   false        ClusterVulnerabilityReport\nconfigauditreports               configaudit                aquasecurity.github.io/v1alpha1   true         ConfigAuditReport\nkubehunterreports                kubehunter                 aquasecurity.github.io/v1alpha1   false        KubeHunterReport\nvulnerabilityreports             vuln,vulns                 aquasecurity.github.io/v1alpha1   true         VulnerabilityReport\n</code></pre> <p>Tip</p> <p>There's also a <code>starboard uninstall</code> subcommand, which can be used to remove all resources created by Starboard.</p> <p>As an example let's run in the current namespace an old version of <code>nginx</code> that we know has vulnerabilities:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>Run the vulnerability scanner to generate vulnerability reports:</p> <pre><code>starboard scan vulnerabilityreports deployment/nginx\n</code></pre> <p>Behind the scenes, by default this uses Trivy in Standalone mode to identify vulnerabilities in the container images associated with the specified Deployment. Once this has been done, you can retrieve the latest vulnerability reports for this workload:</p> <pre><code>starboard get vulnerabilityreports deployment/nginx -o yaml\n</code></pre> <p>For a Deployment with N containers Starboard will create N instances of <code>vulnerabilityreports.aquasecurity.github.io</code> resources. To retrieve a vulnerability report for the specified container use the <code>--container</code> flag:</p> <pre><code>starboard get vulnerabilityreports deployment/nginx --container nginx -o yaml\n</code></pre> <p>Tip</p> <p>It is possible to retrieve vulnerability reports with the <code>kubectl get</code> command, but it requires knowledge of Starboard implementation details. In particular, naming convention and labels and label selectors used to associate vulnerability reports with Kubernetes workloads.</p> <pre><code>$ kubectl get vulnerabilityreports -o wide\nNAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     41m   21         50     34       104   0\n</code></pre> <p>To read more about custom resources and label selectors check Custom Resource Definitions.</p> <p>Moving forward, let's take the same <code>nginx</code> Deployment and audit its Kubernetes configuration. As you remember we've created it with the <code>kubectl create deployment</code> command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure.</p> <p>Run the scanner to audit the configuration using the built-in configuration checker:</p> <pre><code>starboard scan configauditreports deployment/nginx\n</code></pre> <p>Retrieve the configuration audit report:</p> <pre><code>starboard get configauditreports deployment/nginx -o yaml\n</code></pre> <p>or</p> <pre><code>kubectl get configauditreport -o wide\n</code></pre> Result <pre><code>NAME                          SCANNER     AGE   CRITICAL  HIGH   MEDIUM   LOW\nreplicaset-nginx-78449c65d4   Starboard   75s   0         0      6        7\n</code></pre>"},{"location":"cli/getting-started/#generating-html-reports","title":"Generating HTML Reports","text":"<p>Once you scanned the <code>nginx</code> Deployment for vulnerabilities and checked its configuration you can generate an HTML report of identified risks and open it in your web browser:</p> <pre><code>starboard report deployment/nginx &gt; nginx.deploy.html\n</code></pre> <pre><code>open nginx.deploy.html\n</code></pre> <p></p>"},{"location":"cli/getting-started/#whats-next","title":"What's Next?","text":"<ul> <li>Learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter, by running   <code>starboard help</code>.</li> <li>Read up on Infrastructure Scanners integrated with Starboard.</li> </ul>"},{"location":"cli/troubleshooting/","title":"Troubleshooting the Starboard CLI","text":"<p>Feel free to either open an issue, reach out on Slack, or post your questions in the discussion forum.</p>"},{"location":"cli/troubleshooting/#starboard-cannot-be-opened-because-the-developer-cannot-be-verified-macos","title":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS)","text":"<p>Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released.</p> <p></p> <p>To override your security settings and use the Starboard CLI anyway, follow these steps:</p> <ol> <li>In the Finder on your Mac, locate the <code>starboard</code> binary.</li> <li>Control-click the binary icon, then choose Open from the shortcut menu.</li> <li>Click Open.</li> </ol> <p></p> <p>The <code>starboard</code> is saved as an exception to your security settings, and you can use it just as you can any registered    app.</p> <p>You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security &amp; Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command.</p> <p>To open this pane on your Mac, choose Apple menu &gt; System Preferences, click Security &amp; Privacy, then click General.</p> <p></p>"},{"location":"cli/installation/binary-releases/","title":"From the Binary Releases","text":"<p>Every release of Starboard provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed.</p> <ol> <li>Download desired release archive for your platform</li> <li>Unpack it. As an example for macOS platform, run the following command:    <pre><code>tar -zxvf starboard_darwin_x86_64.tar.gz\n</code></pre></li> <li>Find the <code>starboard</code> binary in the unpacked directory, and move it to its desired destination    <pre><code>mv ./starboard /usr/local/bin/starboard\n</code></pre></li> </ol> <p>From there, you should be able to run Starboard CLI commands: <code>starboard help</code></p>"},{"location":"cli/installation/binary-releases/#kubectl-plugin","title":"kubectl plugin","text":"<p>The Starboard CLI is compatible with kubectl and is intended as kubectl plugin, but it's perfectly fine to run it as a stand-alone executable. If you rename the <code>starboard</code> executable to <code>kubectl-starboard</code> and if it's in your path, you can invoke it using <code>kubectl starboard</code>.</p>"},{"location":"cli/installation/docker/","title":"Docker","text":"<p>We also release Docker images <code>aquasec/starboard:0.15.25-2-gab715b7</code> and <code>public.ecr.aws/aquasecurity/starboard:0.15.25-2-gab715b7</code> to run Starboard as a Docker container or to manually schedule Kubernetes scan Jobs in your cluster.</p> <pre><code>$ docker container run --rm public.ecr.aws/aquasecurity/starboard:0.15.25-2-gab715b7 version\nStarboard Version: {Version:0.15.25-2-gab715b7 Commit:ab715b7752625ad4512606342c02f3e78495a6ab Date:2025-06-25T14:50:45+05:30}\n</code></pre>"},{"location":"cli/installation/krew/","title":"Krew","text":"<p>You can also install Starboard as a kubectl plugin with the Krew plugins manager:</p> <pre><code>kubectl krew install starboard\nkubectl starboard help\n</code></pre> <p>If you already installed Starboard plugin with Krew, you can upgrade it to the latest version with:</p> <pre><code>kubectl krew upgrade starboard\n</code></pre>"},{"location":"cli/installation/source/","title":"From Source (Linux, macOS)","text":"<p>Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of Starboard.</p> <p>You must have a working Go environment.</p> <pre><code>git clone --depth 1 --branch v0.15.25-2-gab715b7 git@github.com:aquasecurity/starboard.git\ncd starboard\nmake\n</code></pre> <p>If required, it will fetch the dependencies and cache them. It will then compile <code>starboard</code> and place it in <code>bin/starboard</code>.</p>"},{"location":"compliance/nsa-1.0/","title":"National Security Agency","text":"<p>NSA, CISA Kubernetes Hardening Guidance v1.0 cybersecurity technical report is produced by starboard and validate the following control checks :</p> NAME DESCRIPTION KINDS Non-root containers Check that container is not running as root Workload Immutable container file systems Check that container root file system is immutable Workload Preventing privileged containers Controls whether Pods can run privileged containers Workload Share containers process namespaces Controls whether containers can share process namespaces Workload Share host process namespaces Controls whether share host process namespaces Workload Use the host network Controls whether containers can use the host network Workload Run with root privileges or with root group membership Controls whether container applications can run with root privileges or with root group membership Workload Restricts escalation to root privileges Control check restrictions escalation to root privileges Workload Sets the SELinux context of the container Control checks if pod sets the SELinux context of the container Workload Restrict a container's access to resources with AppArmor Control checks the restriction of containers access to resources with AppArmor Workload Sets the seccomp profile used to sandbox containers Control checks the sets the seccomp profile used to sandbox containers Workload Protecting Pod service account tokens Control check whether disable secret token been mount ,automountServiceAccountToken: false Node Namespace kube-system should not be used by users Control check whether Namespace kube-system is not be used by users NetworkPolicy Pod and/or namespace Selectors usage Control check validate the pod and/or namespace Selectors usage NetworkPolicy Use CNI plugin that supports NetworkPolicy API Control check whether check cni plugin installed Node Use ResourceQuota policies to limit resources Control check the use of ResourceQuota policy to limit aggregate resource usage within namespace ResourceQuota Use LimitRange policies to limit resources Control check the use of LimitRange policy limit resource usage for namespaces or nodes LimitRange Control plan disable insecure port Control check whether control plan disable insecure port Node Encrypt etcd communication Control check whether etcd communication is encrypted Node Ensure kube config file permission Control check whether kube config file permissions Node Check that encryption resource has been set Control checks whether encryption resource has been set Node Check encryption provider Control checks whether encryption provider has been set Node Make sure anonymous-auth is unset Control checks whether anonymous-auth is unset Node Make sure -authorization-mode=RBAC Control check whether RBAC permission is in use Node Audit policy is configure Control check whether audit policy is configure Node Audit log path is configure Control check whether audit log path is configure Node Audit log aging Control check whether audit log aging is configure Node <p>NSA, CISA Kubernetes Hardening Guidance v1.0 report will be generated every three hours by default.</p> <p>The NSA compliance report is composed of two parts :</p> <ul> <li> <p><code>spec</code>: represents the NSA compliance control checks specification, check details, and the mapping to the security scanner</p> </li> <li> <p><code>status</code>: represents the NSA compliance control checks results</p> </li> </ul> <p>Spec can be customized by amending the control checks <code>severity</code> or <code>cron</code> expression (report execution interval). As an example, let's enter <code>vi</code> edit mode and change the <code>cron</code> expression. <pre><code>kubectl edit compliance\n</code></pre> Once the report has been generated, you can fetch and review its results section. As an example, let's fetch the compliance status report in JSON format</p> <pre><code>kubectl get compliance nsa  -o=jsonpath='{.status}' | jq .\n</code></pre> <p>If failures are found in the NSA report and additional investigation is required, you can fetch the nsa-details report for advance investigation. As an example, let's fetch the report in JSON format <pre><code>kubectl get compliancedetail nsa-details -o json\n</code></pre></p>"},{"location":"configuration-auditing/","title":"Configuration Auditing","text":"<p>As your organization deploys containerized workloads in Kubernetes environments, you will be faced with many configuration choices related to images, containers, control plane, and data plane. Setting these configurations improperly creates a high-impact security and compliance risk. DevOps, and platform owners need the ability to continuously assess build artifacts, workloads, and infrastructure against configuration hardening standards to remediate any violations.</p> <p>Starboard configuration audit capabilities are purpose-built for Kubernetes environments. In particular, Starboard Operator continuously checks images, workloads, and Kubernetes infrastructure components against common configurations security standards and generates detailed assessment reports, which are then stored in the default Kubernetes database.</p> <p>Kubernetes applications and other core configuration objects, such as Ingress, NetworkPolicy, ResourceQuota, RBAC resources, are evaluated against Built-in Policies. Beyond that, cluster nodes are constantly assessed against the CIS Kubernetes Benchmarks with the kube-bench Infrastructure Scanner. The results of all these scans are stored as ConfigAuditReport, ClusterConfigAuditReport, and CISKubeBenchReport resources, which could be further aggregated into a ClusterComplianceReport such as NSA, CISA Kubernetes Hardening Guidance.</p> <p>Additionally, application and infrastructure owners can integrate these reports into incident response workflows for active remediation.</p>"},{"location":"configuration-auditing/built-in-policies/","title":"Built-in Configuration Audit Policies","text":"<p>The following sections list built-in configuration audit policies installed with Starboard. They are stored in the <code>starboard-policies-config</code> ConfigMap created in the installation namespace (e.g. <code>starboard-system</code>). You can modify them or add a new policy. For example, follow the Writing Custom Configuration Audit Policies tutorial to add a custom policy that checks for recommended Kubernetes labels on any resource kind.</p>"},{"location":"configuration-auditing/built-in-policies/#general","title":"General","text":"NAME DESCRIPTION KINDS CPU not limited Enforcing CPU limits prevents DoS via resource exhaustion. Workload CPU requests not specified When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload SYS_ADMIN capability added SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root. Workload Default capabilities not dropped The container should drop all default capabilities and add only those that are needed for its execution. Workload Root file system is not read-only An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk. Workload Memory not limited Enforcing memory limits prevents DoS via resource exhaustion. Workload Memory requests not specified When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload hostPath volume mounted with docker.sock Mounting docker.sock from the host can give the container full root access to the host. Workload Runs with low group ID Force the container to run with group ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload Runs with low user ID Force the container to run with user ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload Tiller Is Deployed Check if Helm Tiller component is deployed. Workload Image tag ':latest' used It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version. Workload"},{"location":"configuration-auditing/built-in-policies/#advanced","title":"Advanced","text":"NAME DESCRIPTION KINDS Unused capabilities should be dropped (drop any) Security best practices require containers to run with minimal required capabilities. Workload hostAliases is set Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod\u2019s containers have already been started. Workload User Pods should not be placed in kube-system namespace ensure that User pods are not placed in kube-system namespace Workload Protecting Pod service account tokens ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false Workload Selector usage in network policies ensure that network policies selectors are applied to pods or namespaces to restricted ingress and egress traffic within the pod network NetworkPolicy limit range usage ensure limit range policy has configure in order to limit resource usage for namespaces or nodes LimitRange resource quota usage ensure resource quota policy has configure in order to limit aggregate resource usage within namespace ResourceQuota All container images must start with the *.azurecr.io domain Containers should only use images from trusted registries. Workload All container images must start with a GCR domain Containers should only use images from trusted GCR registries. Workload"},{"location":"configuration-auditing/built-in-policies/#pod-security-standard","title":"Pod Security Standard","text":""},{"location":"configuration-auditing/built-in-policies/#baseline","title":"Baseline","text":"NAME DESCRIPTION KINDS Access to host IPC namespace Sharing the host\u2019s IPC namespace allows container processes to communicate with processes on the host. Workload Access to host network Sharing the host\u2019s network namespace permits processes in the pod to communicate with processes bound to the host\u2019s loopback adapter. Workload Access to host PID Sharing the host\u2019s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration. Workload Privileged container Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges. Workload Non-default capabilities added Adding NET_RAW or capabilities beyond the default set must be disallowed. Workload hostPath volumes mounted HostPath volumes must be forbidden. Workload Access to host ports HostPorts should be disallowed, or at minimum restricted to a known list. Workload Default AppArmor profile not set A program inside the container can bypass AppArmor protection policies. Workload SELinux custom options set Setting a custom SELinux user or role option should be forbidden. Workload Non-default /proc masks set The default /proc masks are set up to reduce attack surface, and should be required. Workload Unsafe sysctl options set Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed 'safe' subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node. Workload"},{"location":"configuration-auditing/built-in-policies/#restricted","title":"Restricted","text":"NAME DESCRIPTION KINDS Non-ephemeral volume types used In addition to restricting HostPath volumes, usage of non-ephemeral volume types should be limited to those defined through PersistentVolumes. Workload Process can elevate its own privileges A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node. Workload Runs as root user 'runAsNonRoot' forces the running image to run as a non-root user to ensure least privileges. Workload A root primary or supplementary GID set Containers should be forbidden from running with a root primary or supplementary GID. Workload Default Seccomp profile not set The RuntimeDefault seccomp profile must be required, or allow specific additional profiles. Workload"},{"location":"configuration-auditing/infrastructure-scanners/","title":"Infrastructure Scanners","text":"<p>Currently, these are the tools for infrastructure checking in Kubernetes:</p> <ul> <li>CIS benchmark for Kubernetes nodes provided by kube-bench.</li> <li>Penetration test results for a Kubernetes cluster provided by kube-hunter.</li> </ul>"},{"location":"configuration-auditing/infrastructure-scanners/#kube-bench","title":"Kube-bench","text":"<p>The CIS benchmark for Kubernetes provides prescriptive guidance for system and application administrators, security specialists, auditors, help desk, and platform deployment personnel who are responsible for establishing secure configuration for solutions that incorporate Kubernetes.</p> <p>Starboard Operator automatically discovers cluster nodes and checks their configuration against CIS benchmarks. With Starboard CLI you can use the <code>scan ciskubebenchreports</code> command:</p> <pre><code>starboard scan ciskubebenchreports\n</code></pre> <p>In both cases, results are stored as CISKubeBenchReport resources controller by the corresponding cluster node.</p> <p>If everything goes fine, list benchmark results with the <code>kubectl get</code> command:</p> <pre><code>$ kubectl get ciskubebenchreports -o wide\nNAME                 SCANNER      AGE   FAIL   WARN   INFO   PASS\nkind-control-plane   kube-bench   13s   11     43     0      69\nkind-worker          kube-bench   14s   1      29     0      19\nkind-worker2         kube-bench   14s   1      29     0      19\n</code></pre> <p>With Starboard CLI it is also possible to generate a CIS Benchmark HTML report and open it in your web browser:</p> <p><pre><code>starboard report nodes/kind-control-plane &gt; kind-control-plane-report.html\n</code></pre> <pre><code>open kind-control-plane-report.html\n</code></pre></p> <p></p>"},{"location":"configuration-auditing/infrastructure-scanners/#kube-hunter","title":"Kube-hunter","text":"<p>Kube-hunter hunts for security weaknesses in Kubernetes clusters. It was developed to increase awareness and visibility for security issues in Kubernetes environments.</p> <p>Tip</p> <p>Kube-hunter is only integrated with Starboard CLI.</p> <p>To run kube-hunter in your cluster as a Pod use the following command:</p> <pre><code>starboard scan kubehunterreports\n</code></pre> <p>If everything goes well, you can retrieve the penetration test report with the <code>kubectl get</code> command:</p> <pre><code>$ kubectl get kubehunterreports -o wide\nNAME      SCANNER       AGE   HIGH   MEDIUM   LOW\ncluster   kube-hunter   27h   0      0        1\n</code></pre>"},{"location":"configuration-auditing/infrastructure-scanners/#whats-next","title":"What's Next?","text":"<ul> <li>See how Starboard Operator can automate Infrastructure Scanning with kube-bench.</li> <li>Watch the video where we demonstrated Automating Kubernetes Compliance Checks with Starboard Operator.</li> </ul>"},{"location":"configuration-auditing/pluggable-scanners/","title":"Pluggable Scanners","text":"<p>Warning</p> <p>Pluggable configuration audit scanners are being deprecated. They did a great job when we first integrated them with Starboard, but recently we found that they are not very efficient and customizable. Therefore, we are switching over to efficient OPA Rego-based configuration scanner that ships with many Built-in Policies.</p> <p>Starboard relies on a configuration checker to run variety of tests on discovered workloads to make sure they are configured using best practices.</p> <p>You can choose any of the included configuration checkers or implement your own plugin. The plugin mechanism is based on in-tree implementations of the <code>configauditreport.Plugin</code> Go interface. For example, check the implementation of the Polaris plugin.</p> <p>These are currently integrated configuration checkers:</p> <ul> <li>Polaris by Fairwinds Ops</li> <li>Conftest by Open Policy Agent</li> </ul>"},{"location":"configuration-auditing/pluggable-scanners/#whats-next","title":"What's Next?","text":"<ul> <li>See the explanation and demo of configuration auditing with Polaris on the   Automating Configuration Auditing with Starboard Operator By Aqua blog.</li> </ul>"},{"location":"configuration-auditing/pluggable-scanners/conftest/","title":"Conftest","text":"<p>Conftest helps you write tests against structured configuration data. Using Conftest you can write tests for your Kubernetes configuration. Conftest uses the Rego language from Open Policy Agent for writing the assertions.</p> <p>Here's a simple policy that checks whether a given container runs as root:</p> <pre><code>package main\n\ndeny[res] {\n  input.kind == \"Deployment\"\n  not input.spec.template.spec.securityContext.runAsNonRoot\n\n  msg := \"Containers must not run as root\"\n\n  res := {\n    \"msg\": msg,\n    \"title\": \"Runs as root user\"\n  }\n}\n</code></pre> <p>To integrate Conftest scanner change the value of the <code>configAuditReports.scanner</code> property to <code>Conftest</code>:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>kubectl patch cm starboard -n $STARBOARD_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"configAuditReports.scanner\": \"Conftest\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>Warning</p> <p>Starboard does not ship with any default policies that can be used with Conftest plugin, therefore you have to add them manually.</p> <p>In the following example, we'll use OPA polices provided by the AppShield project.</p> <p>Start by cloning the AppShield repository and changing the current directory to the cloned repository:</p> <pre><code>git clone https://github.com/aquasecurity/appshield\ncd appshield\n</code></pre> <p>Most of the Kubernetes policies defined by the AppShield project refer to OPA libraries called kubernetes.rego and utils.rego. You must add such libraries to the <code>starboard-conftest-config</code> ConfigMap along with the actual policies.</p> <p>As an example, let's create the <code>starboard-conftest-config</code> ConfigMap with file_system_not_read_only.rego and uses_image_tag_latest.rego policies. Those two are very common checks performed by many other tools:</p> <p><pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>kubectl create configmap starboard-conftest-config --namespace $STARBOARD_NAMESPACE \\\n  --from-literal=conftest.imageRef=openpolicyagent/conftest:v0.30.0 \\\n  --from-file=conftest.library.kubernetes.rego=kubernetes/lib/kubernetes.rego \\\n  --from-file=conftest.library.utils.rego=kubernetes/lib/utils.rego \\\n  --from-file=conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego \\\n  --from-file=conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego \\\n  --from-literal=conftest.policy.file_system_not_read_only.kinds=Workload \\\n  --from-literal=conftest.policy.uses_image_tag_latest.kinds=Workload\n</code></pre></p> <p>Tip</p> <p>For the operator the Helm install command may look as follows. <pre><code>STARBOARD_NAMESPACE=&lt;starboard_namespace&gt;\n</code></pre> <pre><code>helm install starboard-operator aqua/starboard-operator \\\n  --namespace $STARBOARD_NAMESPACE --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"starboard.configAuditReportsPlugin=Conftest\" \\\n  --set-file=\"conftest.library.kubernetes\\.rego=kubernetes/lib/kubernetes.rego\" \\\n  --set-file=\"conftest.library.utils\\.rego=kubernetes/lib/utils.rego\" \\\n  --set-file=\"conftest.policy.file_system_not_read_only.rego=kubernetes/policies/general/file_system_not_read_only.rego\" \\\n  --set-file=\"conftest.policy.uses_image_tag_latest.rego=kubernetes/policies/general/uses_image_tag_latest.rego\" \\\n  --set-string=\"conftest.policy.file_system_not_read_only.kinds=Workload\" \\\n  --set-string=\"conftest.policy.uses_image_tag_latest.kinds=Workload\"\n</code></pre></p> <p>To test this setup out with Starboard CLI you can create the <code>nginx</code> Deployment with the latest <code>nginx</code> image and check its configuration:</p> <pre><code>kubectl create deployment nginx --image nginx\n</code></pre> <pre><code>starboard scan configauditreports deployment/nginx\n</code></pre> <p>Finally, inspect the ConfigAuditReport to confirm that the Deployment is not compliant with test policies:</p> <pre><code>$ starboard get configauditreports deployment/nginx -o yaml\napiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  creationTimestamp: \"2021-10-27T12:42:20Z\"\n  generation: 1\n  labels:\n    plugin-config-hash: 5d5f578dd6\n    resource-spec-hash: 7d48b6dfcf\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6799fc88d8\n    starboard.resource.namespace: default\n  name: replicaset-nginx-6799fc88d8\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: false\n    controller: true\n    kind: ReplicaSet\n    name: nginx-6799fc88d8\n    uid: cdfd93d7-9419-4e2d-a120-107bed2f3d57\n  resourceVersion: \"88048\"\n  uid: 362d5b06-65a5-4925-bf96-19d23f088e0c\nreport:\n  updateTimestamp: \"2021-10-27T12:42:20Z\"\n  scanner:\n    name: Conftest\n    vendor: Open Policy Agent\n    version: v0.25.0\n  summary:\n    dangerCount: 2\n    passCount: 0\n    warningCount: 0\n  checks:\n  - category: Security\n    checkID: Root file system is not read-only\n    message: Container 'nginx' of ReplicaSet 'nginx-6799fc88d8' should set 'securityContext.readOnlyRootFilesystem'\n      to true\n    severity: danger\n    success: false\n  - category: Security\n    checkID: Image tag ':latest' used\n    message: Container 'nginx' of ReplicaSet 'nginx-6799fc88d8' should specify an\n      image tag\n    severity: danger\n    success: false\n</code></pre> <p>Tip</p> <p>The steps for configuring Conftest with Starboard CLI and Starboard Operator are the same except the namespace in which the <code>starboard-conftest-config</code> ConfigMap is created.</p>"},{"location":"configuration-auditing/pluggable-scanners/conftest/#settings","title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>conftest.imageRef</code> <code>docker.io/openpolicyagent/conftest:v0.30.0</code> Conftest image reference <code>conftest.resources.requests.cpu</code> <code>50m</code> The minimum amount of CPU required to run Conftest scanner pod. <code>conftest.resources.requests.memory</code> <code>50M</code> The minimum amount of memory required to run Conftest scanner pod. <code>conftest.resources.limits.cpu</code> <code>300m</code> The maximum amount of CPU allowed to run Conftest scanner pod. <code>conftest.resources.limits.memory</code> <code>300M</code> The maximum amount of memory allowed to run Conftest scanner pod. <code>conftest.library.&lt;name&gt;.rego</code> N/A Rego library with helper functions <code>conftest.policy.&lt;name&gt;.rego</code> N/A Rego policy with the specified name <code>conftest.policy.&lt;name&gt;.kinds</code> N/A A comma-separated list of Kubernetes kinds applicable to the policy with a given name. You can use <code>Workload</code> or <code>*</code> as special kinds to represent any Kubernetes workload or any object."},{"location":"configuration-auditing/pluggable-scanners/polaris/","title":"Polaris","text":"<p>Polaris is the default configuration checker used by Starboard. It runs a variety of checks to ensure that Kubernetes Pods and controllers are configured using best practices.</p> <p>The default Polaris configuration can be customized to do things like:</p> <ul> <li>Turn checks on and off</li> <li>Change the severity level of checks</li> <li>Add new custom checks</li> <li>Add exemptions for particular workloads or namespaces</li> </ul>"},{"location":"configuration-auditing/pluggable-scanners/polaris/#settings","title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>polaris.imageRef</code> <code>quay.io/fairwinds/polaris:4.2</code> Polaris image reference <code>polaris.config.yaml</code> [Check the default value here][default-polaris-config] Polaris configuration file <code>polaris.resources.request.cpu</code> <code>50m</code> The minimum amount of CPU required to run Polaris scanner pod. <code>polaris.resources.request.memory</code> <code>50M</code> The minimum amount of memory required to run Polaris scanner pod. <code>polaris.resources.limit.cpu</code> <code>300m</code> The maximum amount of CPU allowed to run Polaris scanner pod. <code>polaris.resources.limit.memory</code> <code>300M</code> The maximum amount of memory allowed to run polaris scanner pod."},{"location":"configuration-auditing/pluggable-scanners/polaris/#whats-next","title":"What's Next?","text":"<ul> <li>See the Polaris documentation for the list of security, efficiency, and reliability checks.</li> </ul>"},{"location":"crds/","title":"Overview","text":"<p>This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a programmable way.</p> NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport clustervulnerabilityreports clustervulns, clustervuln aquasecurity.github.io false ClusterVulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport clusterconfigauditreports clusterconfigaudit aquasecurity.github.io false ClusterConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport clustercompliancereports compliance aquasecurity.github.io false ClusterComplianceReport clustercompliancereports comoliancedetail aquasecurity.github.io false ClusterComplianceDetailReport <p>Note</p> <p>We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations.</p>"},{"location":"crds/ciskubebench-report/","title":"CISKubeBenchReport","text":"<p>The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node.</p> <p>The following listing shows a sample CISKubeBenchReport associated with the <code>kind-control-plane</code> node.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: CISKubeBenchReport\nmetadata:\n  name: kind-control-plane\n  labels:\n    starboard.resource.kind: Node\n    starboard.resource.name: kind-control-plane\n  uid: 4aec0c8e-c98d-4b53-8727-1e22cacdb772\n  ownerReferences:\n    - apiVersion: v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: Node\n      name: kind-control-plane\n      uid: 6941ddfd-65be-4960-8cda-a4d11e53cbe9\nreport:\n  updateTimestamp: '2021-05-20T11:53:58Z'\n  scanner:\n    name: kube-bench\n    vendor: Aqua Security\n    version: 0.5.0\n  sections:\n    - id: '1'\n      node_type: master\n      tests:\n        - desc: Master Node Configuration Files\n          fail: 1\n          info: 0\n          pass: 18\n          results:\n            - remediation: &gt;\n                Run the below command (based on the file location on your\n                system) on the\n\n                master node.\n\n                For example, chmod 644\n                /etc/kubernetes/manifests/kube-apiserver.yaml\n              scored: true\n              status: PASS\n              test_desc: &gt;-\n                Ensure that the API server pod specification file permissions\n                are set to 644 or more restrictive (Automated)\n              test_number: 1.1.1\n            - remediation: &gt;\n                Run the below command (based on the file location on your\n                system) on the master node.\n\n                For example,\n\n                chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml\n              scored: true\n              status: PASS\n              test_desc: &gt;-\n                Ensure that the API server pod specification file ownership is\n                set to root:root (Automated)\n              test_number: 1.1.2\n          section: '1.1'\n          warn: 2\n      text: Master Node Security Configuration\n      total_fail: 10\n      total_info: 0\n      total_pass: 45\n      total_warn: 10\n      version: '1.6'\n  summary:\n    failCount: 11\n    infoCount: 0\n    passCount: 71\n    warnCount: 40\n</code></pre> <p>Note</p> <p>We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench.</p>"},{"location":"crds/clustercompliance-report/","title":"ClusterComplianceReport","text":"<p>The ClusterComplianceReport is a cluster-scoped resource, which represents the latest compliance control checks results. The report spec defines a mapping between pre-defined compliance control check ids to security scanners check ids. Currently, only <code>kube-bench</code> and <code>config-audit</code> security scanners are supported.</p> <p>The NSA compliance report is composed of two parts:</p> <ul> <li><code>spec:</code> represents the compliance control checks specification, check details, and the mapping to the security scanner   (this part is defined by the user)</li> <li><code>status:</code> represents the compliance control checks (as defined by spec mapping) results extracted from the security   scanners reports (this part is output by starboard)</li> </ul> <p>The following shows a sample ClusterComplianceReport NSA specification associated with the <code>cluster</code>:</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: ''\n  creationTimestamp: '2022-03-27T07:03:29Z'\n  generation: 2\n  labels:\n    app.kubernetes.io/instance: starboard-operator\n    app.kubernetes.io/managed-by: kubectl\n    app.kubernetes.io/name: starboard-operator\n    app.kubernetes.io/version: 0.15.25-2-gab715b7\n  name: nsa\n  resourceVersion: '15745'\n  uid: d11e8af1-daac-457d-96ea-45be4b043814\nspec:\n  controls:\n    - description: Check that container is not running as root\n      id: '1.0'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV012\n        scanner: config-audit\n      name: Non-root containers\n      severity: MEDIUM\n    - description: Check that container root file system is immutable\n      id: '1.1'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV014\n        scanner: config-audit\n      name: Immutable container file systems\n      severity: LOW\n    - description: Controls whether Pods can run privileged containers\n      id: '1.2'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV017\n        scanner: config-audit\n      name: Preventing privileged containers\n      severity: HIGH\n    - description: Controls whether containers can share process namespaces\n      id: '1.3'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV008\n        scanner: config-audit\n      name: Share containers process namespaces\n      severity: HIGH\n    - description: Controls whether share host process namespaces\n      id: '1.4'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV009\n        scanner: config-audit\n      name: Share host process namespaces.\n      severity: HIGH\n    - description: Controls whether containers can use the host network\n      id: '1.5'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV010\n        scanner: config-audit\n      name: use the host network\n      severity: HIGH\n    - description: Controls whether container applications can run with root privileges\n        or with root group membership\n      id: '1.6'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV029\n        scanner: config-audit\n      name: Run with root privileges or with root group membership\n      severity: LOW\n    - description: Control check restrictions escalation to root privileges\n      id: '1.7'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV001\n        scanner: config-audit\n      name: Restricts escalation to root privileges\n      severity: MEDIUM\n    - description: Control checks if pod sets the SELinux context of the container\n      id: '1.8'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV002\n        scanner: config-audit\n      name: Sets the SELinux context of the container\n      severity: MEDIUM\n    - description: Control checks the restriction of containers access to resources\n        with AppArmor\n      id: '1.9'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV030\n        scanner: config-audit\n      name: Restrict a container's access to resources with AppArmor\n      severity: MEDIUM\n    - description: Control checks the sets the seccomp profile used to sandbox containers\n      id: '1.10'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV030\n        scanner: config-audit\n      name: Sets the seccomp profile used to sandbox containers.\n      severity: LOW\n    - description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      id: '1.11'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV036\n        scanner: config-audit\n      name: Protecting Pod service account tokens\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check whether Namespace kube-system is not be used by users\n      id: '1.12'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        checks:\n          - id: KSV037\n        scanner: config-audit\n      name: Namespace kube-system should not be used by users\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check validate the pod and/or namespace Selectors usage\n      id: '2.0'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        checks:\n          - id: KSV038\n        scanner: config-audit\n      name: Pod and/or namespace Selectors usage\n      severity: MEDIUM\n    - description: \"Control check whether check cni plugin installed\\t\"\n      id: '3.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 5.3.1\n        scanner: kube-bench\n      name: Use CNI plugin that supports NetworkPolicy API\n      severity: CRITICAL\n    - defaultStatus: FAIL\n      description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      id: '4.0'\n      kinds:\n        - ResourceQuota\n      mapping:\n        checks:\n          - id: KSV040\n        scanner: config-audit\n      name: Use ResourceQuota policies to limit resources\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      id: '4.1'\n      kinds:\n        - ResourceQuota\n      mapping:\n        checks:\n          - id: KSV039\n        scanner: config-audit\n      name: Use LimitRange policies to limit resources\n      severity: MEDIUM\n    - description: Control check whether control plan disable insecure port\n      id: '5.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.19\n        scanner: kube-bench\n      name: Control plan disable insecure port\n      severity: CRITICAL\n    - description: Control check whether etcd communication is encrypted\n      id: '5.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: '2.1'\n        scanner: kube-bench\n      name: Encrypt etcd communication\n      severity: CRITICAL\n    - description: Control check whether kube config file permissions\n      id: '6.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 4.1.3\n          - id: 4.1.4\n        scanner: kube-bench\n      name: Ensure kube config file permission\n      severity: CRITICAL\n    - description: Control checks whether encryption resource has been set\n      id: '6.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.31\n          - id: 1.2.32\n        scanner: kube-bench\n      name: Check that encryption resource has been set\n      severity: CRITICAL\n    - description: Control checks whether encryption provider has been set\n      id: '6.2'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.3\n        scanner: kube-bench\n      name: Check encryption provider\n      severity: CRITICAL\n    - description: Control checks whether anonymous-auth is unset\n      id: '7.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.1\n        scanner: kube-bench\n      name: Make sure anonymous-auth is unset\n      severity: CRITICAL\n    - description: Control check whether RBAC permission is in use\n      id: '7.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.7\n          - id: 1.2.8\n        scanner: kube-bench\n      name: Make sure -authorization-mode=RBAC\n      severity: CRITICAL\n    - description: Control check whether audit policy is configure\n      id: '8.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 3.2.1\n        scanner: kube-bench\n      name: Audit policy is configure\n      severity: HIGH\n    - description: Control check whether audit log path is configure\n      id: '8.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.22\n        scanner: kube-bench\n      name: Audit log path is configure\n      severity: MEDIUM\n    - description: Control check whether audit log aging is configure\n      id: '8.2'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.23\n        scanner: kube-bench\n      name: Audit log aging\n      severity: MEDIUM\n  cron: \"* * * * *\"\n  description: National Security Agency - Kubernetes Hardening Guidance\n  name: nsa\n  version: '1.0'\nstatus:\n  controlCheck:\n    - description: Controls whether Pods can run privileged containers\n      failTotal: 0\n      id: '1.2'\n      name: Preventing privileged containers\n      passTotal: 11\n      severity: HIGH\n    - description: Controls whether containers can share process namespaces\n      failTotal: 0\n      id: '1.3'\n      name: Share containers process namespaces\n      passTotal: 11\n      severity: HIGH\n    - description: Control checks whether anonymous-auth is unset\n      failTotal: 0\n      id: '7.0'\n      name: Make sure anonymous-auth is unset\n      passTotal: 0\n      severity: CRITICAL\n    - description: Control check restrictions escalation to root privileges\n      failTotal: 6\n      id: '1.7'\n      name: Restricts escalation to root privileges\n      passTotal: 5\n      severity: MEDIUM\n    - description: Control checks the restriction of containers access to resources\n        with AppArmor\n      failTotal: 0\n      id: '1.9'\n      name: Restrict a container's access to resources with AppArmor\n      passTotal: 11\n      severity: MEDIUM\n    - description: Check that container is not running as root\n      failTotal: 9\n      id: '1.0'\n      name: Non-root containers\n      passTotal: 2\n      severity: MEDIUM\n    - description: Controls whether share host process namespaces\n      failTotal: 0\n      id: '1.4'\n      name: Share host process namespaces.\n      passTotal: 11\n      severity: HIGH\n    - description: Control checks whether encryption resource has been set\n      failTotal: 0\n      id: '6.1'\n      name: Check that encryption resource has been set\n      passTotal: 1\n      severity: CRITICAL\n    - description: \"Control check whether check cni plugin installed\\t\"\n      failTotal: 0\n      id: '3.0'\n      name: Use CNI plugin that supports NetworkPolicy API\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      failTotal: 1\n      id: '4.0'\n      name: Use ResourceQuota policies to limit resources\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether kube config file permissions\n      failTotal: 0\n      id: '6.0'\n      name: Ensure kube config file permission\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control checks whether encryption provider has been set\n      failTotal: 0\n      id: '6.2'\n      name: Check encryption provider\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether RBAC permission is in use\n      failTotal: 0\n      id: '7.1'\n      name: Make sure -authorization-mode=RBAC\n      passTotal: 0\n      severity: CRITICAL\n    - description: Check that container root file system is immutable\n      failTotal: 5\n      id: '1.1'\n      name: Immutable container file systems\n      passTotal: 6\n      severity: LOW\n    - description: Control checks if pod sets the SELinux context of the container\n      failTotal: 0\n      id: '1.8'\n      name: Sets the SELinux context of the container\n      passTotal: 11\n      severity: MEDIUM\n    - description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      failTotal: 1\n      id: '1.11'\n      name: Protecting Pod service account tokens\n      passTotal: 10\n      severity: MEDIUM\n    - description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      failTotal: 1\n      id: '4.1'\n      name: Use LimitRange policies to limit resources\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether audit log aging is configure\n      failTotal: 0\n      id: '8.2'\n      name: Audit log aging\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether Namespace kube-system is not be used by users\n      failTotal: 8\n      id: '1.12'\n      name: Namespace kube-system should not be used by users\n      passTotal: 3\n      severity: MEDIUM\n    - description: Controls whether containers can use the host network\n      failTotal: 0\n      id: '1.5'\n      name: use the host network\n      passTotal: 11\n      severity: HIGH\n    - description: Controls whether container applications can run with root privileges\n        or with root group membership\n      failTotal: 1\n      id: '1.6'\n      name: Run with root privileges or with root group membership\n      passTotal: 10\n      severity: LOW\n    - description: Control check whether audit log path is configure\n      failTotal: 0\n      id: '8.1'\n      name: Audit log path is configure\n      passTotal: 1\n      severity: MEDIUM\n    - description: Control checks the sets the seccomp profile used to sandbox containers\n      failTotal: 0\n      id: '1.10'\n      name: Sets the seccomp profile used to sandbox containers.\n      passTotal: 11\n      severity: LOW\n    - description: Control check validate the pod and/or namespace Selectors usage\n      failTotal: 1\n      id: '2.0'\n      name: Pod and/or namespace Selectors usage\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether control plan disable insecure port\n      failTotal: 0\n      id: '5.0'\n      name: Control plan disable insecure port\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether etcd communication is encrypted\n      failTotal: 0\n      id: '5.1'\n      name: Encrypt etcd communication\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether audit policy is configure\n      failTotal: 0\n      id: '8.0'\n      name: Audit policy is configure\n      passTotal: 1\n      severity: HIGH\n  summary:\n    failCount: 33\n    passCount: 113\n  updateTimestamp: '2022-03-27T07:06:00Z'\n</code></pre>"},{"location":"crds/clustercompliancedetail-report/","title":"ClusterComplianceDetailReport","text":"<p>The ClusterComplianceDetailReport is a cluster-scoped resource, which represents the latest result of the Cluster Compliance Detail report. The report data provide granular information on control checks failures that occur in <code>ClusterComplianceReport</code> for further investigation.</p> <p>The compliance detail report provides granular information insight on control check failures:</p> <ul> <li>Failing resource kind</li> <li>Name of the failing resource</li> <li>Namespace of the failing resource</li> <li>Failure error message</li> <li>Remediation</li> </ul> <p>The following listing shows a sample ClusterComplianceDetailReport for NSA specification associated with the <code>cluster</code></p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceDetailReport\nmetadata:\n  creationTimestamp: '2022-03-27T07:04:21Z'\n  generation: 6\n  name: nsa-details\n  resourceVersion: '15788'\n  uid: 9d36889d-086a-4fb3-b660-a3a3ecffe3c6\nreport:\n  controlCheck:\n    - checkResults:\n        - details:\n            - msg: ReplicaSet 'coredns-96cc4f57d' should not be set with 'kube-system' namespace\n              name: replicaset-coredns-96cc4f57d\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'coredns-5789895cd' should not be set with 'kube-system' namespace\n              name: replicaset-coredns-5789895cd\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'traefik-56c4b88c4b' should not be set with 'kube-system'\n                namespace\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'metrics-server-ff9dbcb6c' should not be set with 'kube-system'\n                namespace\n              name: replicaset-metrics-server-ff9dbcb6c\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'local-path-provisioner-84bb864455' should not be set with\n                'kube-system' namespace\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: ReplicaSet\n        - details:\n            - msg: DaemonSet 'svclb-traefik' should not be set with 'kube-system' namespace\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: DaemonSet\n        - details:\n            - msg: Job 'helm-install-traefik-crd' should not be set with 'kube-system' namespace\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Job 'helm-install-traefik' should not be set with 'kube-system' namespace\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: Job\n      description: Control check whether Namespace kube-system is not be used by users\n      id: '1.12'\n      name: Namespace kube-system should not be used by users\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: ResourceQuota\n      description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      id: '4.0'\n      name: Use ResourceQuota policies to limit resources\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'traefik' of ReplicaSet 'traefik-56c4b88c4b' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.allowPrivilegeEscalation' to false\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: pod-nginx-jr99v\n              namespace: starboard-itest\n              status: FAIL\n          id: KSV001\n          objectType: Pod\n      description: Control check restrictions escalation to root privileges\n      id: '1.7'\n      name: Restricts escalation to root privileges\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: ResourceQuota\n      description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      id: '4.1'\n      name: Use LimitRange policies to limit resources\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.readOnlyRootFilesystem' to true\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: pod-nginx-jr99v\n              namespace: starboard-itest\n              status: FAIL\n          id: KSV014\n          objectType: Pod\n      description: Check that container root file system is immutable\n      id: '1.1'\n      name: Immutable container file systems\n      severity: LOW\n    - checkResults:\n        - details:\n            - msg: ReplicaSet 'traefik-56c4b88c4b' should set 'spec.securityContext.runAsGroup',\n                'spec.securityContext.supplementalGroups[*]' and 'spec.securityContext.fsGroup'\n                to integer greater than 0\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n          id: KSV029\n          objectType: ReplicaSet\n      description: Controls whether container applications can run with root privileges\n        or with root group membership\n      id: '1.6'\n      name: Run with root privileges or with root group membership\n      severity: LOW\n    - checkResults:\n        - details:\n            - msg: Container of Pod 'nginx-jr99v' should set 'spec.automountServiceAccountToken'\n                to false\n              name: pod-nginx-jr99v\n              namespace: starboard-itest\n              status: FAIL\n          id: KSV036\n          objectType: Pod\n      description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      id: '1.11'\n      name: Protecting Pod service account tokens\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: NetworkPolicy\n      description: Control check validate the pod and/or namespace Selectors usage\n      id: '2.0'\n      name: Pod and/or namespace Selectors usage\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'starboard-operator' of ReplicaSet 'starboard-operator-7cf866c47b'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-starboard-operator-7cf866c47b\n              namespace: starboard-system\n              status: FAIL\n            - msg: Container 'coredns' of ReplicaSet 'coredns-96cc4f57d' should set 'securityContext.runAsNonRoot'\n                to true\n              name: replicaset-coredns-96cc4f57d\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'coredns' of ReplicaSet 'coredns-5789895cd' should set 'securityContext.runAsNonRoot'\n                to true\n              name: replicaset-coredns-5789895cd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'starboard-operator' of ReplicaSet 'starboard-operator-c94dd56d'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-starboard-operator-c94dd56d\n              namespace: starboard-system\n              status: FAIL\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.runAsNonRoot'\n                to true\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.runAsNonRoot'\n                to true\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.runAsNonRoot'\n                to true\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.runAsNonRoot'\n                to true\n              name: pod-nginx-jr99v\n              namespace: starboard-itest\n              status: FAIL\n          id: KSV012\n          objectType: Pod\n      description: Check that container is not running as root\n      id: '1.0'\n      name: Non-root containers\n      severity: MEDIUM\n  summary:\n    failCount: 33\n    passCount: 113\n  type:\n    description: national security agency - kubernetes hardening guidance\n    name: nsa-details\n    version: '1.0'\n  updateTimestamp: '2022-03-27T07:09:00Z'\n</code></pre>"},{"location":"crds/clusterconfigaudit-report/","title":"ClusterConfigAuditReport","text":"<p>ClusterConfigAuditReport is equivalent to ConfigAuditReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings, and CustomResourceDefinitions.</p>"},{"location":"crds/clustervulnerability-report/","title":"ClusterVulnerabilityReport","text":"<p>ClusterVulnerabilityReport has the same schema as VulnerabilityReport but different life cycle. Instances of ClusterVulnerabilityReport can be named by the container image digest and used to cache scan results at cluster scope.</p>"},{"location":"crds/configaudit-report/","title":"ConfigAuditReport","text":"<p>An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris and Conftest, against a Kubernetes object's configuration. For example, check that a given container image runs as non-root user or that a container has resource requests and limits set. Checks might relate to Kubernetes workloads and other namespaced Kubernetes objects such as Services, ConfigMaps, Roles, and RoleBindings.</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;workload-kind&gt;-&lt;workload-name&gt;</code> naming convention.</p> <p>The following listing shows a sample ConfigAuditReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6\n  namespace: default\n  labels:\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6d4cf56db6\n    starboard.resource.namespace: default\n    plugin-config-hash: 7f65d98b75\n    resource-spec-hash: 7cb64cb677\n  uid: d5cf8847-c96d-4534-beb9-514a34230302\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  updateTimestamp: '2021-05-20T12:38:10Z'\n  scanner:\n    name: Polaris\n    vendor: Fairwinds Ops\n    version: '4.2'\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 9\n    mediumCount: 0\n  checks:\n    - category: Security\n      checkID: hostPIDSet\n      messages:\n        - Host PID is not configured\n      severity: CRITICAL\n      success: true\n    - category: Security\n      checkID: hostIPCSet\n      messages:\n        - Host IPC is not configured\n      severity: CRITICAL\n      success: true\n    - category: Security\n      checkID: hostNetworkSet\n      messages:\n        - Host network is not configured\n      severity: LOW\n      success: true\n    - category: Security\n      checkID: notReadOnlyRootFilesystem\n      messages:\n        - Filesystem should be read only\n      scope:\n        type: Container\n        value: nginx\n      severity: LOW\n      success: false\n    - category: Security\n      checkID: privilegeEscalationAllowed\n      messages:\n        - Privilege escalation should not be allowed\n      scope:\n        type: Container\n        value: nginx\n      severity: CRITICAL\n      success: false\n</code></pre> <p>Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard.</p> <p>Note</p> <p>The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.</p>"},{"location":"crds/kubehunter-report/","title":"KubeHunterReport","text":"<p>The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter's output, but we can make it more generic to onboard third party pen testing tools.</p> <p>As shown in the following listing there's zero to one instances of KubeHunterReports with hardcoded name <code>cluster</code>. Since there's no built-in Kubernetes resource that represents a cluster Starboard does not set any owner reference.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: KubeHunterReport\nmetadata:\n  name: cluster\n  labels:\n    starboard.resource.kind: Cluster\n    starboard.resource.name: cluster\n  uid: 958ca06b-6393-4e44-a6a6-11ce823c94fe\nreport:\n  scanner:\n    name: kube-hunter\n    vendor: Aqua Security\n    version: 0.4.1\n  summary:\n    highCount: 0\n    lowCount: 1\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n  - avd_reference: https://avd.aquasec.com/kube-hunter/none/\n    category: Access Risk\n    description: |-\n      CAP_NET_RAW is enabled by default for pods.\n          If an attacker manages to compromise a pod,\n          they could potentially take advantage of this capability to perform network\n          attacks on other pods running on the same node\n    evidence: \"\"\n    location: Local to Pod (cf63974f-26a4-43f7-9409-44102fc75900-sl7vq)\n    severity: low\n    vid: None\n    vulnerability: CAP_NET_RAW Enabled\n</code></pre>"},{"location":"crds/vulnerability-report/","title":"VulnerabilityReport","text":"<p>An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For a multi-container workload Starboard creates multiple instances of VulnerabilityReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6-nginx\n  namespace: default\n  labels:\n    starboard.container.name: nginx\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: nginx-6d4cf56db6\n    starboard.resource.namespace: default\n    resource-spec-hash: 7cb64cb677\n  uid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  artifact:\n    repository: library/nginx\n    tag: '1.16'\n  registry:\n    server: index.docker.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.16.0\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n    - fixedVersion: 0.9.1-2+deb10u1\n      installedVersion: 0.9.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2019-20367'\n      resource: libbsd0\n      score: 9.1\n      severity: CRITICAL\n      title: ''\n      vulnerabilityID: CVE-2019-20367\n    - fixedVersion: ''\n      installedVersion: 0.6.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2018-25009'\n      resource: libwebp6\n      score: 9.1\n      severity: CRITICAL\n      title: 'libwebp: out-of-bounds read in WebPMuxCreateInternal'\n      vulnerabilityID: CVE-2018-25009\n</code></pre> <p>Note</p> <p>For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288).</p> <p>Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here.</p>"},{"location":"design/","title":"Design Documents","text":"<p>An index of various (informal) design and explanation documents that were created for different purposes. Mainly to brainstorm how Starboard works.</p> <p>NOTE This is not an official documentation of Starboard. Some design documents may be out of date.</p>"},{"location":"design/#overview","title":"Overview","text":"File Description caching_scan_result_by_repo_digest.md [DRAFT] Caching Scan Results by Image Reference design_trivy_file_system_scanner.md Scan Container Images with Trivy Filesystem Scanner design_vulnerability_scan_in_same_ns.md Schedule vulnerability scan jobs in the same namespace as scanned workload design_scan_by_image_digest.png Design of vulnerability scanning by image digest (ContainerStatus vs PodSpec). design_starboard_at_scale.png Design of Starboard Operator at scale with more efficient worker queue. design_vulnerability_scanning_2.0.png Design of most efficient vulnerability scanning that you can imagine. explain_starboard_rescan_jitter.png Explain a preferred way to rescan (evenly distributed vs bursty events). explain_starboard_cli_init.png Explain which K8s API object are created when the <code>starboard init</code> command is executed. design_namespace_security_report.pdf Design of a security report generated by Starboard CLI for a given namespace."},{"location":"design/caching_scan_results_by_repo_digest/","title":"[DRAFT] Caching Scan Results by Image Reference","text":""},{"location":"design/caching_scan_results_by_repo_digest/#tldr","title":"TL;DR;","text":"<p>To find vulnerabilities in container images Starboard creates asynchronous Kubernetes (K8s) Jobs. Even though running a vulnerability scanner as a K8s Job is expensive, Starboard does not reuse scan results in any way. For example, if a workload refers to the image that has already been scanned, Starboard will go ahead and create another (similar) K8s Job.</p> <p>To some extent, the problem of wasteful and long-running K8s Jobs can be mitigated by using Starboard with Trivy in the ClientServer mode instead of the default Standalone mode. In this case a configured Trivy server will cache results of scanning image layers. However, there is still unnecessary overhead for managing K8s Jobs and communication between Trivy client and server. (The only real difference is that some Jobs may complete faster for already scanned images.)</p> <p>To solve the above-mentioned problems, we could cache scan results by image reference. For example, a CRD based implementation can store scan results as instances of ClusterVulnerabilityReport object named after a hash of the repo digest. An alternative implementation may cache vulnerability reports in an AWS S3 bucket or a similar key-value store.</p>"},{"location":"design/caching_scan_results_by_repo_digest/#example","title":"Example","text":"<p>With the proposed cluster-scoped (or global) cache, Starboard can check if the image with the specified reference has already been scanned. If yes, it will just read the corresponding ClusterVulnerabilityReport, copy its payload, and finally create an instance of a namespaced VulnerabilityReport.</p> <p>Let's consider two <code>nginx:1.16</code> Deployments in two different namespaces <code>foo</code> and <code>bar</code>. In the current implementation Starboard will spin up two K8s Jobs to run a scanner and eventually create two VulnerabilityReports in <code>foo</code> and <code>bar</code> namespaces respectively.</p> <p>In a cluster where Starboard is installed for the first time, when we scan the <code>nginx</code> Deployment in the <code>foo</code> namespace there's obviously no ClusterVulnerabilityReport for <code>nginx:1.16</code>. Therefore, Starboard will spin up a K8s Job and wait for its completion. On completion, it will create a cluster-scoped ClusterVulnerabilityReport named after the hash of <code>nginx:1.16</code>. It will also create a namespaced VulnerabilityReport named after the current revision of the <code>nginx</code> Deployment.</p> <p>NOTE Because a repo digest is not a valid name for a K8s API object, we may, for example, calculate a (safe) hash of the repo digest and use is as name instead.</p> <pre><code>$ kubectl get clustervulnerabilityreports\nNo resources found\n</code></pre> <pre><code>$ starboard scan vulnerabilityreports deploy/nginx -n foo -v 3\nI1008 19:58:19.355462   62385 scanner.go:72] Getting Pod template for workload: {Deployment nginx foo}\nI1008 19:58:19.358802   62385 scanner.go:89] Checking if images were already scanned\nI1008 19:58:19.360411   62385 scanner.go:95] Cached scan reports: 0\nI1008 19:58:19.360421   62385 scanner.go:101] Scanning with options: {ScanJobTimeout:0s DeleteScanJob:true}\nI1008 19:58:19.365155   62385 runner.go:79] Running task and waiting forever\nI1008 19:58:19.365190   62385 runnable_job.go:74] Creating job \"starboard/scan-vulnerabilityreport-cbf8c9b99\"\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Event (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376920   62385 reflector.go:255] Listing and watching *v1.Event from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Job (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376937   62385 reflector.go:255] Listing and watching *v1.Job from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.386049   62385 runnable_job.go:130] Event: Created pod: scan-vulnerabilityreport-cbf8c9b99-4nzkb (SuccessfulCreate)\nI1008 19:58:51.243554   62385 runnable_job.go:130] Event: Job completed (Completed)\nI1008 19:58:51.247251   62385 runnable_job.go:109] Stopping runnable job on task completion with status: Complete\nI1008 19:58:51.247273   62385 runner.go:83] Stopping runner on task completion with error: &lt;nil&gt;\nI1008 19:58:51.247278   62385 scanner.go:130] Scan job completed: starboard/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.247297   62385 scanner.go:262] Getting logs for nginx container in job: starboard/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.674449   62385 scanner.go:123] Deleting scan job: starboard/scan-vulnerabilityreport-cbf8c9b99\n</code></pre> <p>Now, if we scan the <code>nginx</code> Deployment in the <code>bar</code> namespace, Starboard will see that there's already a ClusterVulnerabilityReport (<code>84bcb5cd46</code>) for the same image reference <code>nginx:1.16</code> and will skip creation of a K8s Job. It will just read and copy the report as VulnerabilityReport object to the <code>bar</code> namespace.</p> <pre><code>$ kubectl get clustervulnerabilityreports -o wide\nNAME         REPOSITORY      TAG    DIGEST   SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\n84bcb5cd46   library/nginx   1.16            Trivy     17s   21         50     33       104   0\n</code></pre> <pre><code>$ starboard scan vulnerabilityreports deploy/nginx -n bar -v 3\nI1008 19:59:23.891718   62478 scanner.go:72] Getting Pod template for workload: {Deployment nginx bar}\nI1008 19:59:23.895310   62478 scanner.go:89] Checking if image nginx:1.16 was already scanned\nI1008 19:59:23.903058   62478 scanner.go:95] Cache hit\nI1008 19:59:23.903078   62478 scanner.go:97] Copying ClusterVulnerabilityReport to VulnerabilityReport\n</code></pre> <p>As you can see, Starboard eventually created two VulnerabilityReports by spinning up only one K8s Job.</p> <pre><code>$ kubectl get vulnerabilityreports -A\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     5m38s\nfoo         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     6m10s\n</code></pre>"},{"location":"design/caching_scan_results_by_repo_digest/#life-cycle-management","title":"Life-cycle management","text":"<p>Just like any other cache it's very important that it's up to date and contains the correct information. To make sure of this we need to have a automated way of automatically cleaning up the ClusterVulnerabilityReport after some time.</p> <p>My suggestion is to solve this problem just like we did in PR #879. For each ClusterVulnerabilityReport created we should annotate the report with <code>starboard.aquasecurity.github.io/cluster-vulnerability-report-ttl</code>. When the TTL ends the other controller will automatically delete the existing ClusterVulnerabilityReport and the next time the image is created in the cluster and normal vulnerabilityreport scan will happen.</p> <p>I suggest that we have a default value of 72 hours for this report. This is a new feature and I don't see why we shouldn't enable it by default.</p>"},{"location":"design/caching_scan_results_by_repo_digest/#vulnerability-reports","title":"Vulnerability reports","text":"<p>From a vulnerability reports point of view we need to have a simple way for cluster admins to know if the vulnerability report is generated from a cache and if so which one?</p> <p>We could ether do this by setting a status on the vulnerability report that gets created but since this feature won't be on by default I suggest we use annotations.</p> <p>For example: <code>starboard.aquasecurity.github.io/ClusterVulnerabilityReportName: 84bcb5cd46</code> would make it easy to find. We can't use something like ownerReference since it would delete all vulnerabilities at the same time if a ClusterVulnerabilityReport was deleted.</p>"},{"location":"design/caching_scan_results_by_repo_digest/#summary","title":"Summary","text":"<ul> <li>This solution might be the first step towards more efficient vulnerability scanning.</li> <li>It's backward compatible and can be implemented as an experimental feature behind   a gate.</li> <li>Both Starboard CLI and Starboard Operator can read and leverage ClusterVulnerabilityReports.</li> </ul>"},{"location":"design/design_associating_rego_policies_with_k8s_resources/","title":"Associating Rego Policies with Kubernetes Resources","text":""},{"location":"design/design_associating_rego_policies_with_k8s_resources/#overview","title":"Overview","text":"<p>Starboard (with Conftest plugin) evaluates all policies on a given Kubernetes (K8s) resource, which is not efficient for two reasons:</p> <ol> <li>Starboard creates a scan Job to audit a K8s resource even if there are no Rego policies defined for its kind.</li> <li>Starboard rescans all K8s resources even if the change in Rego policies is only related to a particular kind.</li> </ol>"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#solution","title":"Solution","text":""},{"location":"design/design_associating_rego_policies_with_k8s_resources/#tldr","title":"TL;DR;","text":"<p>Extend the configuration of the Conftest plugin to include information about K8s resource kinds (or GVK). This would allow us to:</p> <ol> <li>Group and filter Rego policies for a given resource kind to pass only relevant policies to a scan Job. In particular,    skip creation of a scan Job for a given resource if there are no policies for its kind.</li> <li>Calculate plugin config hash for a given resource kind to enable efficient rescanning by deleting only a subset of    ConfigAuditReports and ClusterConfigAuditReports.</li> </ol>"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#deep-dive","title":"Deep Dive","text":"<p>In the following example we'll consider a set of Rego policies that are applicable to different kinds of resources.</p> <ul> <li>K8s workload (Pod, ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob)<ul> <li><code>file_system_not_read_only.rego</code></li> <li><code>uses_image_tag_latest.rego</code></li> </ul> </li> <li>ConfigMap<ul> <li><code>configmap_with_sensitive_data.rego</code></li> <li><code>configmap_with_secret_data.rego</code></li> </ul> </li> <li>Service<ul> <li><code>service_with_external_ip.rego</code></li> </ul> </li> <li>Any<ul> <li><code>object_without_recommended_labels.rego</code></li> </ul> </li> </ul> <p>There are also two modules with helper functions used throughout Rego policies.</p> <ul> <li><code>kubernetes.rego</code></li> <li><code>utils.rego</code></li> </ul> <p>This is how we represent the example Rego policies as a Conftest configuration object. As you can see there's no mapping between Rego policy and applicable kinds. It's also hard to distinguish helpers from regular policies.</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  namespace: starboard-operator\n  name: starboard-conftest-config\ndata:\n  conftest.imageRef: openpolicyagent/conftest:v0.30.0\n  conftest.resources.requests.cpu: 50\n  conftest.resources.requests.memory: 50M\n  conftest.resources.limits.cpu: 300m\n  conftest.resources.limits.memory: 300M\n\n  conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  conftest.policy.uses_image_tag_latest.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  conftest.policy.service_with_external_ip.rego: \"{REGO CODE}\"\n  conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n\n  conftest.policy.kubernetes.rego: \"{REGO CODE}\"\n  conftest.policy.utils.rego: \"{REGO CODE}\"\n</code></pre> <p>In the proposed solution each Rego policy code will be accompanied by the property that specifies one to many applicable kinds (GVKs). For example, adding <code>conftest.policy.file_system_not_read_only.rego</code> policy will require specifying resource kinds as a comma-separated values stored as <code>conftest.policy.file_system_not_read_only.kinds</code>.</p> <p>If a Rego policy is applicable to any K8s workload, the kind can be express as <code>Workload</code>. If a Rego policy is applicable to any K8s resource, the kind can be expressed as wildcard (<code>*</code>) character.</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  namespace: starboard-operator\n  name: starboard-conftest-config\n  annotations:\n    # Introduce a way to version configuration schema.\n    starboard.plugin.config.version: \"v2\"\ndata:\n  conftest.imageRef: openpolicyagent/conftest:v0.30.0\n  conftest.resources.requests.cpu: 50\n  conftest.resources.requests.memory: 50M\n  conftest.resources.limits.cpu: 300m\n  conftest.resources.limits.memory: 300M\n\n  conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  conftest.policy.uses_image_tag_latest.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  conftest.policy.service_with_external_ip.rego: \"{REGO CODE}\"\n  conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n\n  conftest.policy.file_system_not_read_only.kinds: \"Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob\"\n  # For each K8s workload type a config hash will be the same.\n  # Therefore, we could support a \"virtual\" kind named `Workload`.\n  conftest.policy.uses_image_tag_latest.kinds: Workload\n  conftest.policy.configmap_with_sensitive_data.kinds: ConfigMap\n  conftest.policy.configmap_with_secret_data.kinds: ConfigMap\n  conftest.policy.service_with_external_id.kinds: Service\n  # Use \"*\" to apply a policy to any kind.\n  conftest.policy.object_without_recommended_labels.kinds: \"*\"\n\n  # Distinguish libraries with the conftest.library.* prefix.\n  conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  conftest.library.utils.rego: \"{REGO CODE}\"\n</code></pre> <p>To reconcile K8s resources and create ConfigAuditReports we calculate two hashes based on resource spec and Conftest plugin config. These two hashes are set as <code>resource-spec-hash</code> and <code>plugin-config-hash</code> labels on each ConfigAuditReport instance. The <code>resource-spec-hash</code> is used to rescan a resource when its spec has changed (e.g. update container image tag), whereas the <code>plugin-config-hash</code> is used to rescan the resource when Conftest config has changed (e.g. add new Rego policy or edit existing one).</p> <p>:bulb: Starboard operator has a dedicated controller to watch changes to the <code>starboard-conftest-config</code> ConfigMap. Whenever there's a change the controller calculates a new hash and deletes all ConfigAuditReports, which do not have the same value of the <code>plugin-config-hash</code> label.</p> <p>Currently, we calculate <code>plugin-config-hash</code> values based off of all Rego policies by filtering configuration keys with the <code>conftest.policy.</code> prefix. In the proposed solution we'll group Rego policies by resource kind and then calculate <code>N</code> hashes, where <code>N</code> is the number of different kinds. For example, a ConfigAuditReport associated with a Service will have the <code>plugin-config-hash</code> label calculated based off of policies that are only applicable to Services, i.e. <code>service_with_external_id.rego</code>, <code>object_without_recommended_labels.rego</code>, <code>kubernetes.rego</code>, and <code>utils.rego</code>.</p> <p>The following snippet shows which configuration keys and corresponding values (Rego code) will be considered to calculate the plugin config hash for a specified kind.</p> <pre><code>ConfigMap:\n  - conftest.policy.configmap_with_sensitive_data.rego: \"{REGO CODE}\"\n  - conftest.policy.configmap_with_secret_data.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  # Helper Rego functions may change the logic of any Rego policy\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nService:\n  - conftest.policy.service_with_external_id.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nWorkload:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nPod:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nReplicationController:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nReplicaSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nStatefulSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nDaemonSet:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nJob:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\nCronJob:\n  - conftest.policy.file_system_not_read_only.rego: \"{REGO CODE}\"\n  - conftest.policy.uses_imag_tag_latest.rego: \"{REGO CODE}\"\n  - conftest.policy.object_without_recommended_labels.rego: \"{REGO CODE}\"\n  - conftest.library.kubernetes.rego: \"{REGO CODE}\"\n  - conftest.library.utils.rego: \"{REGO CODE}\"\n</code></pre>"},{"location":"design/design_associating_rego_policies_with_k8s_resources/#scenarios","title":"Scenarios","text":"<p>:bulb: Scenarios in this section are written in Gherkin.</p> <pre><code>Feature: Reconcile Kubernetes resources for configuration auditing\n\n  These scenarios are applicable to ConfigAuditReports and ClusterConfigAuditReports.\n  The only difference is the scope of the resource, i.e. namespace vs cluster.\n\n  Scenario: Scan a K8s resource when there are applicable Rego policies\n\n    Given a set of Rego policies applicable to ConfigMaps\n    When a ConfigMap is discovered by the operator\n    And there is no ConfigAuditReport associated with the ConfigMap\n    Then the operator scans the ConfigMap\n    And eventually, there is the ConfigAuditReport associated with the ConfigMap\n\n  Scenario: Skip scanning a K8s resource when there are no applicable Rego policies\n\n    Given a set of Rego policies not applicable to ConfigMaps\n    When a ConfigMap is discovered by the operator\n    And there is no ConfigAuditReport associated with the ConfigMap\n    Then operator requeues (with delay) the reconciliation key for the ConfigMap\n\n  Scenario: Delete (stale) ConfigAuditReport when applicable Rego policies are removed\n\n    Given a set of Rego policies applicable to ConfigMaps\n    And the ConfigAuditReport associated with a ConfigMap\n    When Rego policies for ConfigMaps are removed\n    Then operator deletes the ConfigAuditReport\n    And the operator requeues (without delay) the reconciliation key for the ConfigMap\n\n  Scenario: Rescan a K8s resource when applicable Rego policies are updated\n\n    Given a set of Rego policies applicable to K8s workloads and ConfigMaps\n    When Rego code applicable to ConfigMaps has changed\n    Then operator deletes ConfigAuditReports associated with ConfigMaps\n    But ConfigAuditReports associated with K8s resources other than ConfigMaps are left intact\n\n  Scenario: Rescan all K8s resources when Rego helper functions have changed\n\n    Given a set of Rego policies applicable to K8s workloads, ConfigMaps, and Services\n    When Rego code of helper functions have changed\n    Then the operator deletes all ConfigAuditReports\n</code></pre>"},{"location":"design/design_compliance_report/","title":"Support Compliance Reports","text":""},{"location":"design/design_compliance_report/#overview","title":"Overview","text":"<p>It is required to leverage starboard security tools capabilities by adding the support for building compliance reports example : NSA - Kubernetes Hardening Guidance</p>"},{"location":"design/design_compliance_report/#solution","title":"Solution","text":""},{"location":"design/design_compliance_report/#tldr","title":"TL;DR;","text":"<ul> <li>A cluster compliance resource ,nsa-1.0.yaml (example below), with spec definition only will be deployed to kubernetes cluster upon startup</li> <li>the spec definition wil include the control check , cron expression for periodical generation, and it's mapping to scanners (kube-bench and audit-config)</li> <li>a new cluster compliance reconcile loop wil be introduced to track this cluster compliance resource </li> <li>when the cluster spec is reconcile  it check if cron expression match current time , if so it generates a compliance report and update the status section with report data</li> <li>if cron expression do not match the event will be requeue until next generation time </li> <li>Two new CRDs will be introduced :</li> <li><code>ClusterComplianceReport</code> to provide summary of the compliance per control</li> <li><code>ClusterComplianceDetailReport</code> to provide more detail compliance report for further investigation</li> <li>It is assumed that all scanners (kube-bench / config-audit) are running by default all the time and producing raw data</li> </ul>"},{"location":"design/design_compliance_report/#the-spec-file","title":"The Spec file :","text":"<ul> <li>The spec will include the mapping (based on Ids) between the compliance report and tools(kube-bench and config-audit) which generate the raw data</li> <li>The spec file will be loaded from the file system</li> </ul>"},{"location":"design/design_compliance_report/#example-for-spec","title":"Example for spec  :","text":"<pre><code>---\napiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\n  name: nsa\nspec:\n  name: nsa\n  description: National Security Agency - Kubernetes Hardening Guidance\n  version: \"1.0\"\n  cron: \"0 */3 * * *\"\n  controls:\n    - name: Non-root containers\n      description: 'Check that container is not running as root'\n      id: '1.0'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV012\n      severity: 'MEDIUM'\n    - name: Immutable container file systems\n      description: 'Check that container root file system is immutable'\n      id: '1.1'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV014\n      severity: 'LOW'\n    - name: Preventing privileged containers\n      description: 'Controls whether Pods can run privileged containers'\n      id: '1.2'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV017\n      severity: 'HIGH'\n    - name: Share containers process namespaces\n      description: 'Controls whether containers can share process namespaces'\n      id: '1.3'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV008\n      severity: 'HIGH'\n    - name: Share host process namespaces.\n      description: 'Controls whether share host process namespaces'\n      id: '1.4'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV009\n      severity: 'HIGH'\n    - name: use the host network\n      description: 'Controls whether containers can use the host network'\n      id: '1.5'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV010\n      severity: 'HIGH'\n    - name:  Run with root privileges or with root group membership\n      description: 'Controls whether container applications can run with root privileges or with root group membership'\n      id: '1.6'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV029\n      severity: 'LOW'\n    - name: Restricts escalation to root privileges\n      description: 'Control check restrictions escalation to root privileges'\n      id: '1.7'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV001\n      severity: 'MEDIUM'\n    - name: Sets the SELinux context of the container\n      description: 'Control checks if pod sets the SELinux context of the container'\n      id: '1.8'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV002\n      severity: 'MEDIUM'\n    - name: Restrict a container's access to resources with AppArmor\n      description: 'Control checks the restriction of containers access to resources with AppArmor'\n      id: '1.9'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV030\n      severity: 'MEDIUM'\n    - name: Sets the seccomp profile used to sandbox containers.\n      description: 'Control checks the sets the seccomp profile used to sandbox containers'\n      id: '1.10'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV030\n      severity: 'LOW'\n    - name: Protecting Pod service account tokens\n      description: 'Control check whether disable secret token been mount ,automountServiceAccountToken: false'\n      id: '1.11'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV036\n      severity: 'MEDIUM'\n    - name: Namespace kube-system should not be used by users\n      description: 'Control check whether Namespace kube-system is not be used by users'\n      id: '1.12'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV037\n      severity: 'MEDIUM'\n    - name: Pod and/or namespace Selectors usage\n      description: 'Control check validate the pod and/or namespace Selectors usage'\n      id: '2.0'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV038\n      severity: 'MEDIUM'\n    - name: Use CNI plugin that supports NetworkPolicy API\n      description: 'Control check whether check cni plugin installed    '\n      id: '3.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 5.3.1\n      severity: 'CRITICAL'\n    - name: Use ResourceQuota policies to limit resources\n      description: 'Control check the use of ResourceQuota policies to limit resources'\n      id: '4.0'\n      kinds:\n        - ResourceQuota\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: \"&lt;check need to be added&gt;\"\n      severity: 'CRITICAL'\n    - name: Control plan disable insecure port\n      description: 'Control check whether control plan disable insecure port'\n      id: '5.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.19\n      severity: 'CRITICAL'\n    - name: Encrypt etcd communication\n      description: 'Control check whether etcd communication is encrypted'\n      id: '5.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: '2.1'\n      severity: 'CRITICAL'\n    - name: Ensure kube config file permission\n      description: 'Control check whether kube config file permissions'\n      id: '6.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 4.1.3\n          - id: 4.1.4\n      severity: 'CRITICAL'\n    - name: Check that encryption resource has been set\n      description: 'Control checks whether encryption resource has been set'\n      id: '6.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.31\n          - id: 1.2.32\n      severity: 'CRITICAL'\n    - name: Check encryption provider\n      description: 'Control checks whether encryption provider has been set'\n      id: '6.2'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.3\n      severity: 'CRITICAL'\n    - name: Make sure anonymous-auth is unset\n      description: 'Control checks whether anonymous-auth is unset'\n      id: '7.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.1\n      severity: 'CRITICAL'\n    - name: Make sure -authorization-mode=RBAC\n      description: 'Control check whether RBAC permission is in use'\n      id: '7.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.7\n          - id: 1.2.8\n      severity: 'CRITICAL'\n    - name: Audit policy is configure\n      description: 'Control check whether audit policy is configure'\n      id: '8.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 3.2.1\n      severity: 'HIGH'\n    - name: Audit log path is configure\n      description: 'Control check whether audit log path is configure'\n      id: '8.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.22\n      severity: 'MEDIUM'\n    - name: Audit log aging\n      description: 'Control check whether audit log aging is configure'\n      id: '8.2'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.23\n      severity: 'MEDIUM'\n    - name: Service mesh is configure\n      description: 'Control check whether service mesh is used in cluster'\n      id: '9.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: \"&lt;check need to be added&gt;\"\n      severity: 'MEDIUM'\n  ....\n ``` \n### The logic :\nUpon starboard start cluster compliance reconcile loop will track the deployed spec file ,nsa-1.0 spec and evaluation the cron expression in spec file, \nif  the cron interval matches , starboard will generate the compliance and compliance detail reports :\n -  `ClusterComplianceReport` status section will be updated with report data \n - `ClusterComplianceDetailReport` will be generated by and saved to etcd\n\n### The mapping\nOnce it is determined that a report need to be generated:\n- all reports (cis-benchmark and audit config) raw data will be fetched by `tool` and `resource` types\n- starboard will iterate all fetched raw data and find a match by `ID`\n- once the data has been mapped and aggregated 2 type of reports will be generated to present summary\n  data and detailed data (in case further investigation need to be made)\n\n### Note: once the report has been generated again to reconcile loop start again the process describe in logic\n\n### The Reports:\n\n#### Example: Compliance spec and status section (report data)\n```json\n{\n  \"kind\": \"ClusterComplianceReport\",\n  \"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n  \"metadata\": {\n    \"name\": \"nsa\",\n    \"resourceVersion\": \"1000\",\n    \"creationTimestamp\": null\n  },\n  \"spec\": {\n    \"kind\": \"compliance\",\n    \"name\": \"nsa\",\n    \"description\": \"National Security Agency - Kubernetes Hardening Guidance\",\n    \"cron\": \"* * * * *\",\n    \"version\": \"1.0\",\n    \"controls\": [\n      {\n        \"id\": \"1.0\",\n        \"name\": \"Non-root containers\",\n        \"description\": \"\",\n        \"resources\": [\n          \"Workload\"\n        ],\n        \"mapping\": {\n          \"tool\": \"config-audit\",\n          \"checks\": [\n            {\n              \"id\": \"KSV012\"\n            }\n          ]\n        }\n      },\n      {\n        \"id\": \"8.2\",\n        \"name\": \"Audit log aging\",\n        \"description\": \"\",\n        \"resources\": [\n          \"Node\"\n        ],\n        \"mapping\": {\n          \"tool\": \"kube-bench\",\n          \"checks\": [\n            {\n              \"id\": \"1.2.23\"\n            }\n          ]\n        }\n      }\n    ]\n  },\n  \"status\": {\n    \"updateTimestamp\": \"2022-02-26T14:11:39Z\",\n    \"summary\": {\n      \"passCount\": 3,\n      \"failCount\": 3\n    },\n    \"control_check\": [\n      {\n        \"id\": \"1.1\",\n        \"name\": \"Immutable container file systems\",\n        \"passTotal\": 0,\n        \"failTotal\": 3,\n        \"severity\": \"\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"design/design_compliance_report/#compliance-details-report","title":"Compliance details report","text":"<pre><code>{\n  \"kind\": \"ClusterComplianceDetailReport\",\n  \"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n  \"metadata\": {\n    \"name\": \"nsa-details\",\n    \"resourceVersion\": \"1\"\n  },\n  \"report\": {\n    \"updateTimestamp\": \"2022-02-26T14:05:29Z\",\n    \"type\": {\n      \"kind\": \"compliance\",\n      \"name\": \"nsa-details\",\n      \"description\": \"national security agency - kubernetes hardening guidance\",\n      \"version\": \"1.0\"\n    },\n    \"summary\": {\n      \"passCount\": 3,\n      \"failCount\": 3\n    },\n    \"controlCheck\": [\n      {\n        \"id\": \"1.1\",\n        \"name\": \"Immutable container file systems\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Pod\",\n            \"id\": \"KSV014\",\n            \"remediation\": \"\",\n            \"details\": [\n              {\n                \"name\": \"pod-rss-site\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'front-end' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              },\n              {\n                \"name\": \"pod-rss-site\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'rss-reader' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              }\n            ]\n          },\n          {\n            \"objectType\": \"ReplicaSet\",\n            \"id\": \"KSV014\",\n            \"remediation\": \"\",\n            \"details\": [\n              {\n                \"name\": \"replicaset-memcached-sample-6c765df685\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'memcached' of ReplicaSet 'memcached-sample-6c765df685' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"3.0\",\n        \"name\": \"Use CNI plugin that supports NetworkPolicy API\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"5.3.1\",\n            \"remediation\": \"If the CNI plugin in use does not support network policies, consideration should be given to\\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\\nin the Kubernetes cluster.\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"warn\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"6.0\",\n        \"name\": \"Ensure kube config file permission\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"4.1.3\",\n            \"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example,\\nchmod 644 /etc/kubernetes/proxy.conf\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"pass\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"6.0\",\n        \"name\": \"Ensure kube config file permission\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"4.1.4\",\n            \"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example, chown root:root /etc/kubernetes/proxy.conf\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"pass\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"design/design_compliance_report/#the-crds","title":"The CRDs","text":""},{"location":"design/design_compliance_report/#clustercompliancereport-crd","title":"ClusterComplianceReport CRD :","text":"<ul> <li>a new CRD <code>clustercompliancereports.crd.yaml</code> will be added to include compliance check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: clustercompliancereports.aquasecurity.github.io\n  labels:\n    app.kubernetes.io/managed-by: starboard\n    app.kubernetes.io/version: \"0.14.1\"\nspec:\n  group: aquasecurity.github.io\n  scope: Cluster\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      additionalPrinterColumns:\n        - jsonPath: .metadata.creationTimestamp\n          type: date\n          name: Age\n          description: The age of the report\n        - jsonPath: .status.summary.failCount\n          type: integer\n          name: Fail\n          priority: 1\n          description: The number of checks that failed with Danger status\n        - jsonPath: .status.summary.passCount\n          type: integer\n          name: Pass\n          priority: 1\n          description: The number of checks that passed\n      schema:\n        openAPIV3Schema:\n          type: object\n          required:\n            - apiVersion\n            - kind\n            - metadata\n            - spec\n          properties:\n            apiVersion:\n              type: string\n            kind:\n              type: string\n            metadata:\n              type: object\n            spec:\n              type: object\n              required:\n                - name\n                - description\n                - version\n                - cron\n                - controls\n              properties:\n                name:\n                  type: string\n                description:\n                  type: string\n                version:\n                  type: string\n                cron:\n                  type: string\n                  pattern: '^(((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1-5]{1}){1}([0-9]{1}){1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1]{1}){1}([0-9]{1}){1}){1}|([2]{1}){1}([0-3]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))|(jan|feb|mar|apr|may|jun|jul|aug|sep|okt|nov|dec)) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-7]{1}){1}))|(sun|mon|tue|wed|thu|fri|sat)))$'\n                  description: 'cron define the intervals for report generation'\n                controls:\n                  type: array\n                  items:\n                    type: object\n                    required:\n                      - name\n                      - id\n                      - kinds\n                      - mapping\n                      - severity\n                    properties:\n                      name:\n                        type: string\n                      description:\n                        type: string\n                      id:\n                        type: string\n                        description: 'id define the control check id'\n                      kinds:\n                        type: array\n                        items:\n                          type: string\n                          description: 'kinds define the list of kinds control check apply on , example: Node,Workload '\n                      mapping:\n                        type: object\n                        required:\n                          - scanner\n                          - checks\n                        properties:\n                          scanner:\n                            type: string\n                            pattern: '^config-audit$|^kube-bench$'\n                            description: 'scanner define the name of the scanner which produce data, currently only config-audit and kube-bench are supported'\n                          checks:\n                            type: array\n                            items:\n                              type: object\n                              required:\n                                - id\n                              properties:\n                                id:\n                                  type: string\n                                  description: 'id define the check id as produced by scanner'\n                      severity:\n                        type: string\n                        description: 'define the severity of the control'\n                        enum:\n                          - CRITICAL\n                          - HIGH\n                          - MEDIUM\n                          - LOW\n                          - UNKNOWN\n            status:\n              x-kubernetes-preserve-unknown-fields: true\n              type: object\n      subresources:\n        # status enables the status subresource.\n        status: { }\n  names:\n    singular: clustercompliancereport\n    plural: clustercompliancereports\n    kind: ClusterComplianceReport\n    listKind: ClusterComplianceReportList\n    categories: [ ]\n    shortNames:\n      - compliance\n</code></pre>"},{"location":"design/design_compliance_report/#clustercompliancedetailreport-crd","title":"ClusterComplianceDetailReport CRD :","text":"<ul> <li>a new CRD <code>clustercompliancedetailreports.crd.yaml</code> will be added to include compliance detail check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: clustercompliancedetailreports.aquasecurity.github.io\n  labels:\n    app.kubernetes.io/managed-by: starboard\n    app.kubernetes.io/version: \"0.14.1\"\nspec:\n  group: aquasecurity.github.io\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      additionalPrinterColumns:\n        - jsonPath: .metadata.creationTimestamp\n          type: date\n          name: Age\n          description: The age of the report\n        - jsonPath: .report.summary.failCount\n          type: integer\n          name: Fail\n          priority: 1\n          description: The number of checks that failed with Danger status\n        - jsonPath: .report.summary.passCount\n          type: integer\n          name: Pass\n          priority: 1\n          description: The number of checks that passed\n      schema:\n        openAPIV3Schema:\n          x-kubernetes-preserve-unknown-fields: true\n          type: object\n  scope: Cluster\n  names:\n    singular: clustercompliancedetailreport\n    plural: clustercompliancedetailreports\n    kind: ClusterComplianceDetailReport\n    listKind: ClusterComplianceDetailReportList\n    categories: []\n    shortNames:\n      - compliancedetail \n</code></pre>"},{"location":"design/design_compliance_report/#permission-changes","title":"Permission changes:","text":"<p>it is required to update <code>02-starboard-operator.rbac.yaml</code> rules to include new permissions to support the following tracked resources kind by NSA plugin with (get,list and watch):</p> <p>```yaml - apiGroups: [\"networking.k8s.io\"]   resources:     - networkpolicies   verbs:     - get     - list     - watch <pre><code>```yaml\n- apiGroups:\n      - \"\"\n    resources:\n      - resourcequota\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre></p>"},{"location":"design/design_compliance_report/#nsa-tool-analysis","title":"NSA Tool Analysis","text":"Test Description Kind Tool Test Non-root containers Check that container is not running as root Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield : kubernetes/policies/pss/restricted/3_runs_as_root.rego Immutable container file systems check that container root file system is immutable Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/general/file_system_not_read_only.rego Scan container images vulnerabilities scan container for vulnerabilities and misconfiguration Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Trivy Trivy Privileged container Controls whether Pods can run privileged containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/2_privileged.rego hostIPC Controls whether containers can share host process namespaces Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_ipc.rego hostPID Controls whether containers can share host process namespaces. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_pid.rego hostNetwork Controls whether containers can use the host network. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_network.rego allowedHostPaths Limits containers to specific paths of the host file system. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest Need to be added to appshield : https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems runAsUser , runAsGroup and supplementalGroups Controls whether container applications can run with root privileges or with root group membership Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/4_runs_with_a_root_gid.rego allowPrivilegeEscalation Restricts escalation to root privileges. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/2_can_elevate_its_own_privileges.rego seLinux Sets the SELinux context of the container. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/7_selinux_custom_options_set.rego AppArmor annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/6_apparmor_policy_disabled.rego seccomp annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/5_runtime_default_seccomp_profile_not_set.rego Protecting Pod service account tokens disable secret token been mount ,automountServiceAccountToken: false Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protecting_pod_service_account_tokens.rego kube-system or kube-public namespace kube-system should should not be used by users Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protect_core_components_namespace.rego Use CNI plugin that supports NetworkPolicy API check cni plugin installed Node Kube-bench 5.3.1 Ensure that the CNI in use supports Network Policies (need to be fixed) Create policies that select Pods using podSelector and/or the namespaceSelector Create policies that select Pods using podSelector and/or the namespaceSelector Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/selector_usage_in_network_policies.rego use a default policy to deny all ingress and egress traffic check that network policy deny all exist NetworkPolicy Kube-bench Add logic to kube-bench https://kubernetes.io/docs/concepts/services-networking/network-policies/ Use LimitRange and ResourceQuota policies to limit resources on a namespace or Pod level check the resource quota resource has been define ResourceQuota Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/concepts/policy/limit-range/ TLS encryption control plan disable insecure port Node Kube-bench 1.2.19 Ensure that the --insecure-port argument is set to 0 Etcd encryption encrypt etcd communication Node Kube-bench 2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate Kubeconfig files ensure file permission Node Kube-bench 4.1.3, 4.1.4 Worker node segmentation node segmentation Node Kube-bench Note sure can be tested Encryption check that encryption resource has been set EncryptionConfiguration Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ Encryption / secrets check encryption provider Node Kube-bench 1.2.3 Ensure that the --encryption-provider-config argument is set as authentication make sure anonymous-auth is unset Node Kube-bench 1.2.1 Ensure that the --anonymous-auth argument is set to false Role-based access control make sure -authorization-mode=RBAC Node Kube-bench 1.2.7/1.2.8 Ensure that the --authorization-mode argument is not set to AlwaysAllow Audit policy file check that policy is configure Node Kube-bench 3.2.1 Ensure that a minimal audit policy is created Audit log path check that log path is configure Node Kube-bench 1.2.22 Ensure that the --audit-log-path argument is set Audit log max age check audit log aging Node Kube-bench 1.2.23 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate service mesh usage check service mesh is used in cluster Node Kube-bench Add Logic to kube-bench check service mesh existenace"},{"location":"design/design_compliance_report/#open-items","title":"Open Items","text":"<ul> <li>compliance support for CLI</li> </ul>"},{"location":"design/design_trivy_file_system_scanner/","title":"Scan Container Images with Trivy Filesystem Scanner","text":"<p>Authors: Devendra Turkar, Daniel Pacak</p>"},{"location":"design/design_trivy_file_system_scanner/#overview","title":"Overview","text":"<p>Starboard currently uses Trivy in Standalone or ClientServer mode to scan and generate VulnerabilityReports for container images by pulling the images from remote registries. Starboard scans a specified K8s workload by running the Trivy executable as a K8s Job. This approach implies that Trivy does not have access to images cached by the container runtime on cluster nodes. Therefore, to scan images from private registries Starboard reads ImagePullSecrets specified on workloads or on service accounts used by the workloads, and passes them down to Trivy executable as <code>TRIVY_USERNAME</code> and <code>TRIVY_PASSWORD</code> environment variables.</p> <p>Since ImagePullSecrets are not the only way to provide registry credential, the following alternatives are not currently supported by Starboard: 1. Pre-pulled images 2. Configuring nodes to authenticate to a private registry 3. Vendor-specific or local extension. For example, methods described on AWS ECR Private registry authentication.</p> <p>Even though we could resolve some of above-mentioned limitations with hostPath volume mounts to the container runtime socket, it would have its own disadvantages that we are trying to avoid. For example, more permissions to schedule scan Jobs and additional information about cluster's infrastructure such as location of the container runtime socket. </p>"},{"location":"design/design_trivy_file_system_scanner/#solution","title":"Solution","text":""},{"location":"design/design_trivy_file_system_scanner/#tldr","title":"TL;DR;","text":"<p>Use Trivy filesystem scanning to scan container images. The main idea, which is discussed in this proposal, is to schedule a scan Job on the same cluster node where the scanned workload. This allows Trivy to scan a filesystem of the container image which is already cached on that node without pulling the image from a remote registry. What's more, Trivy will scan container images from private registries without providing registry credentials (as ImagePullSecret or in any other proprietary way).</p>"},{"location":"design/design_trivy_file_system_scanner/#deep-dive","title":"Deep Dive","text":"<p>To scan a container image of a given K8s workload Starboard will create a corresponding container of a scan Job and override its entrypoint to invoke Trivy filesystem scanner.</p> <p>This approach requires Trivy executable to be downloaded and made available to the entrypoint. We'll do that by adding the init container to the scan Job. Such init container will use the Trivy container image to copy Trivy executable out to the emptyDir volume, which will be shared with the other containers.</p> <p>Another init container is required to download Trivy vulnerability database and save it to the mounted shared volume.</p> <p>Finally, the scan container will use shared volume with the Trivy executable and Trivy database to perform the actual filesystem scan. (See the provided Example to have a better idea of all containers defined by a scan Job and how they share data via the emptyDir volume.)</p> <p>Note that the second init container is required in Standalone mode, which is the only mode supported by Trivy filesystem scanner at the time of writing this proposal.</p> <p>We further restrict scan Jobs to run on the same node where scanned Pod is running and never pull images from remote registries by setting the <code>ImagePullPolicy</code> to <code>Never</code>. To determine the node for a scan Job Starboard will list active Pods controlled by the scanned workload. If the list is not empty it will take the node name from the first Pod, otherwise it will ignore the workload.</p>"},{"location":"design/design_trivy_file_system_scanner/#example","title":"Example","text":"<p>Let's assume that there's the <code>nginx</code> Deployment in the <code>poc-ns</code> namespace. It runs the <code>example.registry.com/nginx:1.16</code> container image from a private registry <code>example.registry.com</code>. Registry credentials are stored in the <code>private-registry</code> ImagePullSecret. (Alternatively, ImagePullSecret can be attached to a service account referred to by the Deployment.)</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      imagePullSecrets:\n        - name: private-registry\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n</code></pre> <p>To scan the <code>nginx</code> container of the <code>nginx</code> Deployment, Starboard will create the following scan Job in the <code>starboard-system</code> namespace and observe it until it's Completed or Failed.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: starboard-system\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      restartPolicy: Never\n      # Explicit nodeName indicates our intention to schedule a scan pod\n      # on the same cluster node where the nginx workload is running.\n      # This could also imply considering taints and tolerations and other\n      # properties respected by K8s scheduler.\n      nodeName: kind-control-plane\n      volumes:\n        - name: scan-volume\n          emptyDir: { }\n      initContainers:\n        # The trivy-get-binary init container is used to copy out the trivy executable\n        # binary from the upstream Trivy container image, i.e. aquasec/trivy:0.19.2,\n        # to a shared emptyDir volume.\n        - name: trivy-get-binary\n          image: aquasec/trivy:0.19.2\n          command:\n            - cp\n            - -v\n            - /usr/local/bin/trivy\n            - /var/starboard/trivy\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n        # The trivy-download-db container is using trivy executable binary\n        # from the previous step to download Trivy vulnerability database\n        # from GitHub releases page.\n        # This won't be required once Trivy supports ClientServer mode\n        # for the fs subcommand.\n        - name: trivy-download-db\n          image: aquasec/trivy:0.19.2\n          command:\n            - /var/starboard/trivy\n            - --download-db-only\n            - --cache-dir\n            - /var/starboard/trivy-db\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n      containers:\n        # The nginx container is based on the container image that\n        # we want to scan with Trivy. However, it has overwritten command (entrypoint)\n        # to invoke trivy file system scan. The scan results are output to stdout\n        # in JSON format, so we can parse them and store as VulnerabilityReport.\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n          # To scan image layers cached on a cluster node without pulling\n          # it from a remote registry.\n          imagePullPolicy: Never\n          securityContext:\n            # Trivy must run as root, so we set UID here.\n            runAsUser: 0\n          command:\n            - /var/starboard/trivy\n            - --cache-dir\n            - /var/starboard/trivy-db\n            - fs\n            - --format\n            - json\n            - /\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n</code></pre> <p>Notice that the scan Job does not use registry credentials stored in the <code>private-registry</code> ImagePullSecret at all. Also, the <code>ImagePullPolicy</code> for the <code>nginx</code> container is set to <code>Never</code> to avoid pulling the image from the <code>example.registry.com/nginx</code> repository that requires authentication. And finally, the <code>nodeName</code> property is explicitly set to <code>kube-control-plane</code> to make sure that the scan Job is scheduled on the same node as a Pod controlled by the <code>nginx</code> Deployment. (We assumed that there was at least one Pod controlled by the <code>nginx</code> Deployment, and it was scheduled on the <code>kube-control-plane</code> node.)</p> <p>Trivy must run as root so the scan Job defined the <code>securityContext</code> with the <code>runAsUser</code> property set to <code>0</code> UID.</p>"},{"location":"design/design_trivy_file_system_scanner/#remarks","title":"Remarks","text":"<ol> <li>The proposed solution won't work with the AlwaysPullImages admission controller, which might be enabled in    a multitenant cluster so that users can be assured that their private images can only be used by those who    have the credentials to pull them. (Thanks kfox1111 for pointing this out!)</li> <li>We cannot scan K8s workloads scaled down to 0 replicas because we cannot infer on which cluster node a scan Job should    run. (In general, a node name is only set on a running Pod.) But once a workload is scaled up, Starboard Operator    will receive the update event and will have another chance to scan it.</li> <li>It's hard to identify Pods managed by the CronJob controller, therefore we'll skip them.</li> <li>Trivy filesystem command does not work in ClientServer mode. Therefore, this solution is subject to the limits of    the Standalone mode. We plan to extend Trivy filesystem command to work in ClientServer mode and improve the    implementation of Starboard once it's available.</li> <li>Trivy must run as root and this may be blocked by some Admission Controllers such as PodSecurityPolicy.</li> </ol>"},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/","title":"Run Vulnerability Scan Job in Same namespace of workload","text":""},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#overview","title":"Overview","text":"<p>When user runs a workload with private managed registry image(eg. image from ECR, ACR) and user is not using ImagePullSecret method to provide access to registry, then starboard operator has challenges to scan such workloads.  - Consider an example of ECR registry, there is one option available in which that user can associate IAM role to service account,  then workloads which are associated with this service account will get authorised to run with the image from that registry.   If user wants to get these images scanned using Starboard operator then currently we have only one way to do that.   User has to associate IAM role to starboard service account, so with when scan job run with <code>starboard-operator</code>service   account, then Trivy will get appropriate permission to pull the image. To know more on how this mechanism works, please   refer to the documents ECR registry configuration, IAM role to service account, but, starboard cannot use permission   set on service account of workload.  </p> <p>Recently, there is one option added in Trivy plugin with Trivy fs command, In which Trivy scans the image which is  cached on a node. And to do that scan job is scheduled on same node where workload is running, so that Trivy can use a  cached image from a node. But, if we want to schedule these scan job on any node, then currently we dont have option to  do that, coz image might not be available on that node. Also, starboard cannot attach imagePullSecret available on the  workload pull the image. We also thought that when we have ImagePullSecret available on a workload, then we can use existing  option of Trivy image scan with which we can scan workload. To do that, starboard operator creates another secret  from existing ImagePullSecret so that registry credentials are provided to Trivy as Env var. But again,  we cannot reuse the same ImagePullSecret available on the workload.     </p>"},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#solution","title":"Solution","text":"<p>Consider there is an option given to enable running vulnerability scan jobs in the same namespace of workload. Operator detects it, so it can schedule and monitor scan jobs in same namespace where workload is running. And plugins will act  accordingly to utilize the service account and ImagePullSecret available on the workload.</p>"},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example","title":"Example","text":""},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example-1","title":"Example 1","text":"<p>Consider starboard operator is running with Trivy image scan mode. And let's assume that there is an <code>nginx</code>  deployment in <code>poc-ns</code> namespace. It is running with image <code>12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16</code>.  This deployment is running with service account <code>poc-sa</code>, which is annotated with ARN: <code>arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/IAM_ROLE_NAME</code></p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/IAM_ROLE_NAME\n  name: poc-sa\n  namespace: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: poc-sa\n      containers:\n        - name: nginx\n          image: 12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16\n</code></pre> <p>When a pod(<code>nginx-65b78bbbd4-nb5kl</code>) comes into running state from above deployment then pod will  have these env var to get access to ECR registry: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>, <code>AWS_WEB_IDENTITY_TOKEN_FILE</code> </p> <p>To scan the <code>nginx</code> deployment, starboard-operator create following scan job in <code>poc-ns</code> namespace. And starboard-operator will monitor this job, and it will parse the result based on completion state of job. This job will run with same  service account(<code>poc-sa</code>) of workload.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: poc-ns\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      serviceAccountName: poc-sa\n      restartPolicy: Never\n      containers:\n      # containers from pod spec returned from existing Trivy plugin\n</code></pre> <p>When a pod(<code>scan-vulnerabilityreport-ab3134-nfkst</code>) gets created from above job spec, then that pod will get injected  with these env var which will help scanner to get access to registry image: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>,  <code>AWS_WEB_IDENTITY_TOKEN_FILE</code></p> <p>Pod will get injected with respective env vars to get access to registry image and Trivy scanner will use these credentials to pull an image for scanning.</p>"},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example-2","title":"Example 2","text":"<p>Consider another example, in which we want to perform vulnerability scan using Trivy <code>fs</code> command. Deployment <code>demo-nginx</code> is running in <code>poc-ns</code> namespace. This deployment is running with image  <code>example.registry.com/nginx:1.16</code> from private registry <code>example.registry.com</code>. Registry credentials are stored in  ImagePullSecret <code>private-registry</code>.  <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-nginx\n  name: demo-nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      imagePullSecrets:\n        - name: private-registry\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n</code></pre></p> <p>To scan the <code>demo-nginx</code> deployment, starboard-operator create following scan job in <code>poc-ns</code> namespace. And starboard-operator  will monitor job, and it will parse the result based on completion state of job.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: poc-ns\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      # ImagePullSecret value will be copied from workload which we are scanning\n      imagePullSecrets:\n        - name: private-registry\n      restartPolicy: Never\n      volumes:\n        - name: scan-volume\n          emptyDir: { }\n      initContainers:\n        - name: trivy-get-binary\n          image: aquasec/trivy:0.19.2\n          command:\n            - cp\n            - -v\n            - /usr/local/bin/trivy\n            - /var/starboard/trivy\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n        - name: trivy-download-db\n          image: aquasec/trivy:0.19.2\n          command:\n            - /var/starboard/trivy\n            - --download-db-only\n            - --cache-dir\n            - /var/starboard/trivy-db\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            # Trivy must run as root, so we set UID here.\n            runAsUser: 0\n          command:\n            - /var/starboard/trivy\n            - --cache-dir\n            - /var/starboard/trivy-db\n            - fs\n            - --format\n            - json\n            - /\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/starboard\n</code></pre> <p>If you observe in the job spec, this scan job will run in <code>poc-ns</code> namespace and it is running with image  <code>example.registry.com/nginx:1.16</code>. It is using ImagePullSecret <code>private-registry</code> which is available in same namespace.  With this approach starboard operator will not have to worry about managing(create/delete) of secret required for scanning. </p>"},{"location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#notes","title":"Notes","text":"<ol> <li>There are some points to consider before using this option<ul> <li>Scan jobs will run in different namespaces. This will create some activity in each namespace available in the cluster.  If we dont use this option then all scan jobs will only run in <code>starboard-operator</code> namespace, and user can see all  activity confined to single namespace i.e <code>starboard-operator</code>.</li> <li>As we will run scan job with service account of workload and if there are some very strict PSP defined in the cluster then scan job will be blocked due to the PSP.</li> </ul> </li> </ol>"},{"location":"design/ttl_scans/","title":"TTL scans","text":""},{"location":"design/ttl_scans/#summary","title":"Summary","text":"<p>Add an option to automatically delete old security reports. In this first version focus on vulnerability reports but in the long run we could add similar functionality to other reports as well.</p>"},{"location":"design/ttl_scans/#motivation","title":"Motivation","text":"<p>In 537 we talk about a need to run nightly vulnerability scans of CVE:s. This way we can make sure to get new CVE reports for long time running pods as well.</p>"},{"location":"design/ttl_scans/#proposal","title":"Proposal","text":"<p>Add a environment variable to the operator, for example <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=86400</code>, this way we can add other ttl values for other reports as well. Adding this environment variable would add a annotation to the generated VulnerabilityReport that we can look for.</p> <p>Create a new controller that looks for changes in vulnerabilityreports and uses RequeueAfter calculated from the TTL annotation. At startup the operator will look through all existing vulnerabilityreports and delete existing ones where TTL have expired. If the TTL haven't expired the the vulnerabilityreports will be requeued and automatically checked again when the TTL have expired.</p> <p>We could calculate the ttl without having creating a new annotation to the reports but the verbosity of showing the users how long each report got a ttl outweighs the \"issue\" of generating a new annotation.</p>"},{"location":"design/ttl_scans/#example","title":"Example","text":"<p>Below you can see a shortened version of the yaml. Notice the <code>metadata.annotations.starboard.aquasecurity.github.io/report-ttl</code> which is new. The operator would automatically apply the <code>starboard.aquasecurity.github.io/report-ttl</code> annotation to all new reports that it generates assuming that the environment variable is set. In theory users could also extend the TTL manually for a specific report by changing the starboard.aquasecurity.github.io/report-ttl annotation per VulnerabilityReport.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  creationTimestamp: \"2021-12-08T12:03:48Z\"\n  annotations:\n    starboard.aquasecurity.github.io/report-ttl: 24h\n  labels:\n    resource-spec-hash: 86b58dcb99\n    starboard.container.name: manager\n    starboard.resource.kind: ReplicaSet\n    starboard.resource.name: source-controller-b5d5cfdf4\n    starboard.resource.namespace: flux-system\n  name: replicaset-source-controller-b5d5cfdf4-manager\nreport:\n  artifact:\n    repository: fluxcd/source-controller\n    tag: v0.16.1\n  registry:\n    server: ghcr.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.19.2\n  summary:\n    criticalCount: 0\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  updateTimestamp: \"2021-12-08T12:03:48Z\"\n  vulnerabilities: []\n</code></pre> <p>Another option is to define a new report entry, the positive thing with that is that we can define the input type, in our case a <code>time.Duration</code>.</p>"},{"location":"design/ttl_scans/#alternatives","title":"Alternatives","text":"<p>Another \"simpler\" option could be to add the same environment variable <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=2h</code> but instead of using RequeueAfter in the controller we could create a cronjob/job that runs once an hour and check for the same annotation.</p> <p>The bad thing hear is that we would have to manage yet another cronjob/job. We would also have to mange a new binary feature flag to run in cronjob cleanup mode. It would also trigger removal of multiple reports at the same time, compared to the event driven solution that would be much more precise per report and thus spreading out the new reports more.</p> <p>But the good thing is that everyone knows how jobs/cronjobs works especially since it's already well used within the starboard operator.</p>"},{"location":"integrations/lens/","title":"Lens Extension","text":"<p>Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes.</p> <p>Starboard Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>"},{"location":"integrations/octant/","title":"Octant Plugin","text":"<p>Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer's toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities.</p> <p>Starboard Octant Plugin provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>"},{"location":"integrations/octant/#installation","title":"Installation","text":""},{"location":"integrations/octant/#prerequisites","title":"Prerequisites","text":"<ul> <li>Octant &gt;= 0.13 should first be installed. On macOS this is as simple as <code>brew install octant</code>. For installation   instructions on other operating systems and package managers, see Octant Installation.</li> <li>Environment authenticated against your Kubernetes cluster</li> </ul> <p>Tip</p> <p>In the following instructions we assume that the <code>$HOME/.config/octant/plugins</code> directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the <code>OCTANT_PLUGIN_PATH</code> environment variable when running Octant.</p>"},{"location":"integrations/octant/#from-the-binary-releases","title":"From the Binary Releases","text":"<p>Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed.</p> <ol> <li>Download your desired version</li> <li>Unpack it (<code>tar -zxvf starboard-octant-plugin_darwin_x86_64.tar</code>)</li> <li>Find the <code>starboard-octant-plugin</code> binary in the unpacked directory, and move it to the default Octant's    configuration directory (<code>mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins</code>).    You might need to create the directory if it doesn't exist already.</li> </ol>"},{"location":"integrations/octant/#from-source-linux-macos","title":"From Source (Linux, macOS)","text":"<p>Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin.</p> <p>You must have a working Go environment.</p> <pre><code>git clone git@github.com:aquasecurity/starboard-octant-plugin.git\ncd starboard-octant-plugin\nmake install\n</code></pre> <p>The <code>make install</code> goal copies the plugin binary to the <code>$HOME/.config/octant/plugins</code> directory.</p>"},{"location":"integrations/octant/#uninstall","title":"Uninstall","text":"<p>Run the following command to remove the plugin:</p> <pre><code>rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin\n</code></pre> <p>where <code>$OCTANT_PLUGIN_PATH</code> is the default plugins location respected by Octant. If not set, it defaults to the <code>$HOME/.config/octant/plugins</code> directory.</p>"},{"location":"integrations/prometheus/","title":"Prometheus Exporter","text":"<p>Giant Swarm developed exporter that exposes vulnerability summary and vulnerability details as Prometheus metrics based on VulnerabilityReports generated by the Starboard Operator.</p>"},{"location":"operator/","title":"Starboard Operator","text":""},{"location":"operator/#overview","title":"Overview","text":"<p>This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started.</p> Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects. <p>Similarly, the operator performs infrastructure checks by watching Kubernetes cluster nodes and executing CIS Kubernetes Benchmark for each of them.</p> Infrastructure reconciler discovers K8s nodes, manages scan jobs, and creates CISKubeBenchReport objects. <p>In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport.</p> <p>Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named <code>plugin-config-hash</code> to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the <code>plugin-config-hash</code> labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration.</p> Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes. <p>Warning</p> <p>Currently, the operator supports vulnerabilityreports, configauditreports, and ciskubebenchreports security resources. We plan to support kubehunterreports. We also plan to implement rescan on configurable schedule, for example every 24 hours.</p>"},{"location":"operator/#whats-next","title":"What's Next?","text":"<ul> <li>Install the operator and follow the Getting Started guide.</li> </ul>"},{"location":"operator/configuration/","title":"Configuration","text":"<p>Configuration of the operator's Pod is done via environment variables at startup.</p> NAME DEFAULT DESCRIPTION <code>OPERATOR_NAMESPACE</code> N/A See Install modes <code>OPERATOR_TARGET_NAMESPACES</code> N/A See Install modes <code>OPERATOR_EXCLUDE_NAMESPACES</code> N/A A comma separated list of namespaces (or glob patterns) to be excluded from scanning in all namespaces Install mode. <code>OPERATOR_SERVICE_ACCOUNT</code> <code>starboard-operator</code> The name of the service account assigned to the operator's pod <code>OPERATOR_LOG_DEV_MODE</code> <code>false</code> The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). <code>OPERATOR_SCAN_JOB_TIMEOUT</code> <code>5m</code> The length of time to wait before giving up on a scan job <code>OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT</code> <code>10</code> The maximum number of scan jobs create by the operator <code>OPERATOR_SCAN_JOB_RETRY_AFTER</code> <code>30s</code> The duration to wait before retrying a failed scan job <code>OPERATOR_BATCH_DELETE_LIMIT</code> <code>10</code> The maximum number of config audit reports deleted by the operator when the plugin's config has changed. <code>OPERATOR_BATCH_DELETE_DELAY</code> <code>10s</code> The duration to wait before deleting another batch of config audit reports. <code>OPERATOR_METRICS_BIND_ADDRESS</code> <code>:8080</code> The TCP address to bind to for serving Prometheus metrics. It can be set to <code>0</code> to disable the metrics serving. <code>OPERATOR_HEALTH_PROBE_BIND_ADDRESS</code> <code>:9090</code> The TCP address to bind to for serving health probes, i.e. <code>/healthz/</code> and <code>/readyz/</code> endpoints. <code>OPERATOR_CIS_KUBERNETES_BENCHMARK_ENABLED</code> <code>true</code> The flag to enable CIS Kubernetes Benchmark scanner <code>OPERATOR_VULNERABILITY_SCANNER_ENABLED</code> <code>true</code> The flag to enable vulnerability scanner <code>OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED</code> <code>false</code> The flag to enable plugin-based configuration audit scanner <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>false</code> The flag to enable config audit scanner to only scan the current revision of a deployment <code>OPERATOR_CONFIG_AUDIT_SCANNER_BUILTIN</code> <code>true</code> The flag to enable built-in configuration audit scanner <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>false</code> The flag to enable vulnerability scanner to only scan the current revision of a deployment <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL</code> <code>\"\"</code> The flag to set how long a vulnerability report should exist. When a old report is deleted a new one will be created by the controller. It can be set to <code>\"\"</code> to disabled the TTL for vulnerability scanner. <code>OPERATOR_LEADER_ELECTION_ENABLED</code> <code>false</code> The flag to enable operator replica leader election <code>OPERATOR_LEADER_ELECTION_ID</code> <code>starboard-lock</code> The name of the resource lock for leader election <code>OPERATOR_CLUSTER_COMPLIANCE_ENABLED</code> <code>true</code> The flag to enable Cluster Compliance report generation"},{"location":"operator/configuration/#install-modes","title":"Install Modes","text":"<p>The values of the <code>OPERATOR_NAMESPACE</code> and <code>OPERATOR_TARGET_NAMESPACES</code> determine the install mode, which in turn determines the multitenancy support of the operator.</p> MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace <code>operators</code> <code>operators</code> The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace <code>operators</code> <code>foo</code> The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace <code>operators</code> <code>foo,bar,baz</code> The operator can be configured to watch for events in more than one namespace. AllNamespaces <code>operators</code> (blank string) The operator can be configured to watch for events in all namespaces."},{"location":"operator/getting-started/","title":"Getting Started","text":""},{"location":"operator/getting-started/#before-you-begin","title":"Before you Begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube or kind, or you can use one of these Kubernetes playgrounds:</p> <ul> <li>Katacoda</li> <li>Play with Kubernetes</li> </ul> <p>You also need the Starboard Operator to be installed in the <code>starboard-system</code> namespace, e.g. with kubectl or Helm. Let's also assume that the operator is configured to discover built-in Kubernetes resources in all namespaces, except <code>kube-system</code> and <code>starboard-system</code>.</p>"},{"location":"operator/getting-started/#workloads-scanning","title":"Workloads Scanning","text":"<p>Let's create the <code>nginx</code> Deployment that we know is vulnerable:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>When the <code>nginx</code> Deployment is created, the operator immediately detects its current revision (aka active ReplicaSet) and scans the <code>nginx:1.16</code> image for vulnerabilities. It also audits the ReplicaSet's specification for common pitfalls such as running the <code>nginx</code> container as root.</p> <p>If everything goes fine, the operator saves scan reports as VulnerabilityReport and ConfigAuditReport resources in the <code>default</code> namespace. Reports are named after the scanned ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container. In this example there is just one container image called <code>nginx</code>:</p> <pre><code>kubectl get vulnerabilityreports -o wide\n</code></pre> Result <pre><code>NAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-78449c65d4-nginx   library/nginx   1.16   Trivy     85s   33         62     49       114   1\n</code></pre> <pre><code>kubectl get configauditreports -o wide\n</code></pre> Result <pre><code>NAME                          SCANNER     AGE    CRITICAL  HIGH   MEDIUM   LOW\nreplicaset-nginx-78449c65d4   Starboard   2m7s   0         0      6        7\n</code></pre> <p>Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport resources are controlled by the active ReplicaSet of the <code>nginx</code> Deployment:</p> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h2m\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h2m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              2m31s\ndefault      \u251c\u2500Pod/nginx-78449c65d4-5wvdx                             True           7h2m\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              2m7s\n</code></pre> <p>Note</p> <p>The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree.</p> <p>Moving forward, let's update the container image of the <code>nginx</code> Deployment from <code>nginx:1.16</code> to <code>nginx:1.17</code>. This will trigger a rolling update of the Deployment and eventually create another ReplicaSet.</p> <pre><code>kubectl set image deployment nginx nginx=nginx:1.17\n</code></pre> <p>Even this time the operator will pick up changes and rescan our Deployment with updated configuration:</p> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h5m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              2m36s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              2m36s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           2m36s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              2m22s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h5m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              5m46s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              5m22s\n</code></pre> <p>By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport resources to build-in Kubernetes objects. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named <code>nginx-78449c65d4</code> is deleted the VulnerabilityReport named <code>replicaset-nginx-78449c65d4-nginx</code> as well as the ConfigAuditReport named <code>replicaset-nginx-78449c65d46</code> are automatically garbage collected.</p> <p>Tip</p> <p>If you only want the latest ReplicaSet in your Deployment to be scanned for vulnerabilities, you can set the value of the <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>true</code> in the operator's deployment descriptor. This is useful to identify vulnerabilities that impact only the running workloads.</p> <p>Tip</p> <p>If you only want the latest ReplicaSet in your Deployment to be scanned for config audit, you can set the value of the <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>true</code> in the operator's deployment descriptor. This is useful to identify config issues that impact only the running workloads.</p> <p>Tip</p> <p>You can get and describe <code>vulnerabilityreports</code> and <code>configauditreports</code> as built-in Kubernetes objects: <pre><code>kubectl get vulnerabilityreport replicaset-nginx-5fbc65fff-nginx -o json\nkubectl describe configauditreport replicaset-nginx-5fbc65fff\n</code></pre></p> <p>Notice that scaling up the <code>nginx</code> Deployment will not schedule new scans because all replica Pods refer to the same Pod template defined by the <code>nginx-5fbc65fff</code> ReplicaSet.</p> <pre><code>kubectl scale deploy nginx --replicas 3\n</code></pre> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h6m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              4m7s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              4m7s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-458n7                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-fk847                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           4m7s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              3m53s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h6m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              7m17s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              6m53s\n</code></pre> <p>Finally, when you delete the <code>nginx</code> Deployment, orphaned security reports will be deleted in the background by the Kubernetes garbage collection controller.</p> <pre><code>kubectl delete deploy nginx\n</code></pre> <pre><code>kubectl get vuln,configaudit\n</code></pre> Result <pre><code>No resources found in default namespace.\n</code></pre> <p>Tip</p> <p>Use <code>vuln</code> and <code>configaudit</code> as short names for <code>vulnerabilityreports</code> and <code>configauditreports</code> resources.</p> <p>Note</p> <p>You can define the validity period for VulnerabilityReports by setting the duration as the value of the <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL</code> environment variable. For example, setting the value to <code>24h</code> would delete reports after 24 hours. When a VulnerabilityReport gets deleted Starboard Operator will automatically rescan the underlying workload. Assuming that the vulnerability scanner has updated its vulnerability database, new VulnerabilityReports will contain the latest vulnerabilities.</p>"},{"location":"operator/getting-started/#infrastructure-scanning","title":"Infrastructure Scanning","text":"<p>The operator discovers also Kubernetes nodes and runs CIS Kubernetes Benchmark checks on each of them. The results are stored as CISKubeBenchReport objects. In other words, for a cluster with 3 nodes the operator will eventually create 3 benchmark reports:</p> <pre><code>kubectl get node\n</code></pre> Result <pre><code>NAME                 STATUS   ROLES    AGE     VERSION\nkind-control-plane   Ready    master   3h27m   v1.18.8\nkind-worker          Ready    &lt;none&gt;   3h26m   v1.18.8\nkind-worker2         Ready    &lt;none&gt;   3h26m   v1.18.8\n</code></pre> <pre><code>kubectl get ciskubebenchreports -o wide\n</code></pre> Result <pre><code>NAME                 SCANNER      AGE   FAIL   WARN   INFO   PASS\nkind-control-plane   kube-bench   8s    12     40     0      70\nkind-worker          kube-bench   9s    2      27     0      18\nkind-worker2         kube-bench   9s    2      27     0      18\n</code></pre> <p>Notice that each CISKubeBenchReport is named after a node and is controlled by that node to inherit its life cycle:</p> <pre><code>kubectl tree node kind-control-plane -A\n</code></pre> Result <pre><code>NAMESPACE        NAME                                              READY  REASON        AGE\n                 Node/kind-control-plane                           True   KubeletReady  48m\n                 \u251c\u2500CISKubeBenchReport/kind-control-plane           -                    44m\n                 \u251c\u2500CSINode/kind-control-plane                      -                    48m\nkube-node-lease  \u251c\u2500Lease/kind-control-plane                        -                    48m\nkube-system      \u251c\u2500Pod/etcd-kind-control-plane                     True                 48m\nkube-system      \u251c\u2500Pod/kube-apiserver-kind-control-plane           True                 48m\nkube-system      \u251c\u2500Pod/kube-controller-manager-kind-control-plane  True                 48m\nkube-system      \u2514\u2500Pod/kube-scheduler-kind-control-plane           True                 48m\n</code></pre>"},{"location":"operator/getting-started/#whats-next","title":"What's Next?","text":"<ul> <li>Find out how the operator scans workloads that use container images from Private Registries.</li> <li>By default, the operator uses Trivy as Vulnerability Scanner and Polaris as Configuration Checker, but you can   choose other tools that are integrated with Starboard or even implement you own plugin.</li> </ul>"},{"location":"operator/troubleshooting/","title":"Troubleshooting the Starboard Operator","text":"<p>The Starboard Operator installs several Kubernetes resources into your Kubernetes cluster.</p> <p>Here are the common steps to check whether the operator is running correctly and to troubleshoot common issues.</p> <p>In addition to having a look at this section, you want to check previous issues to see if someone from the community had similar problems before. Feel free to either open an issue, reach out on Slack, or post your questions in the discussion forum.</p>"},{"location":"operator/troubleshooting/#installation","title":"Installation","text":"<p>Make sure that the latest version of the Starboard Operator is installed inside of your Kubernetes cluster. For this, have a look at the installation options.</p> <p>For instance, if your are using the Helm deployment, you need to check the Helm Chart version deployed to your cluster. You can check the Helm Chart version with the following command: <pre><code>helm list -n &lt;namespace&gt;\n</code></pre></p> <p>Please make sure to replace the <code>namespace</code> with the namespace to which you installed the Starboard Operator. In the installation guide, we are using <code>starboard-system</code> as our namespace.</p>"},{"location":"operator/troubleshooting/#starboard-pod-not-running","title":"Starboard Pod Not Running","text":"<p>The Starboard Operator will run a pod inside your cluster. If you have followed the installation guide, you will have installed the Operator to the <code>starboard-system</code>. If you have installed it to another namespace, make sure to adapt the commands below.</p> <p>Make sure that the pod is in the <code>Running</code> status: <pre><code>kubectl get pods -n starboard-system\n</code></pre></p> <p>This is how it will look if it is running okay:</p> <pre><code>NAMESPACE            NAME                                         READY   STATUS    RESTARTS      AGE\nstarboard-system     starboard-operator-6c9bd97d58-hsz4g          1/1     Running   5 (19m ago)   30h\n</code></pre> <p>If the pod is in <code>Failed</code>, <code>Pending</code>, or <code>Unknown</code> check the events and the logs of the pod.</p> <p>First, check the events, since they might be more descriptive of the problem. However, if the events do not give a clear reason why the pod cannot spin up, then you want to check the logs, which provide more detail.</p> <pre><code>kubectl describe pod &lt;POD-NAME&gt; -n starboard-system\n</code></pre> <p>To check the logs, use the following command: <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></p> <p>If your pod is not running, try to look for errors as they can give an indication on the problem.</p> <p>If there are too many logs messages, try deleting the Starboard pod and observe its behaviour upon restarting. A new pod should spin up automatically after deleting the failed pod.</p>"},{"location":"operator/troubleshooting/#imagepullbackoff-or-errimagepull","title":"ImagePullBackOff or ErrImagePull","text":"<p>Check the status of the Starboard Operator pod running inside of your Kubernetes cluster. If the Status is ImagePullBackOff or ErrImagePull, it means that the Operator either</p> <ul> <li>tries to access the wrong image</li> <li>cannot pull the image from the registry</li> </ul> <p>Make sure that you are providing the right resources upon installing the Starboard Operator.</p>"},{"location":"operator/troubleshooting/#crashloopbackoff","title":"CrashLoopBackOff","text":"<p>If your pod is in <code>CrashLoopBackOff</code>, it is likely the case that the pod cannot be scheduled on the Kubernetes node that it is trying to schedule on. In this case, you want to investigate further whether there is an issue with the node. It could for instance be the case that the node does not have sufficient resources.</p>"},{"location":"operator/troubleshooting/#reconcilation-error","title":"Reconcilation Error","text":"<p>It could happen that the pod appears to be running normally but does not reconcile the resources inside of your Kubernetes cluster.</p> <p>Check the logs for reconcilation errors: <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></p> <p>If this is the case, the Starboard Operator likely does not have the right configurations to access your resource. </p>"},{"location":"operator/troubleshooting/#operator-does-not-create-vulnerabilityreports","title":"Operator does not Create VulnerabilityReports","text":"<p>VulnerabilityReports are owned and controlled by the immediate Kubernetes workload. Every VulnerabilityReport of a pod is thus, linked to a ReplicaSet. In case the Starboard Operator does not create a VulnerabilityReport for your workloads, it could be that it is not monitoring the namespace that your workloads are running on.</p> <p>An easy way to check this is by looking for the <code>ClusterRoleBinding</code> for the Starboard Operator:</p> <pre><code>kubectl get ClusterRoleBinding | grep \"starboard-operator\"\n</code></pre> <p>Alternatively, you could use the <code>kubectl-who-can</code> plugin by Aqua:</p> <pre><code>$ kubectl who-can list vulnerabilityreports\nNo subjects found with permissions to list vulnerabilityreports assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                         TYPE            SA-NAMESPACE\ncluster-admin                                system:masters                  Group\nstarboard-operator                           starboard-operator              ServiceAccount  starboard-system\nsystem:controller:generic-garbage-collector  generic-garbage-collector       ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller            ServiceAccount  kube-system\nsystem:controller:resourcequota-controller   resourcequota-controller        ServiceAccount  kube-system\nsystem:kube-controller-manager               system:kube-controller-manager  User\n</code></pre> <p>If the <code>ClusterRoleBinding</code> does not exist, Starboard currently cannot monitor any namespace outside of the <code>starboard-system</code> namespace. </p> <p>For instance, if you are using the Helm Chart, you want to make sure to set the <code>targetNamespace</code> to the namespace that you want the Operator to monitor.</p>"},{"location":"operator/installation/helm/","title":"Helm","text":"<p>Helm, which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts.</p> <p>To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard Operator. The Helm chart supports all Install Modes.</p> <p>As an example, let's install the operator in the <code>starboard-system</code> namespace and configure it to select all namespaces, except <code>kube-system</code> and <code>starboard-system</code>:</p> <ol> <li>Clone the chart directory:    <pre><code>git clone --depth 1 --branch v0.15.25-2-gab715b7 https://github.com/aquasecurity/starboard.git\ncd starboard\n</code></pre>    Or add Aqua chart repository:    <pre><code>helm repo add aqua https://aquasecurity.github.io/helm-charts/\nhelm repo update\n</code></pre></li> <li>Install the chart from a local directory:    <pre><code>helm install starboard-operator ./deploy/helm \\\n  --namespace starboard-system \\\n  --create-namespace \\\n  --set=\"trivy.ignoreUnfixed=true\"\n</code></pre>    Or install the chart from the Aqua chart repository:    <pre><code>helm install starboard-operator aqua/starboard-operator \\\n  --namespace starboard-system \\\n  --create-namespace \\\n  --set=\"trivy.ignoreUnfixed=true\" \\\n  --version 0.10.22\n</code></pre>    There are many values in the chart that can be set to configure Starboard.</li> <li>Check that the <code>starboard-operator</code> Helm release is created in the <code>starboard-system</code> namespace, and it has status    <code>deployed</code>:    <pre><code>$ helm list -n starboard-system\nNAME                 NAMESPACE           REVISION    UPDATED                                 STATUS      CHART                       APP VERSION\nstarboard-operator   starboard-system    1           2021-01-27 20:09:53.158961 +0100 CET    deployed    starboard-operator-0.10.22  0.15.25-2-gab715b7\n</code></pre>    To confirm that the operator is running, check that the <code>starboard-operator</code> Deployment in the <code>starboard-system</code>    namespace is available and all its containers are ready:    <pre><code>$ kubectl get deployment -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></li> </ol>"},{"location":"operator/installation/helm/#uninstall","title":"Uninstall","text":"<p>You can uninstall the operator with the following command:</p> <pre><code>helm uninstall starboard-operator -n starboard-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the <code>helm install</code> command:</p> <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd clustervulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd ciskubebenchreports.aquasecurity.github.io\nkubectl delete crd kubehunterreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd clustercompliancereports.aquasecurity.github.io\nkubectl delete crd clustercompliancedetailreports.aquasecurity.github.io\n</code></pre>"},{"location":"operator/installation/kubectl/","title":"kubectl","text":"<p>You can use static YAML manifests to install the operator in the <code>starboard-system</code> namespace and configure it to select all namespaces, except <code>kube-system</code> and <code>starboard-system</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.15.25-2-gab715b7/deploy/static/starboard.yaml\n</code></pre> <p>To confirm that the operator is running, check that the <code>starboard-operator</code> Deployment in the <code>starboard-system</code> namespace is available and all its containers are ready:</p> <pre><code>$ kubectl get deployment -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre> <p>If for some reason it's not ready yet, check the logs of the <code>starboard-operator</code> Deployment for errors:</p> <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre> <p>Starboard ensures the default Settings stored in ConfigMaps and Secrets created in the <code>starboard-system</code> namespace. You can always change these settings by editing configuration objects. For example, you can use Trivy in ClientServer mode, which is more efficient that the Standalone mode, or switch to Aqua Enterprise as an alternative vulnerability scanner.</p> <p>You can further adjust the Configuration of the operator with environment variables. For example, to change the target namespace from all namespaces to the <code>default</code> namespace edit the <code>starboard-operator</code> Deployment and change the value of the <code>OPERATOR_TARGET_NAMESPACES</code> environment variable from the blank string (<code>\"\"</code>) to the <code>default</code> value.</p> <p>Starboard can generate the compliance report based on the NSA, CISA Kubernetes Hardening Guidance v1.0. In order to do that you must install the <code>nsa</code> ClusterComplianceReport resource:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.15.25-2-gab715b7/deploy/specs/nsa-1.0.yaml\n</code></pre> <p>Static YAML manifests with fixed values have shortcomings. For example, if you want to change the container image or modify default configuration settings, you have to edit existing manifests or customize them with tools such as Kustomize. Thus, we also provide Helm chart as an alternative installation option.</p>"},{"location":"operator/installation/kubectl/#uninstall","title":"Uninstall","text":"<p>Danger</p> <p>Uninstalling the operator and deleting custom resource definitions will also delete all generated security reports.</p> <p>You can uninstall the operator with the following command:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.15.25-2-gab715b7/deploy/static/starboard.yaml\n</code></pre>"},{"location":"operator/installation/olm/","title":"Operator Lifecycle Manager","text":"<p>The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies.</p> <p>You can install the Starboard operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod.</p> <p>As an example, let's install the operator from the OperatorHub catalog in the <code>starboard-system</code> namespace and configure it to watch the <code>default</code> namespaces:</p> <ol> <li>Install the Operator Lifecycle Manager:    <pre><code>curl -L https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/install.sh -o install.sh\nchmod +x install.sh\n./install.sh v0.20.0\n</code></pre>    or    <pre><code>kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/crds.yaml\nkubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/olm.yaml\n</code></pre></li> <li>Create the namespace to install the operator in:    <pre><code>kubectl create ns starboard-system\n</code></pre></li> <li>Create the OperatorGroup to select all namespaces:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha2\nkind: OperatorGroup\nmetadata:\n  name: starboard-operator\n  namespace: starboard-system\nspec:\n  targetNamespaces: []\nEOF\n</code></pre></li> <li>(Optional) Configure Starboard by creating the <code>starboard</code> ConfigMap and the <code>starboard</code> secret in    the <code>starboard-system</code> namespace. For example, you can use Trivy    in ClientServer mode or    Aqua Enterprise as an active vulnerability scanner.    If you skip this step, the operator will ensure default Settings on startup:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.15.25-2-gab715b7/deploy/static/03-starboard-operator.config.yaml\n</code></pre></li> <li>Install default OPA Rego policies used by the built-in configuration checker:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.15.25-2-gab715b7/deploy/static/04-starboard-operator.policies.yaml\n</code></pre></li> <li>Install the operator by creating the Subscription:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: starboard-operator\n  namespace: starboard-system\nspec:\n  channel: alpha\n  name: starboard-operator\n  source: operatorhubio-catalog\n  sourceNamespace: olm\n  installPlanApproval: Automatic\n  config:\n    env:\n    - name: OPERATOR_EXCLUDE_NAMESPACES\n      value: \"kube-system,starboard-system\"\nEOF\n</code></pre>    The operator will be installed in the <code>starboard-system</code> namespace and will select all namespaces, except    <code>kube-system</code> and <code>starboard-system</code>. Note that the <code>spec.config</code> property allows you to override the default    Configuration of the operator's Deployment.</li> <li>After install, watch the operator come up using the following command:    <pre><code>$ kubectl get clusterserviceversions -n starboard-system\nNAME                        DISPLAY              VERSION   REPLACES                     PHASE\nstarboard-operator.v0.15.25-2-gab715b7  Starboard Operator   0.15.25-2-gab715b7    starboard-operator.v0.15.25   Succeeded\n</code></pre>    If the above command succeeds and the ClusterServiceVersion has transitioned from <code>Installing</code> to <code>Succeeded</code> phase    you will also find the operator's Deployment in the same namespace where the Subscription is:    <pre><code>$ kubectl get deployments -n starboard-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nstarboard-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/starboard-operator -n starboard-system\n</code></pre></li> </ol>"},{"location":"operator/installation/olm/#uninstall","title":"Uninstall","text":"<p>To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup:</p> <pre><code>kubectl delete subscription starboard-operator -n starboard-system\nkubectl delete clusterserviceversion starboard-operator.v0.15.25-2-gab715b7 -n starboard-system\nkubectl delete operatorgroup starboard-operator -n starboard-system\nkubectl delete ns starboard-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the OLM operator:</p> <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd ciskubebenchreports.aquasecurity.github.io\nkubectl delete crd clustercompliancereports.aquasecurity.github.io\nkubectl delete crd clustercompliancedetailreports.aquasecurity.github.io\n</code></pre>"},{"location":"operator/installation/upgrade/","title":"Upgrade","text":"<p>We recommend that you upgrade Starboard Operator often to stay up to date with the latest fixes and enhancements.</p> <p>However, at this stage we do not provide automated upgrades. Therefore, uninstall the previous version of the operator before you install the latest release.</p> <p>Warning</p> <p>Consult release notes and changelog to revisit and migrate configuration settings which may not be compatible between different versions.</p>"},{"location":"tutorials/manage_access_to_security_reports/","title":"Manage Access to Security Reports","text":"<p>In Starboard security reports are stored as CRD instances (e.g. VulnerabilityReport and ConfigAuditReport objects).</p> <p>With Kubernetes RBAC, a cluster administrator can choose the following levels of granularity to manage access to security reports:</p> <ol> <li>Grant administrative access to view any report in any namespace.</li> <li>Grant coarse-grained access to view any report in a specified namespace.</li> <li>Grant fine-grained access to view a specified report in a specified namespace.</li> </ol> <p>Even though you can achieve fine-grained access control with Kubernetes RBAC configuration, it is very impractical to do so with security reports. Mainly because security reports are associated with ephemeral Kubernetes objects such as Pods and ReplicaSets.</p> <p>To sum up, we only recommend using administrative and coarse-grained levels to manage access to security reports.</p> <p>Continue reading to see examples of managing access to VulnerabilityReport objects at different levels of granularity.</p>"},{"location":"tutorials/manage_access_to_security_reports/#create-namespaces-and-deployments","title":"Create Namespaces and Deployments","text":"<p>Let's consider a multitenant cluster with two <code>nginx</code> Deployments in <code>foo</code> and <code>bar</code> namespaces. There's also the <code>redis</code> Deployment in the <code>foo</code> namespace.</p> <pre><code>kubectl create namespace foo\nkubectl create deploy nginx --image nginx:1.16 --namespace foo\nkubectl create deploy redis --image redis:5 --namespace foo\n</code></pre> <pre><code>kubectl create namespace bar\nkubectl create deploy nginx --image nginx:1.16 --namespace bar\n</code></pre> <p>When we scan them Starboard will create VulnerabilityReports which are named by revision kind (<code>replicaset</code>) concatenated with revision name (<code>nginx-7967dc8bfd</code>) and container name (<code>nginx</code>).</p> <pre><code>starboard scan vulnerabilityreports deploy/nginx --namespace foo\nstarboard scan vulnerabilityreports deploy/redis --namespace foo\nstarboard scan vulnerabilityreports deploy/nginx --namespace bar\n</code></pre> <p>Tip</p> <p>For workloads with multiple containers we'll have multiple instances of VulnerabilityReports with the same prefix (<code>replicaset-nginx-7967dc8bfd-</code>) but different suffixes that correspond to container names.</p> <pre><code>$ kubectl tree deploy nginx --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/nginx                                           -              21m\nfoo        \u2514\u2500ReplicaSet/nginx-7967dc8bfd                              -              21m\nfoo          \u251c\u2500Pod/nginx-7967dc8bfd-gqw8h                             True           21m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-nginx-7967dc8bfd-nginx  -              4m36s\n</code></pre> <pre><code>$ kubectl tree deploy nginx --namespace bar\nNAMESPACE  NAME                                                      READY  REASON  AGE\nbar        Deployment/nginx                                          -              20m\nbar        \u2514\u2500ReplicaSet/nginx-f4cc56f6b                              -              20m\nbar          \u251c\u2500Pod/nginx-f4cc56f6b-9cd45                             True           20m\nbar          \u2514\u2500VulnerabilityReport/replicaset-nginx-f4cc56f6b-nginx  -              2m12s\n</code></pre> <pre><code>$ kubectl tree deploy redis --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/redis                                           -              74m\nfoo        \u2514\u2500ReplicaSet/redis-79c5cc7cf8                              -              74m\nfoo          \u251c\u2500Pod/redis-79c5cc7cf8-fz99f                             True           74m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-redis-79c5cc7cf8-redis  -              74m\n</code></pre>"},{"location":"tutorials/manage_access_to_security_reports/#choose-access-level","title":"Choose Access Level","text":"<p>To manage access to VulnerabilityReport instances a cluster administrator will typically create Role or ClusterRole objects and bind them to subjects (users, groups, or service accounts) by creating RoleBinding or ClusterRoleBinding objects.</p> <p>With Kubernetes RBAC there are three different granularity levels at which you can grant access to VulnerabilityReports:</p> <ol> <li>Cluster - a subject can view any report in any namespace</li> <li>Namespace - a subject can view any report in a specified namespace</li> <li>Security Report - a subject can view a specified report in a specified namespace</li> </ol>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-any-namespace","title":"Grant Access to View any VulnerabilityReport in any Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create clusterrolebinding dpacak-can-view-vulnerabilityreports \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as dpacak\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-f4cc56f6b-nginx    library/nginx   1.16   Trivy     40m\nfoo         replicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     43m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as zpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"zpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" at the cluster scope\n</code></pre> <pre><code>$ kubectl who-can get vulnerabilityreports -A\nNo subjects found with permissions to get vulns assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\ndpacak-can-view-vulnerabilityreports         dpacak                     User\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre> <p>Note</p> <p>The who-can command is a kubectl plugin that shows who has RBAC permissions to perform actions on different resources in Kubernetes.</p>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-the-foo-namespace","title":"Grant Access to View any VulnerabilityReport in the foo Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-vulnerabilityreports \\\n  --namespace foo \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace foo --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     51m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace bar --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"dpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" in the namespace \"bar\"\n</code></pre>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-the-replicaset-nginx-7967dc8bfd-nginx-vulnerabilityreport-in-the-foo-namespace","title":"Grant Access to View the replicaset-nginx-7967dc8bfd-nginx VulnerabilityReport in the foo Namespace","text":"<p>Even though you can grant access to a single VulnerabilityReport by specifying its name when you create Role or ClusterRole objects, in practice it's not manageable for these reasons:</p> <ol> <li>The name of a ReplicaSet (e.g. <code>nginx-7967dc8bfd</code>) and hence the name of the corresponding VulnerabilityReport (e.g.    <code>replicaset-nginx-7967dc8bfd-nginx</code>) change over time. This requires that Role or ClusterObject will be updated    respectively.</li> <li>We create a VulnerabilityReport for each container of a Kubernetes workload. Therefore, managing such fine-grained    permissions is even more cumbersome.</li> <li>Last but not least, the naming convention is an implementation details that's likely to change when we add support    for mutable tags or implement caching of scan results.</li> </ol> <pre><code>kubectl create role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --resource vulnerabilityreports \\\n  --resource-name replicaset-nginx-7967dc8bfd-nginx \\\n  --verb get\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-nginx-7967dc8bfd-nginx --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     163m\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-redis-79c5cc7cf8-redis --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io \"rep\nlicaset-redis-79c5cc7cf8-redis\" is forbidden: User \"dpacak\" cannot get resource\n\"vulnerabilityreports\" in API group \"aquasecurity.github.io\" in the namespace \"\nfoo\"\n</code></pre> <pre><code>$ kubectl who-can get vuln/replicaset-nginx-7967dc8bfd-nginx -n foo\nROLEBINDING                                        NAMESPACE  SUBJECT  TYPE  SA-NAMESPACE\ndpacak-can-view-replicaset-nginx-7967dc8bfd-nginx  foo        dpacak   User\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>"},{"location":"tutorials/writing-custom-configuration-audit-policies/","title":"Writing Custom Configuration Audit Policies","text":"<p>Starboard ships with a set of Built-in Configuration Audit Policies defined as OPA Rego policies. You can also define custom policies and associate them with applicable Kubernetes resources to extend basic configuration audit functionality.</p> <p>This tutorial will walk through the process of creating and testing a new configuration audit policy that fails whenever a Kubernetes resource doesn't specify <code>app.kubernetes.io/name</code> or <code>app.kubernetes.io/version</code> labels.</p>"},{"location":"tutorials/writing-custom-configuration-audit-policies/#writing-a-policy","title":"Writing a Policy","text":"<p>To define such a policy, you must first define its metadata. This includes setting a unique identifier, title, severity (<code>CRITICAL</code>, <code>HIGH</code>, <code>MEDIUM</code>, <code>LOW</code>), descriptive text, and remediation steps. In Rego it's defined as the <code>__rego_metadata__</code> rule, which defines the following composite value:</p> <pre><code>package starboard.policy.k8s.custom\n\n__rego_metadata__ := {\n    \"id\": \"recommended_labels\",\n    \"title\": \"Recommended labels\",\n    \"severity\": \"LOW\",\n    \"type\": \"Kubernetes Security Check\",\n    \"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.\",\n    \"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n    \"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n}\n</code></pre> <p>Note that the <code>recommended_labels</code> policy in scoped to the <code>starboard.policy.k8s.custom</code> package to avoid naming collision with built-in policies that are pre-installed with Starboard.</p> <p>Once we've got our metadata defined, we need to create the logic of the policy, which is done in the <code>deny</code> or <code>warn</code> rule.</p> <pre><code>recommended_labels := [\n    \"app.kubernetes.io/name\",\n    \"app.kubernetes.io/version\",\n]\n\ndeny[res] {\n    provided := {label | input.metadata.labels[label]}\n    required := {label | label := recommended_labels[_]}\n    missing := required - provided\n    count(missing) &gt; 0\n    msg := sprintf(\"You must provide labels: %v\", [missing])\n    res := {\"msg\": msg}\n}\n</code></pre> <p>These matches are essentially Rego assertions, so anyone familiar with writing rules for OPA or other tools that use Rego should find the process familiar. In this case, it\u2019s pretty straightforward. We subtract the set of labels specified by the <code>input</code> resource object from the set of recommended labels. The resulting set is stored in the variable called <code>missing</code>. Finally, we check if the <code>missing</code> set is empty. If not, the <code>deny</code> rule fails with the appropriate message.</p> <p>The <code>input</code> document is set by Starboard to a Kubernetes resource when the policy is evaluated. For pods, it would look something like the following listing:</p> <pre><code>{\n  \"apiVersion\": \"v1\",\n  \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"nginx\",\n    \"labels\": {\n      \"run\": \"nginx\"\n    }\n  },\n  \"spec\": {\n    \"containers\": [\n      {\n        \"name\": \"nginx\",\n        \"image\": \"nginx:1.16\",\n      }\n    ]\n  }\n}\n</code></pre> <p>The labels set on the pod resource above can be retrieved with the following Rego expression:</p> <pre><code>provided := {label | input.metadata.labels[label]}\n</code></pre> <p>You can find the complete Rego code listing in recommended_labels.rego.</p>"},{"location":"tutorials/writing-custom-configuration-audit-policies/#testing-a-policy","title":"Testing a Policy","text":"<p>Now that you've created the policy, you need to test it to make sure it works as intended. To do that, add policy code to the <code>starboard-policies-config</code> ConfigMap and associate it with any (<code>*</code>) Kubernetes resource kind:</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: starboard-policies-config\n  namespace: starboard-system\n  labels:\n    app.kubernetes.io/name: starboard-operator\n    app.kubernetes.io/instance: starboard-operator\n    app.kubernetes.io/version: \"0.15.25-2-gab715b7\"\n    app.kubernetes.io/managed-by: kubectl\ndata:\n  policy.recommended_labels.kinds: \"*\"\n  policy.recommended_labels.rego: |\n    package starboard.policy.k8s.custom\n\n    __rego_metadata__ := {\n        \"id\": \"recommended_labels\",\n        \"title\": \"Recommended labels\",\n        \"severity\": \"LOW\",\n        \"type\": \"Kubernetes Security Check\",\n        \"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand\",\n        \"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n        \"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n    }\n\n    recommended_labels := [\n        \"app.kubernetes.io/name\",\n        \"app.kubernetes.io/version\",\n    ]\n\n    deny[res] {\n        provided := {label | input.metadata.labels[label]}\n        required := {label | label := recommended_labels[_]}\n        missing := required - provided\n        count(missing) &gt; 0\n        msg := sprintf(\"You must provide labels: %v\", [missing])\n        res := {\"msg\": msg}\n    }\n</code></pre> <p>In this example, to add a new policy, you must define two data entries in the <code>starboard-policies-config</code> ConfigMap:</p> <ol> <li>The <code>policy.&lt;your_policy_name&gt;.kinds</code> entry is used to designate applicable Kubernetes resources as a comma separated    list of Kubernetes kinds (e.g., <code>Pod,ConfigMap,NetworkPolicy</code>). There is also a special value (<code>Workload</code>) that you    can use to select all Kubernetes workloads, and (<code>*</code>) to select all Kubernetes resources recognized by Starboard.</li> <li>The <code>policy.&lt;your_policy_name&gt;.rego</code> entry holds the policy Rego code.</li> </ol> <p>Starboard automatically detects policies added to the <code>starboard-policies-config</code> ConfigMap and immediately rescans applicable Kubernetes resources.</p> <p>Let's create the <code>test</code> ConfigMap without recommended labels:</p> <pre><code>$ kubectl create cm test --from-literal=foo=bar\nconfigmap/test created\n</code></pre> <p>When you retrieve the corresponding configuration audit report, you'll see that there is one check with <code>LOW</code> severity that's failing:</p> <pre><code>$ kubectl get configauditreport configmap-test -o wide\nNAME             SCANNER     AGE   CRITICAL  HIGH   MEDIUM   LOW\nconfigmap-test   Starboard   24s   0         0      0        1\n</code></pre> <p>If you describe the report you'll see that it's failing because of our custom policy:</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  labels:\n    starboard.resource.kind: ConfigMap\n    starboard.resource.name: test\n    starboard.resource.namespace: default\n    plugin-config-hash: df767ff5f\n    resource-spec-hash: 7c96769cf\n  name: configmap-test\n  namespace: default\n  ownerReferences:\n  - apiVersion: v1\n    blockOwnerDeletion: false\n    controller: true\n    kind: ConfigMap\n    name: test\nreport:\n  scanner:\n    name: Starboard\n    vendor: Aqua Security\n    version: v0.15.25-2-gab715b7\n  summary:\n    criticalCount: 0\n    highCount: 0\n    lowCount: 1\n    mediumCount: 0\n  checks:\n  - checkID: recommended_labels  # (1)\n    title: Recommended labels    # (2)\n    severity: LOW                # (3)\n    category: Kubernetes Security Check  # (4)\n    description: |                       # (5)\n      A common set of labels allows tools to work interoperably,\n      describing objects in a common manner that all tools can\n      understand.\n    success: false  # (6)\n    messages:       # (7)\n    - 'You must provide labels: {\"app.kubernetes.io/name\", \"app.kubernetes.io/version\"}'\n</code></pre> <ol> <li>The <code>checkID</code> property corresponds to the policy identifier, i.e. <code>__rego_meatadata__.id</code>.</li> <li>The <code>title</code> property as defined by the policy metadata in <code>__rego_metadata__.title</code>.</li> <li>The <code>severity</code> property as defined by the policy metadata in <code>__rego_metadata__.severity</code>.</li> <li>The <code>category</code> property as defined by the policy metadata in <code>__rego_metadata__.type</code>.</li> <li>The <code>description</code> property as defined by the policy metadata in <code>__rego_metadata__.description</code>.</li> <li>The flag indicating whether the configuration audit check has failed or passed.</li> <li>The array of messages with details in case of failure.</li> </ol>"},{"location":"vulnerability-scanning/","title":"Vulnerability Scanners","text":"<p>Vulnerability scanning is an important way to identify and remediate security gaps in Kubernetes workloads. The process involves scanning container images to check all software on them and report any vulnerabilities found.</p> <p>Starboard Operator automatically discovers and scans all images that are being used in a Kubernetes cluster, including images of application pods and system pods. Scan reports are saved as VulnerabilityReport resources, which are owned by a Kubernetes controller.</p> <p>For example, when Starboard scans a Deployment, the corresponding VulnerabilityReport instance is attached to its current revision. In other words, the VulnerabilityReport inherits the life cycle of the Kubernetes controller. This also implies that when a Deployment is rolling updated, it will get scanned automatically, and a new instance of the VulnerabilityReport will be created and attached to the new revision. On the other hand, if the previous revision is deleted, the corresponding VulnerabilityReport will be deleted automatically by the Kubernetes garbage collector.</p> <p>The default vulnerability scanning capabilities in Starboard are provided by Trivy scanner. It also has a basic integration with Aqua Enterprise scanner.</p> <p>Starboard may scan Kubernetes workloads that run images from Private Registries and certain Managed Registries.</p>"},{"location":"vulnerability-scanning/aqua-enterprise/","title":"Aqua Enterprise Scanner","text":"<p>You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the <code>scannercli</code> command.</p> <p>The value of <code>aqua.imageRef</code> determines the version of the actual <code>scannercli</code> binary executable and must be compatible with the version of your Aqua server. By default, <code>scannercli</code> 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to <code>docker.io/aquasec/scanner:5.2</code>.</p> <p>To integrate Aqua scanner change the value of the <code>vulnerabilityReports.scanner</code> property to <code>Aqua</code>:</p> <pre><code>kubectl patch cm starboard -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"vulnerabilityReports.scanner\": \"Aqua\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>Specify the container image of Aqua scanner and server URL:</p> <pre><code>AQUA_SERVER_URL=&lt;your console URL&gt;\n\nkubectl create configmap starboard-aqua-config -n &lt;starboard_namespace&gt; \\\n  --from-literal=aqua.imageRef=docker.io/aquasec/scanner:5.3 \\\n  --from-literal=aqua.serverURL=$AQUA_SERVER_URL\n</code></pre> <p>Finally, create or edit the <code>starboard-aqua-config</code> secret to configure <code>aqua.username</code> and <code>aqua.password</code> credentials, which are used to connect to the Aqua's management console:</p> <pre><code>AQUA_CONSOLE_USERNAME=&lt;your username&gt;\nAQUA_CONSOLE_PASSWORD=&lt;your password&gt;\n\nkubectl create secret generic starboard-aqua-config -n &lt;starboard_namespace&gt; \\\n  --from-literal=aqua.username=$AQUA_CONSOLE_USERNAME \\\n  --from-literal=aqua.password=$AQUA_CONSOLE_PASSWORD\n</code></pre> <p>Tip</p> <p>You can use Helm installer to enable Aqua Enterprise scanner as follows: <pre><code>AQUA_SERVER_URL=&lt;your console URL&gt;\nAQUA_CONSOLE_USERNAME=&lt;your username&gt;\nAQUA_CONSOLE_PASSWORD=&lt;your password&gt;\n\nhelm install starboard-operator ./deploy/helm \\\n  --namespace starboard-system --create-namespace \\\n  --set=\"targetNamespaces=default\" \\\n  --set=\"operator.vulnerabilityReportsPlugin=Aqua\" \\\n  --set=\"aqua.imageRef=docker.io/aquasec/scanner:5.3\" \\\n  --set=\"aqua.serverURL=$AQUA_SERVER_URL\" \\\n  --set=\"aqua.username=$AQUA_CONSOLE_USERNAME\" \\\n  --set=\"aqua.password=$AQUA_CONSOLE_PASSWORD\"\n</code></pre></p>"},{"location":"vulnerability-scanning/aqua-enterprise/#settings","title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>aqua.imageRef</code> <code>docker.io/aquasec/scanner:5.3</code> Aqua scanner image reference. The tag determines the version of the <code>scanner</code> binary executable and it must be compatible with version of Aqua console. <code>aqua.serverURL</code> N/A The endpoint URL of Aqua management console SECRET KEY DESCRIPTION <code>aqua.username</code> Aqua management console username <code>aqua.password</code> Aqua management console password"},{"location":"vulnerability-scanning/managed-registries/","title":"Managed Registries","text":""},{"location":"vulnerability-scanning/managed-registries/#amazon-elastic-container-registry-ecr","title":"Amazon Elastic Container Registry (ECR)","text":"<p>You must create an IAM OIDC identity provider for your cluster:</p> <pre><code>eksctl utils associate-iam-oidc-provider \\\n  --cluster &lt;cluster_name&gt; \\\n  --approve\n</code></pre> <p>Assuming that the operator is installed in the <code>&lt;starboard_operator_namespace&gt;</code> namespace you can override the existing <code>starboard-operator</code> service account and attach the IAM policy to grant it permission to pull images from the ECR:</p> <pre><code>eksctl create iamserviceaccount \\\n  --name starboard-operator \\\n  --namespace &lt;starboard_operator_namespace&gt; \\\n  --cluster &lt;cluster_name&gt; \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\\n  --approve \\\n  --override-existing-serviceaccounts\n</code></pre>"},{"location":"vulnerability-scanning/managed-registries/#azure-container-registry-acr","title":"Azure Container Registry (ACR)","text":"<p>Before you can start, you need to install <code>aad-pod-identity</code> inside your cluster, see installation instructions: https://azure.github.io/aad-pod-identity/docs/getting-started/installation/</p> <p>Create a managed identity and assign the permission to the ACR. <pre><code>export IDENTITY_NAME=starboard-identity\nexport AZURE_RESOURCE_GROUP=&lt;my_resource_group&gt;\nexport AZURE_LOCATION=westeurope\nexport ACR_NAME=&lt;my_azure_container_registry&gt;\n\naz identity create --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --location ${AZURE_LOCATION}\n\nexport IDENTITY_ID=(az identity show --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --query id -o tsv)\nexport IDENTITY_CLIENT_ID=$(az identity show --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --query clientId -o tsv)\nexport ACR_ID=$(az acr show --name ${ACR_NAME} --query id -o tsv)\n\naz role assignment create --assignee ${IDENTITY_CLIENT_ID} --role 'AcrPull' --scope ${ACR_ID}\n</code></pre></p> <p>create an <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> resource inside your kubernetes cluster: <pre><code>apiVersion: aadpodidentity.k8s.io/v1\nkind: AzureIdentity\nmetadata:\n  name: starboard-identity\n  namespace: starboard\nspec:\n  clientID: ${IDENTITY_ID}\n  resourceID: ${IDENTITY_CLIENT_ID}\n  type: 0\n</code></pre></p> <pre><code> apiVersion: aadpodidentity.k8s.io/v1\n kind: AzureIdentityBinding\n metadata:\n   name: starboard-id-binding\n   namespace: starboard\n spec:\n   azureIdentity: starboard-identity\n   selector: starboard-label\n</code></pre> <p>add <code>scanJob.podTemplateLabels</code> to the starboard config map, the value must match the <code>AzureIdentityBinding</code> selector.</p> <pre><code>kubectl -n starboard edit cm starboard\n# Insert scanJob.podTemplateLabels: aadpodidbinding=starboard-label in data block\n\n# validate\nstarboard config --get scanJob.podTemplateLabels\n</code></pre>"},{"location":"vulnerability-scanning/private-registries/","title":"Private Registries","text":""},{"location":"vulnerability-scanning/private-registries/#image-pull-secrets","title":"Image Pull Secrets","text":"<ol> <li>Find references to image pull secrets (direct references and via service account).</li> <li>Create the temporary secret with basic credentials for each container of the scanned workload.</li> <li>Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job.</li> <li>Watch the job until it's completed or failed.</li> <li>Parse logs and save vulnerability reports in etcd.</li> <li>Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.</li> </ol>"},{"location":"vulnerability-scanning/trivy/","title":"Trivy Scanner","text":""},{"location":"vulnerability-scanning/trivy/#standalone","title":"Standalone","text":"<p>The default configuration settings enable Trivy <code>vulnerabilityReports.scanner</code> in <code>Standalone</code> <code>trivy.mode</code>. Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of the emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume.</p> <p></p> <p>The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers.</p> <p>Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the <code>trivy.githubToken</code> key to the <code>starboard</code> secret.</p> <pre><code>GITHUB_TOKEN=&lt;your token&gt;\n\nkubectl patch secret starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>"},{"location":"vulnerability-scanning/trivy/#clientserver","title":"ClientServer","text":"<p>You can connect Starboard to an external Trivy server by changing the default <code>trivy.mode</code> from <code>Standalone</code> to <code>ClientServer</code> and specifying <code>trivy.serverURL</code>.</p> <pre><code>TRIVY_SERVER_URL=&lt;your server URL&gt;\n\nkubectl patch cm starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.mode\":      \"ClientServer\",\n    \"trivy.serverURL\": \"$TRIVY_SERVER_URL\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy server.</p> <p>If the server requires access token and / or custom HTTP authentication headers, you may add <code>trivy.serverToken</code> and <code>trivy.serverCustomHeaders</code> properties to the <code>starboard</code> secret.</p> <pre><code>SERVER_TOKEN=&lt;your server token&gt;\nX_API_TOKEN=&lt;your API token&gt;\n\nkubectl patch secret starboard-trivy-config -n &lt;starboard_namespace&gt; \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.serverToken\":         \"$(echo -n $SERVER_TOKEN | base64)\",\n    \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre> <p></p>"},{"location":"vulnerability-scanning/trivy/#settings","title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>trivy.imageRef</code> <code>docker.io/aquasec/trivy:0.25.2</code> Trivy image reference <code>trivy.dbRepository</code> <code>ghcr.io/aquasecurity/trivy-db</code> External OCI Registry to download the vulnerability database <code>trivy.mode</code> <code>Standalone</code> Trivy client mode. Either <code>Standalone</code> or <code>ClientServer</code>. Depending on the active mode other settings might be applicable or required. <code>trivy.severity</code> <code>UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL</code> A comma separated list of severity levels reported by Trivy <code>trivy.ignoreUnfixed</code> N/A Whether to show only fixed vulnerabilities in vulnerabilities reported by Trivy. Set to <code>\"true\"</code> to enable it. <code>trivy.skipFiles</code> N/A A comma separated list of file paths for Trivy to skip traversal. <code>trivy.skipDirs</code> N/A A comma separated list of directories for Trivy to skip traversal. <code>trivy.ignoreFile</code> N/A It specifies the <code>.trivyignore</code> file which contains a list of vulnerability IDs to be ignored from vulnerabilities reported by Trivy. <code>trivy.timeout</code> <code>5m0s</code> The duration to wait for scan completion <code>trivy.serverURL</code> N/A The endpoint URL of the Trivy server. Required in <code>ClientServer</code> mode. <code>trivy.serverTokenHeader</code> <code>Trivy-Token</code> The name of the HTTP header to send the authentication token to Trivy server. Only application in <code>ClientServer</code> mode when <code>trivy.serverToken</code> is specified. <code>trivy.serverInsecure</code> N/A The Flag to enable insecure connection to the Trivy server. <code>trivy.insecureRegistry.&lt;id&gt;</code> N/A The registry to which insecure connections are allowed. There can be multiple registries with different registry <code>&lt;id&gt;</code>. <code>trivy.nonSslRegistry.&lt;id&gt;</code> N/A A registry without SSL. There can be multiple registries with different registry <code>&lt;id&gt;</code>. <code>trivy.registry.mirror.&lt;registry&gt;</code> N/A Mirror for the registry <code>&lt;registry&gt;</code>, e.g. <code>trivy.registry.mirror.index.docker.io: mirror.io</code> would use <code>mirror.io</code> to get images originated from <code>index.docker.io</code> <code>trivy.httpProxy</code> N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. <code>trivy.httpsProxy</code> N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub. <code>trivy.noProxy</code> N/A A comma separated list of IPs and domain names that are not subject to proxy settings. <code>trivy.resources.requests.cpu</code> <code>100m</code> The minimum amount of CPU required to run Trivy scanner pod. <code>trivy.resources.requests.memory</code> <code>100M</code> The minimum amount of memory required to run Trivy scanner pod. <code>trivy.resources.limits.cpu</code> <code>500m</code> The maximum amount of CPU allowed to run Trivy scanner pod. <code>trivy.resources.limits.memory</code> <code>500M</code> The maximum amount of memory allowed to run Trivy scanner pod. SECRET KEY DESCRIPTION <code>trivy.githubToken</code> The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in <code>Standalone</code> mode. <code>trivy.serverToken</code> The token to authenticate Trivy client with Trivy server. Only applicable in <code>ClientServer</code> mode. <code>trivy.serverCustomHeaders</code> A comma separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in <code>ClientServer</code> mode."}]}
{
  "apiVersion": "aquasecurity.github.io/v1alpha1",
  "kind": "CISKubeBenchReport",
  "metadata": {
    "creationTimestamp": "2022-02-23T12:52:35Z",
    "generation": 1,
    "labels": {
      "starboard.resource.kind": "Node",
      "starboard.resource.name": "local-control-plane"
    },
    "name": "local-control-plane",
    "ownerReferences": [
      {
        "apiVersion": "v1",
        "blockOwnerDeletion": false,
        "controller": true,
        "kind": "Node",
        "name": "local-control-plane",
        "uid": "bf0fad4c-59c4-4ad7-9f21-bd99cbb1a193"
      }
    ],
    "resourceVersion": "1109507",
    "uid": "59a63d18-cbf0-462b-8d13-390b87eb6de6"
  },
  "report": {
    "scanner": {
      "name": "kube-bench",
      "vendor": "Aqua Security",
      "version": "v0.6.5"
    },
    "sections": [
      {
        "id": "4",
        "node_type": "node",
        "tests": [
          {
            "desc": "Worker Node Configuration Files",
            "fail": 0,
            "info": 0,
            "pass": 10,
            "results": [
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 644 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n",
                "scored": true,
                "status": "PASS",
                "test_desc": "Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated)",
                "test_number": "4.1.1"
              },
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n",
                "scored": true,
                "status": "PASS",
                "test_desc": "Ensure that the kubelet service file ownership is set to root:root (Automated)",
                "test_number": "4.1.2"
              },
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 644 /etc/kubernetes/proxy.conf\n",
                "scored": false,
                "status": "FAIL",
                "test_desc": "If proxy kubeconfig file exists ensure permissions are set to 644 or more restrictive (Manual)",
                "test_number": "4.1.3"
              },
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example, chown root:root /etc/kubernetes/proxy.conf\n",
                "scored": false,
                "status": "FAIL",
                "test_desc": "If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)",
                "test_number": "4.1.4"
              },
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 644 /etc/kubernetes/kubelet.conf\n",
                "scored": true,
                "status": "PASS",
                "test_desc": "Ensure that the --kubeconfig kubelet.conf file permissions are set to 644 or more restrictive (Automated)",
                "test_number": "4.1.5"
              },
              {
                "remediation": "Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/kubernetes/kubelet.conf\n",
                "scored": false,
                "status": "PASS",
                "test_desc": "Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Manual)",
                "test_number": "4.1.6"
              },
              {
                "remediation": "Run the following command to modify the file permissions of the\n--client-ca-file chmod 644 <filename>\n",
                "scored": false,
                "status": "PASS",
                "test_desc": "Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual)",
                "test_number": "4.1.7"
              },
              {
                "remediation": "Run the following command to modify the ownership of the --client-ca-file.\nchown root:root <filename>\n",
                "scored": false,
                "status": "PASS",
                "test_desc": "Ensure that the client certificate authorities file ownership is set to root:root (Manual)",
                "test_number": "4.1.8"
              },
              {
                "remediation": "Run the following command (using the config file location identified in the Audit step)\nchmod 644 /var/lib/kubelet/config.yaml\n",
                "scored": true,
                "status": "PASS",
                "test_desc": "Ensure that the kubelet --config configuration file has permissions set to 644 or more restrictive (Automated)",
                "test_number": "4.1.9"
              },
              {
                "remediation": "Run the following command (using the config file location identified in the Audit step)\nchown root:root /var/lib/kubelet/config.yaml\n",
                "scored": true,
                "status": "PASS",
                "test_desc": "Ensure that the kubelet --config configuration file ownership is set to root:root (Automated)",
                "test_number": "4.1.10"
              }
            ],
            "section": "4.1",
            "warn": 0
          },
          {
            "desc": "Kubelet",
            "fail": 5,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to\nfalse.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--anonymous-auth=false\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": true,
                "status": "FAIL",
                "test_desc": "Ensure that the anonymous-auth argument is set to false (Automated)",
                "test_number": "4.2.1"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If\nusing executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--authorization-mode=Webhook\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": true,
                "status": "FAIL",
                "test_desc": "Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)",
                "test_number": "4.2.2"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to\nthe location of the client CA file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--client-ca-file=<path/to/client-ca-file>\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": true,
                "status": "FAIL",
                "test_desc": "Ensure that the --client-ca-file argument is set as appropriate (Automated)",
                "test_number": "4.2.3"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set readOnlyPort to 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--read-only-port=0\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --read-only-port argument is set to 0 (Manual)",
                "test_number": "4.2.4"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a\nvalue other than 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--streaming-connection-idle-timeout=5m\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)",
                "test_number": "4.2.5"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set protectKernelDefaults: true.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--protect-kernel-defaults=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": true,
                "status": "FAIL",
                "test_desc": "Ensure that the --protect-kernel-defaults argument is set to true (Automated)",
                "test_number": "4.2.6"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove the --make-iptables-util-chains argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": true,
                "status": "FAIL",
                "test_desc": "Ensure that the --make-iptables-util-chains argument is set to true (Automated)",
                "test_number": "4.2.7"
              },
              {
                "remediation": "Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and remove the --hostname-override argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --hostname-override argument is not set (Manual)",
                "test_number": "4.2.8"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture (Manual)",
                "test_number": "4.2.9"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set tlsCertFile to the location\nof the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile\nto the location of the corresponding private key file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameters in KUBELET_CERTIFICATE_ARGS variable.\n--tls-cert-file=<path/to/tls-certificate-file>\n--tls-private-key-file=<path/to/tls-key-file>\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)",
                "test_number": "4.2.10"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to add the line rotateCertificates: true or\nremove it altogether to use the default value.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove --rotate-certificates=false argument from the KUBELET_CERTIFICATE_ARGS\nvariable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the --rotate-certificates argument is not set to false (Manual)",
                "test_number": "4.2.11"
              },
              {
                "remediation": "Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable.\n--feature-gates=RotateKubeletServerCertificate=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Verify that the RotateKubeletServerCertificate argument is set to true (Manual)",
                "test_number": "4.2.12"
              },
              {
                "remediation": "If using a Kubelet config file, edit the file to set TLSCipherSuites: to\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nor to a subset of these values.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the --tls-cipher-suites parameter as follows, or to a subset of these values.\n--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)",
                "test_number": "4.2.13"
              }
            ],
            "section": "4.2",
            "warn": 8
          }
        ],
        "text": "Worker Node Security Configuration",
        "total_fail": 5,
        "total_info": 0,
        "total_pass": 10,
        "total_warn": 8,
        "version": "cis-1.20"
      },
      {
        "id": "5",
        "node_type": "policies",
        "tests": [
          {
            "desc": "RBAC and Service Accounts",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and\nif they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role :\nkubectl delete clusterrolebinding [name]\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the cluster-admin role is only used where required (Manual)",
                "test_number": "5.1.1"
              },
              {
                "remediation": "Where possible, remove get, list and watch access to secret objects in the cluster.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize access to secrets (Manual)",
                "test_number": "5.1.2"
              },
              {
                "remediation": "Where possible replace any use of wildcards in clusterroles and roles with specific\nobjects or actions.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize wildcard use in Roles and ClusterRoles (Manual)",
                "test_number": "5.1.3"
              },
              {
                "remediation": "Where possible, remove create access to pod objects in the cluster.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize access to create pods (Manual)",
                "test_number": "5.1.4"
              },
              {
                "remediation": "Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\nautomountServiceAccountToken: false\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that default service accounts are not actively used. (Manual)",
                "test_number": "5.1.5"
              },
              {
                "remediation": "Modify the definition of pods and service accounts which do not need to mount service\naccount tokens to disable it.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that Service Account Tokens are only mounted where necessary (Manual)",
                "test_number": "5.1.6"
              },
              {
                "remediation": "Remove the system:masters group from all users in the cluster.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Avoid use of system:masters group (Manual)",
                "test_number": "5.1.7"
              },
              {
                "remediation": "Where possible, remove the impersonate, bind and escalate rights from subjects.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)",
                "test_number": "5.1.8"
              }
            ],
            "section": "5.1",
            "warn": 8
          },
          {
            "desc": "Pod Security Policies",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that\nthe .spec.privileged field is omitted or set to false.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of privileged containers (Automated)",
                "test_number": "5.2.1"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostPID field is omitted or set to false.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers wishing to share the host process ID namespace (Automated)",
                "test_number": "5.2.2"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostIPC field is omitted or set to false.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers wishing to share the host IPC namespace (Automated)",
                "test_number": "5.2.3"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.hostNetwork field is omitted or set to false.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers wishing to share the host network namespace (Automated)",
                "test_number": "5.2.4"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.allowPrivilegeEscalation field is omitted or set to false.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers with allowPrivilegeEscalation (Automated)",
                "test_number": "5.2.5"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.runAsUser.rule is set to either MustRunAsNonRoot or MustRunAs with the range of\nUIDs not including 0.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of root containers (Automated)",
                "test_number": "5.2.6"
              },
              {
                "remediation": "Create a PSP as described in the Kubernetes documentation, ensuring that the\n.spec.requiredDropCapabilities is set to include either NET_RAW or ALL.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers with the NET_RAW capability (Automated)",
                "test_number": "5.2.7"
              },
              {
                "remediation": "Ensure that allowedCapabilities is not present in PSPs for the cluster unless\nit is set to an empty array.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers with added capabilities (Automated)",
                "test_number": "5.2.8"
              },
              {
                "remediation": "Review the use of capabilites in applications running on your cluster. Where a namespace\ncontains applicaions which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Minimize the admission of containers with capabilities assigned (Manual)",
                "test_number": "5.2.9"
              }
            ],
            "section": "5.2",
            "warn": 9
          },
          {
            "desc": "Network Policies and CNI",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\nin the Kubernetes cluster.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the CNI in use supports Network Policies (Manual)",
                "test_number": "5.3.1"
              },
              {
                "remediation": "Follow the documentation and create NetworkPolicy objects as you need them.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that all Namespaces have Network Policies defined (Manual)",
                "test_number": "5.3.2"
              }
            ],
            "section": "5.3",
            "warn": 2
          },
          {
            "desc": "Secrets Management",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "if possible, rewrite application code to read secrets from mounted secret files, rather than\nfrom environment variables.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Prefer using secrets as files over secrets as environment variables (Manual)",
                "test_number": "5.4.1"
              },
              {
                "remediation": "Refer to the secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Consider external secret storage (Manual)",
                "test_number": "5.4.2"
              }
            ],
            "section": "5.4",
            "warn": 2
          },
          {
            "desc": "Extensible Admission Control",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "Follow the Kubernetes documentation and setup image provenance.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)",
                "test_number": "5.5.1"
              }
            ],
            "section": "5.5",
            "warn": 1
          },
          {
            "desc": "General Policies",
            "fail": 0,
            "info": 0,
            "pass": 0,
            "results": [
              {
                "remediation": "Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Create administrative boundaries between resources using namespaces (Manual)",
                "test_number": "5.7.1"
              },
              {
                "remediation": "Use security context to enable the docker/default seccomp profile in your pod definitions.\nAn example is as below:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Ensure that the seccomp profile is set to docker/default in your pod definitions (Manual)",
                "test_number": "5.7.2"
              },
              {
                "remediation": "Follow the Kubernetes documentation and apply security contexts to your pods. For a\nsuggested list of security contexts, you may refer to the CIS Security Benchmark for Docker\nContainers.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "Apply Security Context to Your Pods and Containers (Manual)",
                "test_number": "5.7.3"
              },
              {
                "remediation": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n",
                "scored": false,
                "status": "WARN",
                "test_desc": "The default namespace should not be used (Manual)",
                "test_number": "5.7.4"
              }
            ],
            "section": "5.7",
            "warn": 4
          }
        ],
        "text": "Kubernetes Policies",
        "total_fail": 0,
        "total_info": 0,
        "total_pass": 0,
        "total_warn": 26,
        "version": "cis-1.20"
      }
    ],
    "summary": {
      "failCount": 5,
      "infoCount": 0,
      "passCount": 10,
      "warnCount": 34
    },
    "updateTimestamp": "2022-02-23T12:52:35Z"
  }
}
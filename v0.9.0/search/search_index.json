{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Starboard Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom resources definitions and a Go module to work with a range of existing security scanners, as well as a kubectl -compatible command, the Octant plugin , and the Lens extension that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command , so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security reports in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. Tip Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes workloads. Therefore, the operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments, and running appropriate scanners against the underlying deployment descriptors. You can read more about the motivations and use cases here and join our discussions . We also gave a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020 .","title":"Introduction"},{"location":"#welcome-to-starboard","text":"Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom resources definitions and a Go module to work with a range of existing security scanners, as well as a kubectl -compatible command, the Octant plugin , and the Lens extension that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command , so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security reports in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. Tip Even though manual scanning through the command-line is useful, the fact that it's not automated makes it less suitable with a large number of Kubernetes workloads. Therefore, the operator provides a better option for these scenarios, constantly monitoring built-in Kubernetes resources, such as Deployments, and running appropriate scanners against the underlying deployment descriptors. You can read more about the motivations and use cases here and join our discussions . We also gave a detailed introduction to Starboard with demos at KubeCon + CloudNativeCon NA 2020 .","title":"Welcome to Starboard"},{"location":"crds/","text":"Overview This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport Note We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations. VulnerabilityReport An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For multi-container workloads Starboard creates multiple instances of VulnerabilityReports, which are stored in the same namespace and are owned by this workload. Each report follows the naming convention <workload kind>-<workload name>-<container-name> . Note For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288 ). Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here . ConfigAuditReport An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris , against a Kubernetes workload's configuration. For example, check that a given container image runs as non root user or that a container has resource requests and limits set. Currently checks only relate to Kubernetes workloads, but most likely we'll extend this model to cater for other Kubernetes objects such as Services, ConfigMaps, etc (see #300 ). Each report owned by the underlying Kubernetes workload and is stored in the same namespace, following the <workload-kind>-<workload-name> naming convention. Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard. Note The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls. CISKubeBenchReport The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node. We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench . KubeHunterReport The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter 's output, but we can make it more generic to onboard third party pen testing tools. There's zero to one instances of KubeHunterReports with hardcoded name cluster without any owner reference being set as there's no built-in Kubernetes resource that represents a cluster.","title":"Custom Resource Definitions"},{"location":"crds/#overview","text":"This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport Note We are open to suggestions for adding new or changes to the existing CRDs in the case that would enable additional third-party integrations.","title":"Overview"},{"location":"crds/#vulnerabilityreport","text":"An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For multi-container workloads Starboard creates multiple instances of VulnerabilityReports, which are stored in the same namespace and are owned by this workload. Each report follows the naming convention <workload kind>-<workload name>-<container-name> . Note For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288 ). Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with Starboard. You can find the list of available integrations here .","title":"VulnerabilityReport"},{"location":"crds/#configauditreport","text":"An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as Polaris , against a Kubernetes workload's configuration. For example, check that a given container image runs as non root user or that a container has resource requests and limits set. Currently checks only relate to Kubernetes workloads, but most likely we'll extend this model to cater for other Kubernetes objects such as Services, ConfigMaps, etc (see #300 ). Each report owned by the underlying Kubernetes workload and is stored in the same namespace, following the <workload-kind>-<workload-name> naming convention. Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with Starboard. Note The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.","title":"ConfigAuditReport"},{"location":"crds/#ciskubebenchreport","text":"The CISKubeBenchReport is a cluster scoped resource owned by a Kubernetes node, which represents the latest result of running CIS Kubernetes Benchmark tests on that node. It's named after a corresponding node. We do not anticipate many (at all) kube-bench alike tools, hence the schema of this report is currently the same as the output of kube-bench .","title":"CISKubeBenchReport"},{"location":"crds/#kubehunterreport","text":"The KubeHunterReport is a cluster scoped resource which represents the outcome of running pen tests against your cluster. Currently the data model is the same as kube-hunter 's output, but we can make it more generic to onboard third party pen testing tools. There's zero to one instances of KubeHunterReports with hardcoded name cluster without any owner reference being set as there's no built-in Kubernetes resource that represents a cluster.","title":"KubeHunterReport"},{"location":"faq/","text":"Frequently Asked Questions Why do you duplicate instances of VulnerabilityReports for the same image digest? Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected. Why do you create an instance of the VulnerabilityReport for each container? The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB. Is Starboard CLI required to run Starboard Operator or vice versa? No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","text":"Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","text":"The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.","title":"Why do you create an instance of the VulnerabilityReport for each container?"},{"location":"faq/#is-starboard-cli-required-to-run-starboard-operator-or-vice-versa","text":"No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Is Starboard CLI required to run Starboard Operator or vice versa?"},{"location":"settings/","text":"The Starboard CLI and Starboard Operator both read their configuration settings from a ConfigMap, as well as a secret that holds confidential settings (such as a GitHub token). The starboard init command creates the starboard ConfigMap and the starboard secret in the starboard namespace with default settings. Similarly, the operator ensures the starboard ConfigMap and the starboard secret in the OPERATOR_NAMESPACE . You can change the default settings with kubectl patch or kubectl edit commands. For example, by default Trivy displays vulnerabilities with all severity levels ( UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL ). However, you can opt in to display only HIGH and CRITICAL vulnerabilities by patching the trivy.severity value in the starboard ConfigMap: kubectl patch cm starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.severity\": \"HIGH,CRITICAL\" } } EOF )\" To set the GitHub token used by Trivy in Standalone mode add the trivy.githubToken value to the starboard secret instead: GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" The following tables list available configuration settings with their default values. Tip You only need to configure the settings for the scanner you are using (i.e. trivy.* parameters are used if vulnerabilityReports.scanner is set to Trivy ). Check integrations page to see example configuration settings for common use cases. CONFIGMAP KEY DEFAULT DESCRIPTION vulnerabilityReports.scanner Trivy The name of the scanner that generates vulnerability reports. Either Trivy or Aqua . trivy.httpProxy N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL A comma separated list of severity levels reported by Trivy trivy.imageRef docker.io/aquasec/trivy:0.14.0 Trivy image reference trivy.mode Standalone Trivy client mode. Either Standalone or ClientServer . Depending on the active mode other settings might be applicable or required. trivy.serverURL N/A The endpoint URL of the Trivy server. Required in ClientServer mode. trivy.serverTokenHeader Trivy-Token The name of the HTTP header to send the authentication token to Trivy server. Only application in ClientServer mode when trivy.serverToken is specified. aqua.imageRef docker.io/aquasec/scanner:5.3 Aqua scanner image reference. The tag determines the version of the scanner binary executable and it must be compatible with version of Aqua console. aqua.serverURL N/A The endpoint URL of Aqua management console kube-bench.imageRef docker.io/aquasec/kube-bench:0.4.0 kube-bench image reference kube-hunter.imageRef docker.io/aquasec/kube-hunter:0.4.0 kube-hunter image reference kube-hunter.quick \"false\" Whether to use kube-hunter's \"quick\" scanning mode (subnet 24). Set to \"true\" to enable. polaris.imageRef quay.io/fairwinds/polaris:3.0 Polaris image reference polaris.config.yaml Check the default value here Polaris configuration file SECRET KEY DESCRIPTION trivy.githubToken The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.serverToken The token to authenticate Trivy client with Trivy server. Only applicable in ClientServer mode. trivy.serverCustomHeaders A comma-separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in ClientServer mode. aqua.username Aqua management console username aqua.password Aqua management console password Tip You can find it handy to delete a configuration key, which was not created by default by the starboard init command. For example, the following kubectl patch command deletes the trivy.httpProxy key: kubectl patch cm starboard -n <starboard_operator> \\ --type json \\ -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'","title":"Starboard Settings"},{"location":"cli/","text":"Starboard CLI is a single executable binary which can be used to find risks, such as vulnerabilities or insecure pod descriptors, in Kubernetes workloads. By default, the risk assessment reports are stored as custom resources . To learn more about the available Starboard CLI commands, run starboard help or type a command followed by the -h flag: starboard scan kubehunterreports -h","title":"Overview"},{"location":"cli/getting-started/","text":"The easiest way to get started with Starboard is to use the starboard command, which allows scanning Kubernetes workloads deployed in your cluster. To begin with, execute the following one-time setup command: starboard init The init subcommand creates the starboard namespace, in which Starboard executes Kubernetes jobs to perform scans. It also sends custom security resources definitions to the Kubernetes API: $ kubectl api-resources --api-group aquasecurity.github.io NAME SHORTNAMES APIGROUP NAMESPACED KIND ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport Tip There's also a starboard cleanup subcommand, which can be used to remove all resources created by Starboard. As an example let's run in the current namespace an old version of nginx that we know has vulnerabilities: kubectl create deployment nginx --image nginx:1.16 Run the vulnerability scanner to generate vulnerability reports: starboard scan vulnerabilityreports deployment/nginx Behind the scenes, by default this uses Trivy in Standalone mode to identify vulnerabilities in the container images associated with the specified deployment. Once this has been done, you can retrieve the latest vulnerability reports for this workload: starboard get vulnerabilities deployment/nginx -o yaml Starboard relies on labels and label selectors to associate vulnerability reports with the specified Deployment. For a Deployment with N container images Starboard creates N instances of vulnerabilityreports.aquasecurity.github.io resources. In addition, each instance has the starboard.container.name label to associate it with a particular container's image. This means that the same data retrieved by the starboard get vulnerabilities subcommand can be fetched with the standard kubectl get command: kubectl get vulnerabilityreports -o yaml \\ -l starboard.resource.kind=Deployment,starboard.resource.name=nginx In this example, the nginx deployment has a single container called nginx , hence only one instance of the vulnerabilityreports.aquasecurity.github.io resource is created with the label starboard.container.name=nginx . To read more about custom resources and label selectors check custom resource definitions . Next Steps Let's take the same nginx Deployment and audit its Kubernetes configuration. As you remember we've created it with the kubectl create deployment command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure. Run the scanner to audit the configuration using Polaris : starboard scan configauditreports deployment/nginx Retrieve the configuration audit report: starboard get configaudit deployment/nginx -o yaml or kubectl get configauditreport -o yaml \\ -l starboard.resource.kind=Deployment,starboard.resource.name=nginx To learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter , use starboard help .","title":"Getting Started"},{"location":"cli/getting-started/#next-steps","text":"Let's take the same nginx Deployment and audit its Kubernetes configuration. As you remember we've created it with the kubectl create deployment command which applies the default settings to the deployment descriptors. However, we also know that in Kubernetes the defaults are usually the least secure. Run the scanner to audit the configuration using Polaris : starboard scan configauditreports deployment/nginx Retrieve the configuration audit report: starboard get configaudit deployment/nginx -o yaml or kubectl get configauditreport -o yaml \\ -l starboard.resource.kind=Deployment,starboard.resource.name=nginx To learn more about the available Starboard commands and scanners, such as kube-bench or kube-hunter , use starboard help .","title":"Next Steps"},{"location":"cli/troubleshooting/","text":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS) Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"Troubleshooting"},{"location":"cli/troubleshooting/#starboard-cannot-be-opened-because-the-developer-cannot-be-verified-macos","text":"Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS)"},{"location":"cli/installation/binary-releases/","text":"Every release of Starboard provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard_darwin_x86_64.tar.gz ) Find the starboard binary in the unpacked directory, and move it to its desired destination ( mv starboard_darwin_x86_64/starboard /usr/local/bin/starboard ) From there, you should be able to run Starboard CLI commands: starboard help kubectl plugin The Starboard CLI is compatible with kubectl and is intended as kubectl plugin , but it's perfectly fine to run it as a stand-alone executable. If you rename the starboard executable to kubectl-starboard and if it's in your path, you can invoke it using kubectl starboard .","title":"From the Binary Releases"},{"location":"cli/installation/binary-releases/#kubectl-plugin","text":"The Starboard CLI is compatible with kubectl and is intended as kubectl plugin , but it's perfectly fine to run it as a stand-alone executable. If you rename the starboard executable to kubectl-starboard and if it's in your path, you can invoke it using kubectl starboard .","title":"kubectl plugin"},{"location":"cli/installation/docker/","text":"We also release Docker images aquasec/starboard:0.9.0 and public.ecr.aws/aquasecurity/starboard:0.9.0 to run Starboard as a Docker container or to manually schedule Kubernetes scan Jobs in your cluster. $ docker container run --rm public.ecr.aws/aquasecurity/starboard:0.9.0 version Starboard Version: {Version:0.9.0 Commit:ffae344d40ad450c007353a2294d9e244d750157 Date:2021-01-26T17:49:49Z}","title":"Docker"},{"location":"cli/installation/krew/","text":"You can also install Starboard as a kubectl plugin with the Krew plugins manager: kubectl krew install starboard kubectl starboard help If you already installed Starboard plugin with Krew, you can upgrade it to the latest version with: kubectl krew upgrade starboard","title":"Krew"},{"location":"cli/installation/source/","text":"Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of Starboard. You must have a working Go environment. git clone --depth 1 --branch v0.9.0 git@github.com:aquasecurity/starboard.git cd starboard make If required, it will fetch the dependencies and cache them. It will then compile starboard and place it in bin/starboard .","title":"From Source (Linux, macOS)"},{"location":"integrations/lens/","text":"Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes. Starboard Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.","title":"Lens Extension"},{"location":"integrations/managed-registries/","text":"Amazon Elastic Container Registry (ECR) You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Managed Registries"},{"location":"integrations/managed-registries/#amazon-elastic-container-registry-ecr","text":"You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Amazon Elastic Container Registry (ECR)"},{"location":"integrations/octant/","text":"Octant is a tool for developers to understand how applications run on a Kubernetes cluster. It aims to be part of the developer's toolkit for gaining insight and approaching complexity found in Kubernetes. Octant offers a combination of introspective tooling, cluster navigation, and object management along with a plugin system to further extend its capabilities. Starboard Octant Plugin provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources. Installation Prerequisites Octant >= 0.13 should first be installed. On macOS this is as simple as brew install octant . For installation instructions on other operating systems and package managers, see Octant Installation . Environment authenticated against your Kubernetes cluster Tip In the following instructions we assume that the $HOME/.config/octant/plugins directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the OCTANT_PLUGIN_PATH environment variable when running Octant. From the Binary Releases Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard-octant-plugin_darwin_x86_64.tar ) Find the starboard-octant-plugin binary in the unpacked directory, and move it to the default Octant's configuration directory ( mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins ). You might need to create the directory if it doesn't exist already. From Source (Linux, macOS) Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin. You must have a working Go environment. git clone git@github.com:aquasecurity/starboard-octant-plugin.git cd starboard-octant-plugin make install The make install goal copies the plugin binary to the $HOME/.config/octant/plugins directory. Uninstall Run the following command to remove the plugin: rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin where $OCTANT_PLUGIN_PATH is the default plugins location respected by Octant. If not set, it defaults to the $HOME/.config/octant/plugins directory.","title":"Octant Plugin"},{"location":"integrations/octant/#installation","text":"","title":"Installation"},{"location":"integrations/octant/#prerequisites","text":"Octant >= 0.13 should first be installed. On macOS this is as simple as brew install octant . For installation instructions on other operating systems and package managers, see Octant Installation . Environment authenticated against your Kubernetes cluster Tip In the following instructions we assume that the $HOME/.config/octant/plugins directory is the default plugins location respected by Octant. Note that the default location might be changed by setting the OCTANT_PLUGIN_PATH environment variable when running Octant.","title":"Prerequisites"},{"location":"integrations/octant/#from-the-binary-releases","text":"Every release of Starboard Octant Plugin provides binary releases for a variety of operating systems. These binary versions can be manually downloaded and installed. Download your desired version Unpack it ( tar -zxvf starboard-octant-plugin_darwin_x86_64.tar ) Find the starboard-octant-plugin binary in the unpacked directory, and move it to the default Octant's configuration directory ( mv starboard-octant-plugin_darwin_x86_64/starboard-octant-plugin $HOME/.config/octant/plugins ). You might need to create the directory if it doesn't exist already.","title":"From the Binary Releases"},{"location":"integrations/octant/#from-source-linux-macos","text":"Building from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) version of the plugin. You must have a working Go environment. git clone git@github.com:aquasecurity/starboard-octant-plugin.git cd starboard-octant-plugin make install The make install goal copies the plugin binary to the $HOME/.config/octant/plugins directory.","title":"From Source (Linux, macOS)"},{"location":"integrations/octant/#uninstall","text":"Run the following command to remove the plugin: rm -f $OCTANT_PLUGIN_PATH/starboard-octant-plugin where $OCTANT_PLUGIN_PATH is the default plugins location respected by Octant. If not set, it defaults to the $HOME/.config/octant/plugins directory.","title":"Uninstall"},{"location":"integrations/private-registries/","text":"Image Pull Secrets Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.","title":"Private Registries"},{"location":"integrations/private-registries/#image-pull-secrets","text":"Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.","title":"Image Pull Secrets"},{"location":"integrations/vulnerability-scanners/","text":"","title":"Overview"},{"location":"integrations/vulnerability-scanners/aqua-enterprise/","text":"You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the scannercli command. The value of aqua.imageRef determines the version of the actual scannercli binary executable and must be compatible with the version of your Aqua deployment. By default, scannercli 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to docker.io/aquasec/scanner:5.2 . To integrate Aqua scanner change the value of the vulnerabilityReports.scanner property to Aqua and specify the aqua.serverURL : AQUA_SERVER_URL=<your console URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"vulnerabilityReports.scanner\": \"Aqua\", \"aqua.serverURL\": \"$AQUA_SERVER_URL\" } } EOF )\" Finally, edit the starboard secret to configure aqua.username and aqua.password credentials, which are used to connect to the Aqua's management console: AQUA_CONSOLE_USERNAME=<your username> AQUA_CONSOLE_PASSWORD=<your password> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"aqua.username\": \"$(echo -n $AQUA_CONSOLE_USERNAME | base64)\", \"aqua.password\": \"$(echo -n $AQUA_CONSOLE_PASSWORD | base64)\" } } EOF )\"","title":"Aqua Enterprise"},{"location":"integrations/vulnerability-scanners/trivy/","text":"Standalone The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each pod created by a scan job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with the main containers that perform the actual scanning. Finally, the pod is deleted along with the emptyDir volume. The number of containers defined by a scan job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" ClientServer You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\"","title":"Trivy"},{"location":"integrations/vulnerability-scanners/trivy/#standalone","text":"The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each pod created by a scan job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with the main containers that perform the actual scanning. Finally, the pod is deleted along with the emptyDir volume. The number of containers defined by a scan job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\"","title":"Standalone"},{"location":"integrations/vulnerability-scanners/trivy/#clientserver","text":"You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\"","title":"ClientServer"},{"location":"operator/","text":"This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new pod is started. In other words, the desired state for this operator is that for each workload there are security reports stored in the cluster as custom resources. Warning Currently, the operator only supports vulnerabilityreports and configauditreports security resources as depicted below. However, we plan to support all custom security resources .","title":"Overview"},{"location":"operator/configuration/","text":"Configuration of the operator's pod is done via environment variables at startup. NAME DEFAULT DESCRIPTION OPERATOR_NAMESPACE N/A See Install modes OPERATOR_TARGET_NAMESPACES N/A See Install modes OPERATOR_LOG_DEV_MODE false The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). OPERATOR_SCAN_JOB_TIMEOUT 5m The length of time to wait before giving up on a scan job OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT 3 The maximum number of scan jobs create by the operator OPERATOR_SCAN_JOB_RETRY_AFTER 30s The duration to wait before retrying a failed scan job OPERATOR_METRICS_BIND_ADDRESS :8080 The TCP address to bind to for serving Prometheus metrics. It can be set to 0 to disable the metrics serving. OPERATOR_HEALTH_PROBE_BIND_ADDRESS :9090 The TCP address to bind to for serving health probes, i.e. /healthz/ and /readyz/ endpoints. Install Modes The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Configuration"},{"location":"operator/configuration/#install-modes","text":"The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Install Modes"},{"location":"operator/getting-started/","text":"Assuming that you installed the operator in the starboard-operator namespace, and it's configured to discover Kubernetes workloads in the default namespace, let's create the nginx Deployment that we know is vulnerable: kubectl create deployment nginx --image nginx:1.16 When the first pod controlled by the nginx Deployment is started, the operator immediately detects that and creates the Kubernetes job in the starboard-operator namespace to scan the nginx container's image ( nginx:1.16 ) for vulnerabilities. It also creates another job to audit the pod's template for common configuration pitfalls such as running the nginx container as root: $ kubectl get job -n starboard-operator NAME COMPLETIONS DURATION AGE scan-configauditreport-c4956cb9d 0/1 1s 1s scan-vulnerabilityreport-c4956cb9d 0/1 1s 1s If everything goes fine, the scan jobs are deleted, and the operator saves scan reports as custom resources in the default namespace, named after the Deployment's active ReplicaSet. For image vulnerability scans, the operator creates a report for each different container defined in the active ReplicaSet. In this example there is just one container image called nginx : $ kubectl get vulnerabilityreports -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN replicaset-nginx-7ff78f74b9-nginx library/nginx 1.16 Trivy 9m57s 1 33 22 86 0 Similarly, the operator creates a configauditreport resource holding the result of auditing the configuration of the active ReplicaSet controlled by the nginx Deployment: $ kubectl get configauditreports -o wide NAME SCANNER PASS DANGER WARNING replicaset-nginx-7ff78f74b9 Polaris 9 0 8 Notice that scan reports generated by the operator are owned by corresponding Kubernetes workloads. In our example, vulnerabilityreports and configauditreports objects are owned by the active ReplicaSet of the nginx Deployment: $ kubectl tree deploy nginx NAMESPACE NAME READY REASON AGE default Deployment/nginx - 44m default \u2514\u2500ReplicaSet/nginx-7ff78f74b9 - 44m default \u251c\u2500ConfigAuditReport/replicaset-nginx-7ff78f74b9 - 44m default \u251c\u2500Pod/nginx-7ff78f74b9-l6w8p True 44m default \u2514\u2500VulnerabilityReport/replicaset-nginx-7ff78f74b9-nginx - 43m Note The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree. Tip You can get and describe vulnerabilityreports and configauditreports as built-in Kubernetes objects: kubectl get vulnerabilityreports replicaset-nginx-7ff78f74b9-nginx -o json kubectl describe configauditreports replicaset-nginx-7ff78f74b9","title":"Getting Started"},{"location":"operator/installation/helm/","text":"Helm , which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts . To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard operator. The Helm chart supports all install modes . As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespaces: Clone the chart repository: git clone --depth 1 --branch v0.9.0 https://github.com/aquasecurity/starboard.git cd starboard Create the starboard-operator namespace: kubectl create namespace starboard-operator (Optional) Configure Starboard by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. For example, you can use Trivy in ClientServer mode or Aqua Enterprise as an active vulnerability scanner. If you skip this step, the operator will ensure configuration objects on startup with the default settings: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Install the chart: helm install starboard-operator ./deploy/helm \\ -n starboard-operator \\ --set=\"targetNamespaces=default\" Check that the starboard-operator Helm release is created in the starboard-operator namespace: $ helm list -n starboard-operator NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION starboard-operator starboard-operator 1 2021-01-27 20:09:53.158961 +0100 CET deployed starboard-operator-0.4.0 0.9.0 To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: $ kubectl get deployment -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator Note You have to manually delete custom resource definitions created by the helm install command: kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"Helm"},{"location":"operator/installation/helm/#uninstall","text":"You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator Note You have to manually delete custom resource definitions created by the helm install command: kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"Uninstall"},{"location":"operator/installation/kubectl/","text":"You can install the operator with provided static YAML manifests with fixed values. However, this approach has its shortcomings. For example, if you want to change the container image or modify default configuration settings, you have to create new manifests or edit existing ones. As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespace: Send custom resource definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/configauditreports.crd.yaml Send the following Kubernetes objects definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/01-starboard-operator.ns.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/04-starboard-operator.clusterrolebinding.yaml (Optional) Configure Starboard by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. For example, you can use Trivy in ClientServer mode or Aqua Enterprise as an active vulnerability scanner. If you skip this step, the operator will ensure configuration objects on startup with the default settings: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Finally, create the starboard-operator Deployment in the starboard-operator namespace to start the operator's pod: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/06-starboard-operator.deployment.yaml To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: $ kubectl get deployment -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/05-starboard-operator.config.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/01-starboard-operator.ns.yaml Delete custom resources definitions: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/configauditreports.crd.yaml Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"kubectl"},{"location":"operator/installation/kubectl/#uninstall","text":"You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/05-starboard-operator.config.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/01-starboard-operator.ns.yaml Delete custom resources definitions: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/vulnerabilityreports.crd.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/crd/configauditreports.crd.yaml Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"Uninstall"},{"location":"operator/installation/olm/","text":"The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies. You can install the Starboard operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod. As an example, let's install the operator from the OperatorHub catalog in the starboard-operator namespace and configure it to watch the default namespaces: Install the Operator Lifecycle Manager: curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/0.16.1/install.sh | bash -s 0.16.1 Create the namespace to install the operator in: kubectl create ns starboard-operator Declare the target namespaces by creating the OperatorGroup: cat << EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha2 kind: OperatorGroup metadata: name: starboard-operator namespace: starboard-operator spec: targetNamespaces: - default EOF (Optional) Configure Starboard by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. For example, you can use Trivy in ClientServer mode or Aqua Enterprise as an active vulnerability scanner. If you skip this step, the operator will ensure configuration objects on startup with the default settings: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/v0.9.0/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Install the operator by creating the Subscription: cat << EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: starboard-operator namespace: starboard-operator spec: channel: alpha name: starboard-operator source: operatorhubio-catalog sourceNamespace: olm installPlanApproval: Automatic config: env: - name: OPERATOR_SCAN_JOB_TIMEOUT value: \"60s\" - name: OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT value: \"10\" - name: OPERATOR_LOG_DEV_MODE value: \"true\" EOF The operator will be installed in the starboard-operator namespace and will be usable from the default namespace. Note that the spec.config property allows you to override the default configuration of the operator's Deployment. After install, watch the operator come up using the following command: $ kubectl get clusterserviceversions -n starboard-operator NAME DISPLAY VERSION REPLACES PHASE starboard-operator.v0.9.0 Starboard Operator 0.9.0 starboard-operator.v0.8.0 Succeeded If the above command succeeds and the ClusterServiceVersion has transitioned from Installing to Succeeded phase you will also find the operator's Deployment in the same namespace where the Subscription is: $ kubectl get deployments -n starboard-operator NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs deployment/starboard-operator -n starboard-operator Uninstall To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup: kubectl delete subscription starboard-operator -n starboard-operator kubectl delete clusterserviceversion starboard-operator.v0.9.0 -n starboard-operator kubectl delete operatorgroup starboard-operator -n starboard-operator Note You have to manually delete custom resource definitions created by the OLM operator: kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"Operator Lifecycle Manager"},{"location":"operator/installation/olm/#uninstall","text":"To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup: kubectl delete subscription starboard-operator -n starboard-operator kubectl delete clusterserviceversion starboard-operator.v0.9.0 -n starboard-operator kubectl delete operatorgroup starboard-operator -n starboard-operator Note You have to manually delete custom resource definitions created by the OLM operator: kubectl delete crd vulnerabilityreports.aquasecurity.github.io kubectl delete crd configauditreports.aquasecurity.github.io Danger Deleting custom resource definitions will also delete all security reports generated by the operator.","title":"Uninstall"}]}